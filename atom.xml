<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>EpsilonJohn&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-08-08T15:04:09.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>EpsilonJohn</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>外参未知的立体相机|对极几何</title>
    <link href="http://yoursite.com/2021/08/08/First_Principles_of_CV/%E5%A4%96%E5%8F%82%E6%9C%AA%E7%9F%A5%E7%9A%84%E7%AB%8B%E4%BD%93%E7%9B%B8%E6%9C%BA_%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95.md/"/>
    <id>http://yoursite.com/2021/08/08/First_Principles_of_CV/%E5%A4%96%E5%8F%82%E6%9C%AA%E7%9F%A5%E7%9A%84%E7%AB%8B%E4%BD%93%E7%9B%B8%E6%9C%BA_%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95.md/</id>
    <published>2021-08-08T14:42:16.000Z</published>
    <updated>2021-08-08T15:04:09.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="回顾">回顾</h1><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808224355.png"></p><h1 id="问题描述">问题描述</h1><p>所谓的未标定立体相机问题，就是不知道相机外参而进行3D空间点恢复，其中包含以下几个议题：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808224743.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808225626.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808225922.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808225941.png"></p><p>总结步骤如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808230202.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;回顾&quot;&gt;回顾&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com:59080/images/2021/08/08/20210808224355.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;问题描述&quot;&gt;问题描述&lt;/h1&gt;
&lt;p&gt;所谓的未标定立体相机
      
    
    </summary>
    
    
      <category term="First_Principles_of_CV" scheme="http://yoursite.com/categories/First-Principles-of-CV/"/>
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>外参未知的立体相机|概述</title>
    <link href="http://yoursite.com/2021/08/08/First_Principles_of_CV/%E5%A4%96%E5%8F%82%E6%9C%AA%E7%9F%A5%E7%9A%84%E7%AB%8B%E4%BD%93%E7%9B%B8%E6%9C%BA_%E6%A6%82%E8%BF%B0.md/"/>
    <id>http://yoursite.com/2021/08/08/First_Principles_of_CV/%E5%A4%96%E5%8F%82%E6%9C%AA%E7%9F%A5%E7%9A%84%E7%AB%8B%E4%BD%93%E7%9B%B8%E6%9C%BA_%E6%A6%82%E8%BF%B0.md/</id>
    <published>2021-08-08T14:42:16.000Z</published>
    <updated>2021-08-08T15:03:45.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="回顾">回顾</h1><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808224355.png"></p><h1 id="问题描述">问题描述</h1><p>所谓的未标定立体相机问题，就是不知道相机外参而进行3D空间点恢复，其中包含以下几个议题：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808224743.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808225626.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808225922.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808225941.png"></p><p>总结步骤如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808230202.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;回顾&quot;&gt;回顾&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com:59080/images/2021/08/08/20210808224355.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;问题描述&quot;&gt;问题描述&lt;/h1&gt;
&lt;p&gt;所谓的未标定立体相机
      
    
    </summary>
    
    
      <category term="First_Principles_of_CV" scheme="http://yoursite.com/categories/First-Principles-of-CV/"/>
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>相机标定|内参和外参矩阵提取</title>
    <link href="http://yoursite.com/2021/08/08/First_Principles_of_CV/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A_%E6%8F%90%E5%8F%96%E5%86%85%E5%A4%96%E5%8F%82%E7%9F%A9%E9%98%B5.md/"/>
    <id>http://yoursite.com/2021/08/08/First_Principles_of_CV/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A_%E6%8F%90%E5%8F%96%E5%86%85%E5%A4%96%E5%8F%82%E7%9F%A9%E9%98%B5.md/</id>
    <published>2021-08-08T07:52:26.000Z</published>
    <updated>2021-08-08T08:42:25.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从投影矩阵提取内外参矩阵">从投影矩阵提取内/外参矩阵</h1><p>首先忽略齐次形式，重写成内参矩阵*旋转矩阵，</p><p>又因为：</p><ul><li>内参矩阵是上三角矩阵</li><li>旋转矩阵是正交矩阵</li><li>qr分解可以将矩阵分解为 [上三角矩阵]和[正交矩阵]</li></ul><p>所以，对投影矩阵的前3x3个元素构成的子矩阵进行qr分解，就可以得到内参矩阵</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808163804.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808164054.png"></p><h1 id="畸变的讨论比较简略">畸变的讨论（比较简略）</h1><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808164214.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;从投影矩阵提取内外参矩阵&quot;&gt;从投影矩阵提取内/外参矩阵&lt;/h1&gt;
&lt;p&gt;首先忽略齐次形式，重写成内参矩阵*旋转矩阵，&lt;/p&gt;
&lt;p&gt;又因为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;内参矩阵是上三角矩阵&lt;/li&gt;
&lt;li&gt;旋转矩阵是正交矩阵&lt;/li&gt;
&lt;li&gt;qr分解可以将矩阵
      
    
    </summary>
    
    
      <category term="First_Principles_of_CV" scheme="http://yoursite.com/categories/First-Principles-of-CV/"/>
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>相机标定|相机标定过程</title>
    <link href="http://yoursite.com/2021/08/08/First_Principles_of_CV/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A_%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%E8%BF%87%E7%A8%8B.md/"/>
    <id>http://yoursite.com/2021/08/08/First_Principles_of_CV/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A_%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A%E8%BF%87%E7%A8%8B.md/</id>
    <published>2021-08-08T07:52:26.000Z</published>
    <updated>2021-08-08T08:35:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="相机标定过程">相机标定过程</h1><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808162337.png"></p><p>以立方体的一个角作为世界坐标系原点，相机拍摄图片，就可以得到一系列3D-2D关联。</p><p>对投影模型展开，得到线性等式如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808162459.png"></p><p>将矩阵中的12个参数作为向量，可以重写方程如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808162605.png"></p><p>由于尺度不确定，所以通常有如下解决方法：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808162942.png"></p><p>对上面的Loss func求导，可以得到一个关于特征值的问题，所以问题转化为求解矩阵<span class="math inline">\(A^TA\)</span>最小特征值对应的特征向量，即</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808163105.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;相机标定过程&quot;&gt;相机标定过程&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com:59080/images/2021/08/08/20210808162337.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;以立方体的一个角作为世界坐标系原点，相机拍摄图片，就
      
    
    </summary>
    
    
      <category term="First_Principles_of_CV" scheme="http://yoursite.com/categories/First-Principles-of-CV/"/>
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>相机标定|线性相机模型</title>
    <link href="http://yoursite.com/2021/08/08/First_Principles_of_CV/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A_%E7%BA%BF%E6%80%A7%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B.md/"/>
    <id>http://yoursite.com/2021/08/08/First_Principles_of_CV/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A_%E7%BA%BF%E6%80%A7%E7%9B%B8%E6%9C%BA%E6%A8%A1%E5%9E%8B.md/</id>
    <published>2021-08-08T07:52:26.000Z</published>
    <updated>2021-08-08T08:21:39.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="forward-imaging-model-3d-to-2d">Forward Imaging Model： 3D to 2D</h1><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808155436.png"></p><h1 id="perspective-projection">Perspective Projection</h1><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808160713.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808155532.png"></p><h2 id="相机成像平面与图像的映射">相机成像平面与图像的映射</h2><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808155657.png"> <img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808155756.png"> <img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808155950.png"></p><h2 id="点的齐次坐标表示">点的齐次坐标表示</h2><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808160053.png"></p><h2 id="矩阵表示透视投影模型">矩阵表示透视投影模型</h2><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808160423.png"> <img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808160625.png"></p><h1 id="坐标变换外参">坐标变换（外参）</h1><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808160845.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808161007.png"></p><p>从世界坐标系到相机坐标系的旋转矩阵<span class="math inline">\(R\)</span>的特性：</p><ul><li>第一行： <span class="math inline">\(\hat{x}_c\)</span>在世界坐标系的方向</li><li>第二行： <span class="math inline">\(\hat{y}_c\)</span>的方向</li><li>第三行： <span class="math inline">\(\hat{z}_c\)</span>的方向</li></ul><p>旋转矩阵是正交的，也就是说：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808161243.png"></p><p>因此，当已知：</p><ul><li>从世界坐标系到相机坐标系的旋转矩阵<span class="math inline">\(R\)</span></li><li>相机坐标系在世界坐标系的位移<span class="math inline">\(c_w\)</span></li></ul><p>就可以将一个点从世界坐标系变换到相机坐标系如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808161723.png"></p><p>使用齐次表达如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808161842.png"></p><h1 id="两个过程合起来的表示">两个过程合起来的表示</h1><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808162049.png"></p><blockquote><p>标定相机，就是求解矩阵的12个元素</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;forward-imaging-model-3d-to-2d&quot;&gt;Forward Imaging Model： 3D to 2D&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com:59080/images/2021/08/08/20210
      
    
    </summary>
    
    
      <category term="First_Principles_of_CV" scheme="http://yoursite.com/categories/First-Principles-of-CV/"/>
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>相机标定|简单立体相机</title>
    <link href="http://yoursite.com/2021/08/08/First_Principles_of_CV/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A_%E7%AE%80%E5%8D%95%E7%AB%8B%E4%BD%93%E7%9B%B8%E6%9C%BA.md/"/>
    <id>http://yoursite.com/2021/08/08/First_Principles_of_CV/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A_%E7%AE%80%E5%8D%95%E7%AB%8B%E4%BD%93%E7%9B%B8%E6%9C%BA.md/</id>
    <published>2021-08-08T07:52:26.000Z</published>
    <updated>2021-08-08T09:11:34.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="backward-projection-2d-to-3d">Backward Projection: 2D to 3D</h1><p>假设给定一个标定好的相机，如何从2D图像恢复3D点？</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808164803.png"> <img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808164831.png"></p><h1 id="双目">双目</h1><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808170138.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808170213.png"></p><ul><li>深度与视差成反比</li><li>深度与基线成正比</li><li>基线越大，精度越高</li></ul><h1 id="立体匹配寻找视差">立体匹配（寻找视差）</h1><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808170418.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808170530.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808170615.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808170852.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808171023.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/08/08/20210808171102.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;backward-projection-2d-to-3d&quot;&gt;Backward Projection: 2D to 3D&lt;/h1&gt;
&lt;p&gt;假设给定一个标定好的相机，如何从2D图像恢复3D点？&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com
      
    
    </summary>
    
    
      <category term="First_Principles_of_CV" scheme="http://yoursite.com/categories/First-Principles-of-CV/"/>
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>Localization_for_Ground_Robots论文阅读</title>
    <link href="http://yoursite.com/2021/07/25/Localization_for_Ground_Robots%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/07/25/Localization_for_Ground_Robots%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-07-25T11:03:53.000Z</published>
    <updated>2021-08-06T02:00:42.993Z</updated>
    
    <content type="html"><![CDATA[<h1 id="localization-for-ground-robots-on-manifold-representation-integration-re-parameterization-and-optimization">Localization for Ground Robots: On Manifold Representation, Integration, Re-Parameterization, and Optimization</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725191637.png"></p><h1 id="摘要">摘要</h1><p>本文专注于通过概率融合里程计和单目相机进行地面机器人定位的任务。具体而言，(1) 提出了一种新的方法，通过参数重表示的方法形成<code>motion manifold</code> (2) 使用轮式里程计进行6D的整合 (3) 重新参数化流形等式，减少误差。最后，提出一种基于流形辅助的滑动窗口估计器的完整定位算法，其中使用轮式里程计、单目相机以及可选的IMU。</p><h1 id="符号标记和传感器模型">符号标记和传感器模型</h1><h2 id="符号">符号</h2><p>在这项工作中，我们假设关于全局参考框架{G}的地面机器人，其车轮始终与路面接触。我们使用{C}和{O}来表示相机和里程计的坐标系。</p><p>里程计坐标系{O}的中心位于机器人的轮子中间，其中x轴向前，z轴向上，如图2所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725192754.png"></p><p>另外，我们使用<span class="math inline">\({ }^{\mathbf{A}} \mathbf{p _ { B }}\)</span>和<span class="math inline">\({ }^{\mathbf{A}}_{\mathbf{B}}\bar {\mathbf{q}}\)</span>来表示B系相对于A系的位置和旋转。<span class="math inline">\({ }^{A}_{B} \mathbf{R}\)</span>表示与<span class="math inline">\({ }^{\mathbf{A}}_{\mathbf{B}}\bar {\mathbf{q}}\)</span>对应的旋转矩阵。</p><p>另外，</p><ul><li><span class="math inline">\(\hat{a}\)</span>表示估计值</li><li><span class="math inline">\(\tilde{a}\)</span>表示误差</li><li><span class="math inline">\(a^{T}\)</span>表示转置</li><li><span class="math inline">\(\dot{a}\)</span>表示微分</li><li><span class="math inline">\(\|a\|\)</span>表示二范数</li><li><span class="math inline">\(e_i\)</span>是 3x1的向量，其中第i个元素为1，其他为0</li><li><span class="math inline">\(e_{ij}=[e_i,e_j]\)</span></li></ul><h2 id="轮式里程计观测模型">轮式里程计观测模型</h2><p>与[1,6,21]相似，在时间t，经过标定后的轮式里程计测量值如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725193727.png"></p><p>其中，</p><ul><li><span class="math inline">\({}^{\mathbf{O}(t)} {\mathbf{v}}\)</span>表示车轮坐标系{0}的线速度在车轮坐标系{0}的表示</li><li><span class="math inline">\({}^{\mathbf{O}(t)} {\mathbf{\omega}}\)</span>表示车轮坐标系{0}的角速度在车轮坐标系{0}的表示</li><li><span class="math inline">\(n_{v o}\)</span> and <span class="math inline">\(n_{\omega o}\)</span>是测量噪声</li><li><span class="math inline">\(\mathbf{n}_{v}=\left[\begin{array}{lll}n_{v o} &amp; 0 &amp; 0\end{array}\right]^{\top}\)</span></li><li><span class="math inline">\(\mathbf{n}_{\omega}=\left[\begin{array}{lll}0 &amp; 0 &amp; n_{\omega o}\end{array}\right]^{\top}\)</span></li></ul><p>等式(1)清晰的展示了轮式里程计测量仅提供2D的运动信息，即向前的线速度和关于yaw角的旋转速度。因此，通过使用等式1提供的测量值，理论上可以进行基于平面表面的位姿整合，而执行6D姿态集成。</p><h1 id="基于流形的6d位姿整合">基于流形的6D位姿整合</h1><h2 id="流形表示">流形表示</h2><p>为了允许使用车轮测量测量的6D姿态集成，我们首先通过参数方程式进行运动建模。特别的，我们选用了通过二次多项式来对3D位置p的运动流形进行近似：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725194550.png"></p><p>其中，</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725194602.png"></p><p>流形的参数如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725194634.png"></p><p>我们注意到，传统方法[1,6,21]假设平面环境，等同于流形参数为<span class="math inline">\(\mathbf{m}=[c, 0,0,0,0,0]^{T}\)</span>。他们的设计选择不能代表室外路面的一般情况，因此不适合高精度估计</p><p>现有的工作如[6,21]展示了使用里程计观测对6D定位的困难，这主要是轮速计仅提供2D的测量，仅使用这些测量值，在6D空间进行姿态估计是不可行的。</p><p>然而，使用如式(4)的流形参数定义，6D的位姿估计变得可行，为了进一步描述，我们记姿态微分方程：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725195200.png"></p><p>其中，</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725195226.png"> 为了进行旋转积分，由轮速计提供的角速度<span class="math inline">\({}^{\mathbf{O}(t)} {\mathbf{\omega}}\)</span>需要知道。然而，显然式(1)中的轮速计只能提供向量<span class="math inline">\({}^{\mathbf{O}(t)} {\mathbf{\omega}}\)</span>的第三个元素。</p><p>为了获取另外两个元素，需要使用流形的参数表达，首先：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725195457.png"></p><blockquote><p>这个式子的物理意义是，机器人的旋转矢量与机器人所处位置的流形的法向量是垂直的。</p></blockquote><p>这个式子表达的是，运动流形<span class="math inline">\(\mathcal{M}\)</span>已经显式定义了地面机器人的roll和pitch，其应该与<span class="math inline">\({}_{\mathbf{O}(t)}^{G} {\mathbf{R}}\)</span>具有一致性。换句话说，流形的梯度向量应该与机器人z轴方向平行。</p><p>因此，对式(7)进行微分，得到：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725200153.png"></p><p>通过使用<span class="math inline">\(\mathcal{M}(t)=\mathcal{M}\left({ }^{\mathrm{G}} \mathbf{p}_{\mathbf{O}(t)}\right)\)</span>作为缩写，然后将式（5）和（7）代入式（8），得到：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725201713.png"></p><p>根据上述等式，<span class="math inline">\({}^{\mathbf{O}(t)} {\mathbf{\omega}}\)</span>的前两个元素可以被计算，而第3个元素不能被识别，因为<span class="math inline">\({}^{\mathbf{O}(t)} {\mathbf{\omega}}\)</span>被<span class="math inline">\([e_3]_{\times}\)</span>左乘，其中：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725202130.png"></p><p>另一方面，<span class="math inline">\({}^{\mathbf{O}(t)} {\mathbf{\omega}}\)</span>被<span class="math inline">\([e_3]_{\times}\)</span>的第三个元素起始可以直接从轮速计中读取。这些观测结果验证了我们将轮速计观测和流形表示进行结合的动机，依赖于他们的互补特性，以实现6D的位姿估计。</p><p>为了从式（11）寻找<span class="math inline">\({}^{\mathbf{O}(t)} {\mathbf{\omega}}\)</span>的前两个元素，为了实现这个目的，基于式（7）可以写出如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725202446.png"></p><blockquote><p>这是基于两个向量平行得到的，不信的话等式两边同时叉乘一个<span class="math inline">\(({}_{\mathbf{O}(t)}^{G} {\mathbf{R}} e_3)\)</span></p></blockquote><p>结果，等式（11）变成：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725203623.png"></p><p>其中，还用到了如下性质：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725204222.png"></p><p>且有：<span class="math inline">\({}^{\mathbf{O}^{(t)}} \boldsymbol{\omega}_{12}=\mathbf{e}_{12}^{T} {}^{\mathbf{O}^{(t)}} \boldsymbol{\omega}\)</span></p><p>通过考虑里程计的测量，我们有：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725204351.png"></p><p>通过整合式（18），就可以计算3D的旋转估计了。一旦旋转被计算，就可以进一步通过积分计算位置：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725204442.png"></p><p>我们同样注意到我们的流形表示，如式（2），其隐式定义了运动模型的积分位置必须满足<span class="math inline">\(\mathcal{M}\left({ }^{\mathbf{G}} \mathbf{p}_{\mathbf{O}(t)}\right)=0\)</span>。</p><p>实际上，等式（19）确实满足这个流形约束，为了展示，我们记：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/25/20210725204846.png"></p><blockquote><p>应用到了链式求导，矩阵求导法则，以及等式（13）</p></blockquote><p>其中，在等式（21）中<span class="math inline">\({}_{\mathbf{G}}^{\mathbf{O}(t)} \mathbf{R}^{T} \mathbf{e}_{3}\)</span>与<span class="math inline">\(\nabla \mathcal{M}\left({ }^{\mathbf{G}} \mathbf{p}_{\mathbf{O}(t)}\right)\)</span>成比例，（根据等式13的结果）。</p><p>可以发现，等式（19）中<span class="math inline">\({ }^{\mathrm{G}} \mathbf{v}_{\mathbf{O}(t)}\)</span>的表达明确的满足等式（21）中的manifold约束（天向速度为0呗）。</p><p>随着等式（18）和（19），我们可以进行在流形上的基于里程计的6D位姿整合。整个过程不需要IMU的使用，为了直观地解释这个过程，我们注意到，执行6D姿态估计需要使用传感器测量值或者运动约束来解析6个自由度。在我们的系统中，两个自由度来自轮式里程计测量，两个自由度来自车辆约束（非完整性约束），最后两个自由度由流形等式决定（如等式（18）），并且直觉的解释与数学推导相符。</p><blockquote><p>非完整性约束：来源于等式（19），这个约束表明局部线速度只有在x轴有非零值。 然而当底盘是麦克纳姆轮时，机器人不受该约束，并且必须重新设计方程。然而，这类型的底盘不常用于商业户外机器人或车辆。</p></blockquote><h2 id="状态和误差状态预测">状态和误差状态预测</h2><p>本节，将描述使用提出的流形表示来进行状态和误差状态的传播。由于积分过程需要姿态(位置和方向向量)和流形方程的显式表示，我们定义状态向量如下</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726094454.png"></p><p>状态向量微分方程如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726094542.png"></p><p>其中，</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726094603.png"></p><p>我们指出，流形参数的运动动力学将在后面的小节中详细描述，在式（25）这里先忽略。</p><p>进一步的，有：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726094747.png"></p><p>其中，</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726094813.png"></p><p>式（27）到（29）可以基于轮式里程计进行数值积分（如龙格库塔）。</p><p>为了描述误差状态的传播细节，我们首先对式（18）进行一阶泰勒展开，得到：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726095241.png"></p><p>其中，</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726095305.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726095335.png"></p><blockquote><p>这是角速度对状态和噪声求导</p></blockquote><hr><p>本文定义的误差角<span class="math inline">\(\delta \theta\)</span>是local的[48]，因此，有：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726095523.png"></p><blockquote><p>这个推导可以参见深蓝多传感器融合ppt</p></blockquote><p>进一步的，参考附录，我们可以写出等式（35），关于误差角的微分方程：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726095838.png"></p><blockquote><p>附录A：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726095918.png"></p></blockquote><p>因为<span class="math inline">\(\delta \theta\)</span>是误差状态<span class="math inline">\(\tilde{x}\)</span>的一部分，式（35)的前两项可以结合，因此得到：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726100122.png"></p><p>关于位置项，基于等式（19），可以直接得到如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726100809.png"></p><p>对于流形参数，我们有：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726100831.png"></p><p>结合等式（35）（38）（39），误差状态的微分方程已经推导完成，我们可以进行6D的误差状态集成，把所有的等式放到向量中，我们可以得到误差状态模型：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726100939.png"></p><p>其中，</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726100956.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726101008.png"></p><p>需要注意的是：</p><ul><li><span class="math inline">\((\cdot)_{[:, i]}\)</span>表示矩阵的第i列</li></ul><p>我们注意到，<span class="math inline">\(\mathbf{F}_{c}\)</span>和<span class="math inline">\(\mathbf{G}_{c}\)</span>是连续时间下的误差状态转移矩阵和噪声雅可比矩阵，为了实现离散时间的概率估计器，则需要离散时间下的误差状态转移矩阵<span class="math inline">\(\boldsymbol{\Phi}(t, \tau)\)</span>，这可以通过积分得到：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/26/20210726101304.png"></p><p>数值积分的具体实现和剩下的步骤是离散时间局部化估计的标准步骤，可以参考[12,48]</p><h2 id="流形重新参数化">流形重新参数化</h2><p>等式（2）定义了全局参考坐标系的运动流形，其数值上是正确的，并且能够很容易地给出一个随机估计量。然而，这种表示方式使得流形参数m的传播过程极具挑战性。具体地说，很难描述流形的运动动力学，它随时间而变化。文献[24]简单使用<span class="math inline">\(\dot{\mathbf{m}}=\mathbf{n}_{\omega m}\)</span>，其中<span class="math inline">\(\mathbf{n}_{\omega m}\)</span>是零均值高斯噪声向量。然而，这个方法受限于大规模应用。为了说明细节，让我们以下面的方程为例：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/27/20210727094742.png"></p><p>等式绘制如图3所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/27/20210727094813.png"></p><p>实际上，等式（44）可以考虑为一个二维的流形，如果一个二次二维流形可以用如下方程表示：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/27/20210727094923.png"></p><p>等式（44）实际上是一个分段二次二维流形，其分段形式的参数是：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/27/20210727095419.png"></p><p>可以清晰的看见，当<span class="math inline">\(x=1000\)</span>时，会有明显的jump，在所估计的流形参数，由于我们选择了二次表示来模拟一个非二次方程。</p><p>如果我们能用无穷多个多项式来近似流形，这个问题就不会产生了，然而，这在计算上是不可行的。因此，流形参数的正常传播过程(见Eq. 25)无法捕捉这种类型的变化。</p><p>虽然流形表示在此条件下仍然可行，但通过去除和重新初始化流形参数，这肯定不是高精度姿态估计的首选。为此，我们建议将Eq. 45修改为：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/27/20210727095810.png"></p><p>其中，<span class="math inline">\(x_{o}\)</span>是固定的常值参数，可以通过当前<span class="math inline">\(x\)</span>的估计来得到。如果当<span class="math inline">\(x=1000\)</span>，我们得到一个估计的<span class="math inline">\(x_{o}=999.9\)</span>，来代替等式（46）所展示的jump，即我们的流形参数为：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/27/20210727100145.png"></p><p>通过使用参数<span class="math inline">\(x_{o}\)</span>，流形参数的jump大幅减小。这使得流形的传播等式（状态、误差状态估计）变得可行。</p><p>通过用上面的例子说明我们的动机和高级概念，我们介绍了我们的正式数学-局部流形表示和重新参数化的数学方程。通过假设前一个重新参数化步骤是在时间t k时执行的，下一个步骤是在t k+1时触发的，我们有：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/28/20210728095058.png"></p><p>其中，等式（49）：</p><ul><li><span class="math inline">\(\mathbf{R}_{o}\)</span>和<span class="math inline">\(\mathbf{x}_{o}\)</span>是m(tk)的固定重参数化参数</li><li><span class="math inline">\(\mathbf{R}_{1}\)</span>和<span class="math inline">\(\mathbf{x}_{1}\)</span>是m(tk+1)的固定重参数化参数</li><li><span class="math inline">\(\gamma, \Xi,\boldsymbol{\Psi}\)</span>都是关于<span class="math inline">\(\delta \mathbf{R}\)</span>和<span class="math inline">\(\delta \mathbf{x}\)</span>的函数</li></ul><p>为了选择重新参数化参数，类似于等式（48），我们为<span class="math inline">\(\mathbf{x}_{o}\)</span>的选择<span class="math inline">\({ }^{\mathbf{G}} \mathbf{p}_{\mathbf{O}\left(t_{k}\right)}\)</span>的第一次估计和为<span class="math inline">\(\mathbf{x}_{1}\)</span>选择<span class="math inline">\({ }^{\mathbf{G}} \mathbf{p}_{\mathbf{O}\left(t_{k+1}\right)}\)</span>的第一次估计</p><p>等式（49）的详细推导和 <span class="math inline">\(\mathbf{R}_{o}，\mathbf{R}_{1}\)</span>的选择参见附录C。</p><p>因此，通过式（49），对于状态估计及其相应的不确定性，我们能够在任何时间t周围重新参数化流形表示。我们注意到，这类似于用逆深度参数化重新参数化SLAM特征[15][35]。重要的是要指出，在我们的重新参数化过程中，<span class="math inline">\(\mathbf{R}_{o}，\mathbf{x}_{o}, \mathbf{R}_{1} ，\mathbf{x}_{1}\)</span>被用作固定的常量向量和矩阵，而不是随机变量。这样就不需要计算它们对应的雅可比矩阵，这也是SLAM特征重参数化中广泛使用的技巧[35]，[15]。</p><p><strong>需要指出的是，我们的数值微分和重表示都是基于参数<span class="math inline">\(m\)</span>的，如（Sec. IV-A - IV-B and Appendix B），并且这个参数<span class="math inline">\(m\)</span>还未被任何重新参数化的步骤之前。</strong></p><p>一旦引入重参数化，在<span class="math inline">\(t_{k+1}\)</span>时刻存储在状态向量中的流形变为<span class="math inline">\(m(t_{k+1})\)</span>，而在状态积分中使用的相应流形参数仍为<span class="math inline">\(m\)</span>，这是由于全局流形表示保持不变的事实，<span class="math inline">\(\mathbf{m}\)</span>和<span class="math inline">\(\mathbf{m}(t_{k+1})\)</span>的关系如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/29/20210729094813.png"></p><p>因此，在状态预测和雅可比计算过程中，我们可以简单应用<span class="math inline">\(\mathbf{m}=\left(\prod_{i=0}^{k} \mathbf{\Lambda}\left(t_{i}\right)\right)^{-1} \mathbf{m}\left(t_{k}\right)\)</span>，这允许明确考虑<span class="math inline">\(\mathbf{m}(t_{k+1})\)</span>的不确定性，而不产生额外的计算成本和复杂的软件设计</p><p>通过定义流形的重新参数化过程，我们重新讨论了运动流形的运动动力学特征问题。由于地面机器人所在的表面上导航的流形实际会随着时间的推移而变化，为了保证准确性，必须明确地建模这一事实，具体地说，我们建议在重新参数化过程中考虑这一点：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/29/20210729095318.png"></p><p>其中，</p><ul><li><span class="math inline">\(\mathcal{N}\left(0, \sigma_{i, w m}^{2}\right)\)</span>表示零均值且方差为<span class="math inline">\(\sigma_{i, w m}^{2}\)</span>的高斯分布。</li><li>特别的，标准差<span class="math inline">\(\sigma_{i, w m}\)</span>可以定义如下：</li></ul><p><img src="http://s1.nsloop.com:59080/images/2021/07/29/20210729095444.png"></p><p>其中，<span class="math inline">\(\alpha_{i,p},\alpha_{i,q}\)</span>是控制参数。需要指出的是，在我们提出的公式中，流形不确定性(如<span class="math inline">\(\sigma_{i, w m}\)</span>)是空间位移的函数(见Eq. 54)。<strong>这与VIO文献[12]、[30]中的标准噪声传播方程不同，在这些方程中，噪声的特征是时间函数</strong>。因为运动流形是一个空间概念而不是时间概念，所以我们的设计选择更适合我们的定位问题。</p><h1 id="定位算法">定位算法</h1><p>为了实现地面机器人的高精度定位，在本节中，我们提出了一种详细的定位算法，用于融合来自单目摄像机、车轮里程表和可选IMU的测量数据。具体地说，我们提出了一种基于滑动窗口迭代优化的算法来最小化来自传感器测量和流形约束的代价函数。在接下来的内容中，我们首先描述我们提出的不使用IMU的方法，与IMU相关的额外操作将在第二节中讨论。</p><h2 id="数据插值">数据插值</h2><p>在本文中，我们假设车轮里程表和摄像机传感器在硬件上完全同步，摄像机与机器人刚性连接。在介绍概率传感器融合的细节之前，我们首先注意到我们的系统中需要数据插值，这是由于提出的系统中的一些操作需要执行基于里程计的位姿积分（两帧图像之间）。然而，由于多传感器系统的特性，我们可能无法在捕获图像的同时精确地获取里程计信息。为此，我们建议通过插值图像时间戳前后的最近测量值来计算额外的虚拟车轮里程表测量值。由于车轮里程表测量的频率相对较高(例如，在我们的例子中是100Hz)，而且在人造环境中的地面机器人通常处于平滑运动状态，因此我们选择应用线性插值。</p><h2 id="图像处理">图像处理</h2><p>一旦接收到新图像，我们继续执行位姿积分，通过车轮里程表测量和流形表示计算相应的预测位姿，一旦通过姿态预测检测到足够的平移或旋转位移(例如，在我们的测试中，20厘米和3度)，新图像将被处理，否则它将被丢弃。</p><p>对于特征处理，提取FAST特征[49]，计算FREAK[50]描述符，这是由于它们在低成本处理器上的效率，然后是特征匹配和RANSAC几何验证步骤</p><h2 id="状态向量与迭代优化">状态向量与迭代优化</h2><p>为了说明定位算法，首先介绍状态向量，在时间<span class="math inline">\(t_k\)</span>，状态向量为：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/04/20210804123652.png"></p><blockquote><p>为了更简单的表示，我们在本节的介绍中忽略了传感器的外部参数。然而，在我们的一些实际实验中，当离线传感器外部标定精度不高时，这些参数被明确地建模在我们的公式中，并用于优化。</p></blockquote><p>其中，<span class="math inline">\(x\)</span>在等式（22）中定义，<span class="math inline">\(\mathrm{O}_{k}\)</span>式在k时刻下滑动窗口中的位姿：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/06/20210806094717.png"></p><p>当在<span class="math inline">\(t_{k+1}\)</span>记录到新的图像，将会执行pose积分来计算<span class="math inline">\(\mathbf{X} \mathbf{O}_{k+1}\)</span>，随后，我们使用如下cost func来调整我们的状态：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/06/20210806095019.png"></p><p>其中，</p><ul><li><span class="math inline">\(\boldsymbol{\eta}_{k}\)</span>和<span class="math inline">\(\boldsymbol{\Sigma}_{k}\)</span>分别是在之前的时间步中估计的先验信息向量和矩阵。</li><li><span class="math inline">\(\|\mathbf{a}\|_{\Sigma}\)</span>由<span class="math inline">\(\mathbf{a}^{T} \mathbf{\Sigma} \mathbf{a}\)</span>计算得到</li><li><span class="math inline">\(\mathbf{f}_{k+1}\)</span>是被包含到优化过程中的视觉landmarks的集合</li><li><span class="math inline">\(\mathbf{S}_{i, j}\)</span>表示关键帧和观测到的特征的匹配对集合</li><li><span class="math inline">\(\gamma_{i, j}\)</span>是计算的视觉重投影残差向量</li><li><span class="math inline">\(\boldsymbol{\psi}_{i}\)</span>是与运动流形相关联的残差</li><li><span class="math inline">\(\boldsymbol{\beta}_{i}\)</span>是在<span class="math inline">\(t_k\)</span>和<span class="math inline">\(t_{k+1}\)</span>时间内，基于轮速里程计测量的位姿预测残差（Sec. IV-B）</li></ul><p>特别的，视觉重投影残差计算如下：</p><p><img src="http://s1.nsloop.com:59080/images/2021/08/06/20210806095813.png"></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{Z}_{i j}\)</span>表示与位姿<span class="math inline">\(i\)</span>和视觉landmark <span class="math inline">\(\mathbf{f}_{j}\)</span>相关的相机观测 （图像点）</li><li><span class="math inline">\(\boldsymbol{\Sigma}_{C}\)</span>是观测的信息矩阵</li><li>函数<span class="math inline">\(h(\cdot)\)</span>表示经过校准的相机模型[20]</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;localization-for-ground-robots-on-manifold-representation-integration-re-parameterization-and-optimization&quot;&gt;Localization for Ground 
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>MSCKF论文阅读</title>
    <link href="http://yoursite.com/2021/07/20/MSCKF%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/07/20/MSCKF%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-07-20T14:03:53.000Z</published>
    <updated>2021-07-24T14:07:21.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="a-multi-state-constraint-kalman-filter-for-vision-aided-inertial-navigation">A Multi-State Constraint Kalman Filter for Vision-aided Inertial Navigation</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720220945.png"></p><h1 id="摘要">摘要</h1><p>本文介绍了一个基于扩展卡尔曼滤波器的算法，用于实时视觉辅助的惯性导航算法。本项工作的主要贡献是观测模型的导出，其能够表达从多个相机pose观察到静态特征所构成的几何约束。该测量模型不需要在EKF的状态向量中包含3D特征点的位置。</p><p>提出的视觉辅助惯性导航算法的计算复杂性只与特征点数量线性相关，并且能够在大型现实环境中进行高精度的姿态估计。</p><h1 id="系统描述">系统描述</h1><p>目标是估计与IMU固定的坐标系相对于全局参考坐标系的3D位姿。</p><p>为了简化地球自转对IMU测量的影响，本文选取全局坐标系为ECEF坐标系，整体算法流程如<strong>算法1</strong>所示.</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/24/20210724152005.png"></p><p>IMU测量被用于进行实时处理，用于传播EKF状态和协方差（Section III-B）。另一方面，每一帧图像到达时，相机的位姿估计被添加到状态向量中（Section III-C）。状态增广是为了处理特征观测所必须的，因为每个跟踪的特征点的观测用于在所有相机位姿中施加约束，从而用于对观测到该特征点的相机位姿进行约束。</p><p>因此，在任何时候，EKF状态向量都包含：</p><ul><li>IMU状态</li><li>过去Nmax帧相机位姿</li></ul><h2 id="ekf状态向量的结构">EKF状态向量的结构</h2><p>IMU状态：</p><p><span class="math display">\[\mathbf{X}_{\mathrm{IMU} }=\left[\begin{array}{lllll}{ }_{G}^{I} \bar{q}^{T} &amp; \mathbf{b}_{g}{ }^{T} &amp; { }^{G} \mathbf{v}_{I}^{T} &amp; \mathbf{b}_{a}{ }^{T} &amp; { }^{G} \mathbf{p}_{I}^{T}\end{array}\right]^{T}\]</span></p><p>其中，</p><ul><li>单位四元数[19] <span class="math inline">\({ }_{G}^{I} \bar{q}\)</span>用于描述从全局坐标系<span class="math inline">\(G\)</span>到IMU坐标系<span class="math inline">\(I\)</span>的旋转</li><li><span class="math inline">\({ }^{G} \mathbf{p}_{I}\)</span>和<span class="math inline">\({ }^{G} \mathbf{v}_{I}\)</span>分别描述了IMU位置和速度，相对于全局坐标系<span class="math inline">\(G\)</span></li><li><span class="math inline">\(b_g\)</span>和<span class="math inline">\(b_a\)</span>是 3x1的向量，描述IMU的bias，IMU Biases被建模为受高斯噪声<span class="math inline">\(n_{wg},n_{wa}\)</span>驱动的随机游走过程。</li></ul><p>因此，IMU的误差状态模型被定义为如下：</p><p><span class="math display">\[\widetilde{\mathbf{X} }_{\mathrm{IMU} }=\left[\begin{array}{lllll}\boldsymbol{\delta} \boldsymbol{\theta}_{I}^{T} &amp; \widetilde{\mathbf{b} }_{g}^{T} &amp; { }^{G} \widetilde{\mathbf{v} }_{I}^{T} &amp; \widetilde{\mathbf{b} }_{a}^{T} &amp; { }^{G} \widetilde{\mathbf{p} }_{I}^{T}\end{array}\right]^{T}\]</span></p><p>对于位置、速度、biases，使用了标准的误差add定义，如位置误差<span class="math inline">\(\widetilde{x}=x-\hat{x}\)</span>，然而，对于四元数，则使用另外的准则。</p><p>实际上，如果<span class="math inline">\(\hat{\bar{q} }\)</span>是四元数<span class="math inline">\(\bar{q}\)</span>的估计值，那么，旋转误差可以描述为四元数误差<span class="math inline">\(\delta \bar{q}\)</span>，其中<span class="math inline">\(\bar{q}=\delta \bar{q} \otimes \hat{\bar{q} }\)</span> (这个要区分误差是定义在哪里，这里跟ESKF一样，定义在局部的，只不过这里的q是从全局坐标系到机体坐标系)</p><p>此处，误差四元数如下：</p><p><span class="math display">\[\delta \bar{q} \simeq\left[\begin{array}{ll}\frac{1}{2} \boldsymbol{\delta} \boldsymbol{\theta}^{T} &amp; 1\end{array}\right]^{T}\]</span></p><p>直观的，误差四元数<span class="math inline">\(\delta \bar{q}\)</span>描述了一个估计姿态和真实姿态之间的小角度旋转。因为姿态包含了3个自由度，使用<span class="math inline">\(\delta \theta\)</span>来描述姿态误差才是最小的表示。</p><p>假设在时间步骤k的EKF状态向量中包含N个相机姿势，该向量具有以下形式：</p><p><span class="math display">\[\hat{\mathbf{X} }_{k}=\left[\begin{array}{llllll}\hat{\mathbf{X} }_{\mathrm{IMU}_{k} }^{T} &amp; { }_{G}^{C_{1} } \hat{\bar{q} }^{T} &amp; { }^{G} \hat{\mathbf{p} }_{C_{1} }^{T} &amp; \ldots &amp; { }_{G}^{C_{N} } \hat{q}^{T} &amp; { }^{G} \hat{\mathbf{p} }_{C_{N} }^{T}\end{array}\right]^{T}\]</span></p><p>其中，<span class="math inline">\({}_G^{G_i} \hat{\bar{q} }, {}^G {\hat{p}_{C_i}, i=1\dots N}\)</span>是N个相机的姿态和位置。</p><p>因此，EKF的误差状态向量定义如下：</p><p><span class="math display">\[\widetilde{\mathbf{X} }_{k}=\left[\begin{array}{llllll}\widetilde{\mathbf{X} }_{\mathrm{IMU}_{k} }^{T} &amp; \boldsymbol{\delta} \boldsymbol{\theta}_{C_{1} }^{T} &amp; { }^{G} \widetilde{\mathbf{p} }_{C_{1} }^{T} &amp; \ldots &amp; \boldsymbol{\delta} \boldsymbol{\theta}_{C_{N} }^{T} &amp; { }^{T} \widetilde{\mathbf{p} }_{C_{N} }^{T}\end{array}\right]^{T}\]</span></p><h2 id="传播">传播</h2><p>滤波器传播的等式通过连续时间模型的离散化形式导出，定义如下：</p><h3 id="连续时间系统模型">连续时间系统模型</h3><p>IMU状态微分方程描述：</p><p><span class="math display">\[{ }_{G}^{I} \dot{\bar{q} }(t)=\frac{1}{2} \boldsymbol{\Omega}(\boldsymbol{\omega}(t))_{G}^{I} \bar{q}(t)\]</span></p><p><span class="math display">\[\dot{\mathbf{b} }_{g}(t)=\mathbf{n}_{w g}(t)\]</span></p><p><span class="math display">\[{ }^{G} \dot{\mathbf{v} }_{I}(t)={ }^{G} \mathbf{a}(t)\]</span></p><p><span class="math display">\[\dot{\mathbf{b} }_{a}(t)=\mathbf{n}_{w a}(t)\]</span></p><p><span class="math display">\[{ }^{G} \mathbf{p}_{I}(t)={ }^{G} \mathbf{v}_{I}(t)\]</span></p><p>其中，<span class="math inline">\({ }^{G} \mathbf{a}\)</span>表示机体加速度在全局坐标系的表示，<span class="math inline">\(\boldsymbol{\omega}=\left[\begin{array}{lll}\omega_{x} &amp; \omega_{y} &amp; \omega_{z}\end{array}\right]^{T}\)</span>表示IMU坐标系的旋转角速度。</p><p>另外的计算符号定义如下：</p><p><span class="math display">\[\boldsymbol{\Omega}(\boldsymbol{\omega})=\left[\begin{array}{cc}-\lfloor\boldsymbol{\omega} \times\rfloor &amp; \boldsymbol{\omega} \\-\boldsymbol{\omega}^{T} &amp; 0\end{array}\right], \quad\lfloor\boldsymbol{\omega} \times\rfloor=\left[\begin{array}{ccc}0 &amp; -\omega_{z} &amp; \omega_{y} \\\omega_{z} &amp; 0 &amp; -\omega_{x} \\-\omega_{y} &amp; \omega_{x} &amp; 0\end{array}\right]\]</span></p><p>陀螺仪和加速度计的测量描述如下 [20 ]：</p><p><span class="math display">\[\boldsymbol{\omega}_{m}=\boldsymbol{\omega}+\mathbf{C}\left({ }_{G}^{I} \bar{q}\right) \boldsymbol{\omega}_{G}+\mathbf{b}_{g}+\mathbf{n}_{g}\]</span></p><p><span class="math display">\[\begin{aligned}\mathbf{a}_{m}=&amp; \mathbf{C}\left({ }_{G}^{I} \bar{q}\right)\left({ }^{G} \mathbf{a}-{ }^{G} \mathbf{g}+2\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor^{G} \mathbf{v}_{I}+\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor^{2}{ }^{G} \mathbf{p}_{I}\right) \\&amp;+\mathbf{b}_{a}+\mathbf{n}_{a}\end{aligned}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(C(\cdot)\)</span>表示旋转矩阵</li><li><span class="math inline">\(n_g,n_a\)</span>为零均值高斯白噪声</li><li>值得注意的是，IMU测量结合了星球的旋转，<span class="math inline">\(w_{G}\)</span>的效果</li><li>此外，加速度计测量包括引力加速度,<span class="math inline">\({ }^{G} \mathbf{g}\)</span>, (expressed in the local frame)</li></ul><p>在上面的连续时间状态方程中，应用这些运算符，就可以得到了IMU的状态估计方程：</p><p><span class="math display">\[{}_{G}^{I} \dot{\hat{\bar{q} } }=\frac{1}{2} \boldsymbol{\Omega}(\hat{\boldsymbol{\omega} })_{G}^{I} \hat{\bar{q} }\]</span></p><p><span class="math display">\[\dot{\hat{\mathbf{b} } }_{g}=\mathbf{0}_{3 \times 1}\]</span></p><p><span class="math display">\[{ }^{G} \dot{\hat{\mathbf{v} } }_{I}=\mathbf{C}_{\hat{q} }^{T} \hat{\mathbf{a} }-2\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor^{G} \hat{\mathbf{v} }_{I}-\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor^{2}{ }^{G} \hat{\mathbf{p} }_{I}+{ }^{G} \mathbf{g}\]</span></p><p><span class="math display">\[\dot{\hat{\mathbf{b} } }_{a}=\mathbf{0}_{3 \times 1}\]</span></p><p><span class="math display">\[{ }^{G} \dot{\hat{\mathbf{p} } }_{I}={ }^{G} \hat{\mathbf{v} }_{I}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{C}_{\hat{q} }=C({}_G^{I}\hat{\bar{q} })\)</span></li><li><span class="math inline">\(\hat{a}=a_m-\bar{b}_{a}\)</span></li><li><span class="math inline">\(\hat{\omega}=\omega_{m}-\hat{b}_{g}-C_{\hat{q} }\omega_{G}\)</span></li></ul><p>IMU误差状态的线性化连续时间模型如下表示：</p><p><span class="math display">\[\dot{\widetilde{\mathbf{X} } }_{\mathrm{IMU} }=\mathbf{F} \tilde{\mathbf{X} }_{\mathrm{IMU} }+\mathbf{G} \mathbf{n}_{\mathrm{IMU} }\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{n}_{\mathrm{IMU} }=\left[\begin{array}{llll}\mathbf{n}_{g}^{T} &amp; \mathbf{n}_{w g}^{T} &amp; \mathbf{n}_{a}^{T} &amp; \mathbf{n}_{w a}^{T}\end{array}\right]^{T}\)</span>是系统噪声</li><li>关于<span class="math inline">\(\mathbf{n}_{IMU}\)</span>的协方差矩阵，<span class="math inline">\(\mathbf{Q}_{\mathrm{IMU} }\)</span>，取决于IMU噪声特性，可以再传感器标定期间离线计算。</li></ul><p>最后，F矩阵和G矩阵可以整理如下：</p><p><span class="math display">\[\mathbf{F}=\left[\begin{array}{ccccc}-\lfloor\hat{\boldsymbol{\omega} } \times\rfloor &amp; \mathbf{- I}_{3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} \\-\mathbf{C}_{\hat{q} }^{T}\lfloor\hat{\mathbf{a} } \times\rfloor &amp; \mathbf{0}_{3 \times 3} &amp; -2\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor &amp; -\mathbf{C}_{\hat{q} }^{T} &amp; -\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor^{2} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{I}_{3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3}\end{array}\right]\]</span></p><p><span class="math display">\[\mathbf{G}=\left[\begin{array}{cccc}-\mathbf{I}_{3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{I}_{3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; -\mathbf{C}_{\hat{q} }^{T} &amp; \mathbf{0}_{3 \times 3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{I}_{3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3}\end{array}\right]\]</span></p><h3 id="离散时间模型实现">离散时间模型实现</h3><p>由于IMU在周期T内进行采样，得到采样信号<span class="math inline">\(\omega_{m},a_{m}\)</span>，每次接收到新的IMU测量时，IMU状态估计采用5阶龙哥库塔(RK-5)记性积分传播。</p><p>此外，EKF的协方差矩阵必须传播，因此，我们介绍对协方差的分区：</p><p><span class="math display">\[\mathbf{P}_{k \mid k}=\left[\begin{array}{ll}\mathbf{P}_{I I_{k \mid k} } &amp; \mathbf{P}_{I C_{k \mid k} } \\\mathbf{P}_{I C_{k \mid k} }^{T} &amp; \mathbf{P}_{C C_{k \mid k} }\end{array}\right]\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{P}_{I I_{k \mid k} }\)</span>是 15x15的协方差矩阵，关于IMU状态</li><li><span class="math inline">\(\mathbf{P}_{C C_{k \mid k} }\)</span>是6Nx6N的协方差矩阵，关于相机位姿估计的</li><li><span class="math inline">\(\mathbf{P}_{I C_{k \mid k} }\)</span>是IMU状态和相机位姿估计误差的相关性</li></ul><p>通过这样的分块，传播状态的协方差矩阵按如下进行：</p><p><span class="math display">\[\mathbf{P}_{k+1 \mid k}=\left[\begin{array}{cc}\mathbf{P}_{I I_{k+1 \mid k} } &amp; \mathbf{\Phi}\left(t_{k}+T, t_{k}\right) \mathbf{P}_{I C_{k \mid k} } \\\mathbf{P}_{I C_{k \mid k} }^{T} \mathbf{\Phi}\left(t_{k}+T, t_{k}\right)^{T} &amp; \mathbf{P}_{C C_{k \mid k} }\end{array}\right]\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{P}_{I I_{k+1 \mid k} }\)</span>有李雅普诺夫(Lyapunov)等式进行数值积分得到：</li></ul><p><span class="math display">\[\dot{\mathbf{P} }_{I I}=\mathbf{F} \mathbf{P}_{I I}+\mathbf{P}_{I I} \mathbf{F}^{T}+\mathbf{G} \mathbf{Q}_{\mathrm{IMU} } \mathbf{G}^{T}\]</span></p><p>数值积分即以初始值<span class="math inline">\(\mathbf{P}_{I I_{k\mid k} }\)</span>对时间间隔<span class="math inline">\((t_k,t_{k+T})\)</span>进行积分。</p><ul><li>误差转移矩阵<span class="math inline">\(\mathbf{\Phi}\left(t_{k}+T, t_{k}\right)\)</span>由微分方程进行数值积分来近似得到：</li></ul><p><span class="math display">\[\dot{\boldsymbol{\Phi} }\left(t_{k}+\tau, t_{k}\right)=\mathbf{F} \boldsymbol{\Phi}\left(t_{k}+\tau, t_{k}\right), \quad \tau \in[0, T]\]</span></p><p>其中，初始条件为</p><p><span class="math display">\[\mathbf{\Phi}\left(t_{k}, t_{k}\right)=\mathbf{I}_{15\times 15}\]</span></p><h3 id="状态增广">状态增广</h3><p>当接受到新的图像时，首先聪IMU姿态估计来计算相机的姿态估计初值：</p><p><span class="math display">\[_{G}^{C} \hat{\bar{q} }=\underset{I}{C} \bar{q} \otimes_{G}^{I} \hat{\bar{q} }\]</span></p><p><span class="math display">\[{ }^{G} \hat{\mathbf{p} }_{C}={ }^{G} \hat{\mathbf{p} }_{I}+\mathbf{C}_{\hat{q} }^{T}{ }^{I} \mathbf{p}_{C}\]</span></p><p>其中，</p><ul><li><span class="math inline">\({}_I^C \bar{q}\)</span>表示从IMU坐标系到相机坐标系的变换</li><li><span class="math inline">\({ }^{I} \mathbf{p}_{C}\)</span>表示相对于IMU坐标系的原点，相机坐标系的位置。</li></ul><p>由于相机位姿估计附加到状态向量中，因此EKF的协方差矩阵也进行增广:</p><p><span class="math display">\[\mathbf{P}_{k \mid k} \leftarrow\left[\begin{array}{c}\mathbf{I}_{6 N+15} \\\mathbf{J}\end{array}\right] \mathbf{P}_{k \mid k}\left[\begin{array}{c}\mathbf{I}_{6 N+15} \\\mathbf{J}\end{array}\right]^{T}\]</span></p><p>其中，雅克比<span class="math inline">\(J\)</span>根据式(14)进行微分推导 （谁对谁求导？）</p><p>式(14)截图如下： <img src="http://s1.nsloop.com:59080/images/2021/07/24/20210724170432.png"></p><p>最后得到雅克比如下：</p><p><span class="math display">\[\mathbf{J}=\left[\begin{array}{cccc}\mathbf{C}\left({ }_{I}^{C} \bar{q}\right) &amp; \mathbf{0}_{3 \times 9} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 6 N} \\\left\lfloor\mathbf{C}_{\hat{q} }^{T I} \mathbf{p}_{C} \times\right\rfloor &amp; \mathbf{0}_{3 \times 9} &amp; \mathbf{I}_{3} &amp; \mathbf{0}_{3 \times 6 N}\end{array}\right]\]</span></p><h3 id="测量模型">测量模型</h3><p>我们现在介绍用于更新状态估计的测量模型，这是本文的主要贡献。</p><p>由于EKF用于状态估计，因此对于构造测量模型，需要定义残差r ， 这取决于误差状态<span class="math inline">\(\widetilde{\mathbf{X} }\)</span>，因此，根据通用形式，有：</p><p><span class="math display">\[\mathbf{r}=\mathbf{H} \widetilde{\mathbf{X} }+ noise\]</span></p><p>在这个表达式中，H是测量雅比亚矩阵。对于EKF框架，应用的对于误差状态的噪声项必须是零均值、不相关的白噪声。</p><p>为了衍生我们的测量模型，我们的动机是通过多个相机的静态特征来实现涉及所有这些姿势的约束。在我们的工作中，相机观测按跟踪的特征进行分组，而不是每个相机姿势记录对应的观测（如[7,13,14]）。</p><p>同一个3D点的所有测量值都用于定于约束方程（在后面的等式24），与测量发生的所有相机姿势所相关联。<strong>这样的方法实现了在状态向量中可以不包含特征点的位置</strong>。</p><p>我们通过考虑单个特征 <span class="math inline">\(f_j\)</span> 被多个相机pose<span class="math inline">\(\left({ }_{G}^{C_{i} } \bar{q},{ }^{G} \mathbf{p}_{C_{i} }\right), i \in \mathcal{S}_{j}\)</span>所构成的集合<span class="math inline">\(M_{j}\)</span>所共同观测到的情况来提出测量模型。 <span class="math inline">\(M_{j}\)</span>中的每个观测都可以使用如下模型来描述：</p><p><span class="math display">\[\mathbf{z}_{i}^{(j)}=\frac{1}{ { }^{C_{i} } Z_{j} }\left[\begin{array}{c}{ }^{C_{i} } X_{j} \\{ }^{C_{i} } Y_{j}\end{array}\right]+\mathbf{n}_{i}^{(j)}, \quad i \in \mathcal{S}_{j}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{n}_{i}^{(j)}\)</span>是2x1的图像噪声向量，具有协方差矩阵<span class="math inline">\(\mathbf{R}_{i}^{(j)}=\sigma_{i m}^{2} \mathbf{I}_{2}\)</span></li><li>特征点的位置<span class="math inline">\({ }^{C_{i} } \mathbf{p}_{f_{j} }\)</span>被表示为在相机坐标系中，如下得到：</li></ul><p><span class="math display">\[{ }^{C_{i} } \mathbf{p}_{f_{j} }=\left[\begin{array}{c}{ }^{C_{i} } X_{j} \\{ }^{C_{i} } Y_{j} \\{ }^{C_{i} } Z_{j}\end{array}\right]=\mathbf{C}\left({ }_{G}^{C_{i} } \bar{q}\right)\left({ }^{G} \mathbf{p}_{f_{j} }-{ }^{G} \mathbf{p}_{C_{i} }\right)\]</span></p><p>其中，</p><ul><li><span class="math inline">\({ }^{G} \mathbf{p}_{f_{j} }\)</span>表示特征点在全局坐标系的3D位置，由于这是未知的，在我们的算法的第一步中，我们使用最小二乘最小化以获得估计值<span class="math inline">\({ }^{G} \hat{\mathbf{p} }_{f_{j} }\)</span>。 这是利用观测值<span class="math inline">\(\mathbf{z}_{i}^{(j)}, i \in \mathcal{S}_{j}\)</span>以及滤波器估计值关于相机姿态来实现的（具体参考附录）</li></ul><p>一旦获得了特征位置的估计，我们计算观测的残差：</p><p><span class="math display">\[\mathbf{r}_{i}^{(j)}=\mathbf{z}_{i}^{(j)}-\hat{\mathbf{z} }_{i}^{(j)}\]</span></p><p>其中，</p><p><span class="math display">\[\hat{\mathbf{z} }_{i}^{(j)}=\frac{1}{ {}^{C_{i} } \hat{Z}_{j} }\left[\begin{array}{c}{ }^{C_{i} } \hat{X}_{j} \\{ }^{C_{i} } \hat{Y}_{j}\end{array}\right]\]</span></p><p><span class="math display">\[\left[\begin{array}{c}{ }^{C_{i} } \hat{X}_{j} \\{ }^{C_{i} } \hat{Y}_{j} \\{ }^{C_{i} } \hat{Z}_{j}\end{array}\right]=\mathbf{C}\left({ }_{G}^{C_{i} } \hat{q}\right)\left({ }^{G} \hat{\mathbf{p} }_{f_{j} }-{ }^{G} \hat{\mathbf{p} }_{C_{i} }\right)\]</span></p><p>对上式( 等式20 )中关于相机位姿和特征点位置进行线性化，得到近似如下：</p><p><span class="math display">\[\mathbf{r}_{i}^{(j)} \simeq \mathbf{H}_{\mathbf{X}_{i} }^{(j)} \widetilde{\mathbf{X} }+\mathbf{H}_{f_{i} }^{(j) G} \widetilde{\mathbf{p} }_{f_{j} }+\mathbf{n}_{i}^{(j)}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{H}_{\mathbf{X}_{i} }^{(j)}\)</span>是观测<span class="math inline">\(\mathbf{z}_{i}^{(j)}\)</span>对状态的雅克比</li><li><span class="math inline">\(\mathbf{H}_{f_{i} }^{(j)}\)</span>是观测<span class="math inline">\(\mathbf{z}_{i}^{(j)}\)</span>对特征点位置的雅克比 ( <span class="math inline">\(\mathbf{z}_{i}^{(j)}\)</span> 难道不是直接从图像获取的特征点吗)</li><li><span class="math inline">\(\widetilde{\mathbf{X} }\)</span>滤波器的误差状态</li><li><span class="math inline">\({ }^{G} \widetilde{\mathbf{p} }_{f_{j} }\)</span> 关于特征点<span class="math inline">\(f_j\)</span>的误差</li><li>上面H矩阵的具体推导可参见[21]</li></ul><p>通过叠加关于特征点<span class="math inline">\(f_j\)</span>的相机位姿集合<span class="math inline">\(M_j\)</span>中所有的残差，可以得到：</p><p><span class="math display">\[\mathbf{r}^{(j)} \simeq \mathbf{H}_{\mathbf{X} }^{(j)} \widetilde{\mathbf{X} }+\mathbf{H}_{f}^{(j) G} \widetilde{\mathbf{p} }_{f_{j} }+\mathbf{n}^{(j)}\]</span></p><p>其中，<span class="math inline">\(\mathbf{r}^{(j)}, \mathbf{H}_{\mathbf{X} }^{(j)}, \mathbf{H}_{f}^{(j)}\)</span>, and <span class="math inline">\(\mathbf{n}^{(j)}\)</span>都是分别包含如下元素<span class="math inline">\(\mathbf{r}_{i}^{(j)}, \mathbf{H}_{\mathbf{X}_{i} }^{(j)}, \mathbf{H}_{f_{i} }^{(j)}\)</span>, and <span class="math inline">\(\mathbf{n}_{i}^{(j)}\)</span>的向量或矩阵。 并且由于不同图像中的特征观测是独立的，因此<span class="math inline">\(\mathbf{n}^{(j)}\)</span>的协方差矩阵为<span class="math inline">\(\mathbf{R}^{(j)}=\sigma_{\mathrm{im} }^{2} \mathbf{I}_{2 M_{j} }\)</span>。</p><p>请注意，由于状态估计值<span class="math inline">\(\mathbf{X}\)</span>，用于计算特征点的位置估计（参考附录），式(22) 即线性化后的残差等式与误差状态<span class="math inline">\(\mathbf{\tilde{X} }\)</span>相关联。因此，残差<span class="math inline">\(\mathbf{r}^{(j)}\)</span>并非如等式(17)的形式(<span class="math inline">\(\mathbf{r}=\mathbf{H} \tilde{\mathbf{X} }+\)</span> noise)，不能直接用于EKF的测量更新步骤。</p><p>为了克服这个问题，我们通过将<span class="math inline">\(\mathbf{r}^{(j)}\)</span>投影到矩阵<span class="math inline">\(\mathbf{H}_{f}^{(j)}\)</span>的左零空间，从而定义了一个新的残差<span class="math inline">\(\mathbf{r}_{o}^{(j)}\)</span>。特别的，如果我们使用酉矩阵来表示<span class="math inline">\(\mathbf{A}\)</span>，其中它的列形成了关于<span class="math inline">\(\mathbf{H}_{f}\)</span>左零空间中的bias （这说的啥意思）</p><p><span class="math display">\[\begin{aligned}\mathbf{r}_{o}^{(j)} &amp;=\mathbf{A}^{T}\left(\mathbf{z}^{(j)}-\hat{\mathbf{z} }^{(j)}\right) \simeq \mathbf{A}^{T} \mathbf{H}_{\mathbf{X} }^{(j)} \widetilde{\mathbf{X} }+\mathbf{A}^{T} \mathbf{n}^{(j)} \\&amp;=\mathbf{H}_{o}^{(j)} \widetilde{\mathbf{X} }^{(j)}+\mathbf{n}_{o}^{(j)}\end{aligned}\]</span></p><p>因为 <span class="math inline">\(2M_j \times 3\)</span>的矩阵<span class="math inline">\(\mathbf{H}_{f}^{(j)}\)</span>是列满秩的，他的左零空间维度为<span class="math inline">\(2 M_{j}-3\)</span>。因此<span class="math inline">\(\mathbf{r}_{o}^{(j)}\)</span>是<span class="math inline">\(\left(2 M_{j}-3\right) \times 1\)</span>的向量。这种残差独立于特征坐标中的误差，因此可以基于它执行EKF更新。</p><p>式<span class="math inline">\(\mathbf{H}_{o}^{(j)} \widetilde{\mathbf{X} }^{(j)}+\mathbf{n}_{o}^{(j)}\)</span>定义了所有观测到特征点<span class="math inline">\(f_j\)</span>。这表达了测量<span class="math inline">\(\mathbf{z}_{i}^{(j)}\)</span>为<span class="math inline">\(M_j\)</span>的状态提供所有的有效信息，因此产生的EKF更新是最优的，除了由于线性化所引起的不准确性。</p><p>应该提到的是，为了计算残差<span class="math inline">\(\mathbf{r}_{O}^{(j)}\)</span>和观测矩阵<span class="math inline">\(\mathbf{H}_{o}^{(j)}\)</span>，酉矩阵<span class="math inline">\(\mathbf{A}\)</span>不需要显式地被评估。相反，残差<span class="math inline">\(\mathbf{r}\)</span>和矩阵<span class="math inline">\(\mathbf{H}_{\mathbf{X} }^{(j)}\)</span>在<span class="math inline">\(\mathbf{H}_{f}^{(j)}\)</span>矩阵左零空间的投影可以通过使用<code>Givens</code>旋转[22]来计算得到，操作复杂度为<span class="math inline">\(O\left(M_{j}^{2}\right)\)</span>。另外，<span class="math inline">\(\mathbf{A}\)</span>是酉矩阵，因此向量<span class="math inline">\(\mathbf{n}_{o}^{(j)}\)</span>的协方差可以如下计算：</p><p><span class="math display">\[E\left\{\mathbf{n}_{o}^{(j)} \mathbf{n}_{o}^{(j) T}\right\}=\sigma_{\mathrm{im} }^{2} \mathbf{A}^{T} \mathbf{A}=\sigma_{\mathrm{im} }^{2} \mathbf{I}_{2 M_{j}-3}\]</span></p><h3 id="ekf更新">EKF更新</h3><p>在前面的部分中，我们呈现了一种测量模型，其表示通过观察来自多个相机姿势的静态特征而施加的几何约束。 我们现在详细介绍了EKF的更新阶段，其中使用从观察多个特征的约束。 EKF更新由以下两个事件之一触发</p><ul><li>当检测不到之前在多个图像跟踪的特征时，则使用第III-D部分中呈现的方法处理此特征的所有测量。 这种情况最常出现，因为特征点有可能在相机视野范围之外。</li><li>每次记录新图像时，当前相机姿势估计将被包含在状态向量中，如果已经达到了设定的最大相机位姿数<span class="math inline">\(N_{max}\)</span>，则必须删除最少一个旧的相机位姿。在丢弃状态之前，使用在相应的时间瞬间发生的所有特征观测，以便利用其局部信息。在我们的算法中，从第二最旧的相机位姿开始，我们选择均匀间隔的<span class="math inline">\(\frac{N_{max} }{3}\)</span>的位姿，在使用这些姿势共有的特征的约束执行 EKF 更新后，这些被丢弃。我们选择始终保持最古老的姿势在状态向量中，因为涉及及时进一步姿势的几何结构通常对应于较大的基线，因此携带更有价值的定位信息，这种方法在实践中表现得非常好。</li></ul><p>考虑到在给定的时间步骤中，必须处理由上述两个标准选择的<span class="math inline">\(L\)</span>个特征点的约束。遵循前一节中描述的过程，我们对每一个特征点计算残差向量<span class="math inline">\(\mathbf{r}_{o}^{(j)}, j=1 \ldots L\)</span>以及相关联的观测矩阵<span class="math inline">\(\mathbf{H}_{o}^{(j)}, j=1 \ldots L\)</span>。通过将所有残差堆叠在一个向量中，我们得到：</p><p><span class="math display">\[\mathbf{r}_{o}=\mathbf{H}_{\mathbf{X} } \widetilde{\mathbf{X} }+\mathbf{n}_{o}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{r}_{o}\)</span>的块元素为<span class="math inline">\(\mathbf{r}_{o}^{(j)}\)</span></li><li><span class="math inline">\(\mathbf{n}_{o}\)</span>的块元素为<span class="math inline">\(\mathbf{n}_{o}^{(j)}, j=1 \ldots L\)</span></li><li><span class="math inline">\(\mathbf{H}_{\mathbf{X} }\)</span>矩阵具有行块元素<span class="math inline">\(\mathbf{H}_{\mathbf{X} }^{(j)}, j=1 \ldots L\)</span></li></ul><p>由于特征测量是统计上的，因此噪声向量<span class="math inline">\(\mathbf{n}_{o}^{(j)}\)</span>是不相关的，因此，其协方差矩阵等价于<span class="math inline">\(\mathbf{R}_{o}=\sigma_{\mathrm{im} }^{2} \mathbf{I}_{d}\)</span>，其中 <span class="math inline">\(d=\sum_{j=1}^{L}\left(2 M_{j}-3\right)\)</span> 是残差<span class="math inline">\(\mathbf{r}_{o}\)</span>的维度</p><p>一个实践中的问题是，<span class="math inline">\(d\)</span>可以是一个相当大的数字，例如，如果10个特征点在10个相机位姿中都被观测到，那么残差的维度是170 {(2x10-3)x10=170}.</p><p>为了降低EKF更新的计算复杂度，我们采用QR分解，对<span class="math inline">\(\mathbf{H}_{\mathbf{X} }\)</span>，特别的，我们记分解为如下形式：</p><p><span class="math display">\[\mathbf{H}_{\mathbf{X} }=\left[\begin{array}{ll}\mathbf{Q}_{1} &amp; \mathbf{Q}_{2}\end{array}\right]\left[\begin{array}{c}\mathbf{T}_{H} \\\mathbf{0}\end{array}\right]\]</span></p><p>其中，</p><ul><li>Q1和Q2是关于矩阵<span class="math inline">\(\mathbf{H}_{\mathbf{X} }\)</span>列形式的range和nullspace。</li><li><span class="math inline">\(\mathbf{T}_{H}\)</span>是上三角矩阵</li></ul><p>根据这个定义，等式(25)(<span class="math inline">\(\mathbf{r}_{o}=\mathbf{H}_{\mathbf{X} } \widetilde{\mathbf{X} }+\mathbf{n}_{o}\)</span>) 可以产生如下形式：</p><p><span class="math display">\[\begin{aligned}\mathbf{r}_{o} &amp;=\left[\begin{array}{ll}\mathbf{Q}_{1} &amp; \mathbf{Q}_{2}\end{array}\right]\left[\begin{array}{c}\mathbf{T}_{H} \\\mathbf{0}\end{array}\right] \tilde{\mathbf{X} }+\mathbf{n}_{o} \Rightarrow \\\left[\begin{array}{c}\mathbf{Q}_{1}^{T} \mathbf{r}_{o} \\\mathbf{Q}_{2}^{T} \mathbf{r}_{o}\end{array}\right] &amp;=\left[\begin{array}{c}\mathbf{T}_{H} \\\mathbf{0}\end{array}\right] \widetilde{\mathbf{X} }+\left[\begin{array}{c}\mathbf{Q}_{1}^{T} \mathbf{n}_{o} \\\mathbf{Q}_{2}^{T} \mathbf{n}_{o}\end{array}\right]\end{aligned}\]</span></p><p>从最后一个等式开始，通过投影<span class="math inline">\(\mathbf{H}_{\mathbf{X} }\)</span>范围的基础向量，我们保留了测量中的所有有用信息。</p><p>残差中的<span class="math inline">\(\mathbf{Q}_{2}^{T} \mathbf{r}_{o}\)</span>只是噪声，并且可以完全丢弃。因此，相比于使用等式(25)中的残差表示，我们使用下面形式的残差来执行EKF更新：</p><p><span class="math display">\[\mathbf{r}_{n}=\mathbf{Q}_{1}^{T} \mathbf{r}_{o}=\mathbf{T}_{H} \widetilde{\mathbf{X} }+\mathbf{n}_{n}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{n}_{n}=\mathbf{Q}_{1}^{T} \mathbf{n}_{o}\)</span> 是噪声向量，其协方差矩阵等价于<span class="math inline">\(\mathbf{R}_{n}=\mathbf{Q}_{1}^{T} \mathbf{R}_{o} \mathbf{Q}_{1}=\sigma_{\operatorname{im} }^{2} \mathbf{I}_{r}\)</span>，且r为Q1的列数</li></ul><p>EKF更新步骤计算卡尔曼增益：</p><p><span class="math display">\[\mathbf{K}=\mathbf{P} \mathbf{T}_{H}^{T}\left(\mathbf{T}_{H} \mathbf{P} \mathbf{T}_{H}^{T}+\mathbf{R}_{n}\right)^{-1}\]</span></p><p>并且，矫正的状态按下式给出：</p><p><span class="math display">\[\Delta \mathbf{X}=\mathbf{K} \mathbf{r}_{n}\]</span></p><p>最后，状态的协方差矩阵如下更新：</p><p><span class="math display">\[\mathbf{P}_{k+1 \mid k+1}=\left(\mathbf{I}_{\xi}-\mathbf{K} \mathbf{T}_{H}\right) \mathbf{P}_{k+1 \mid k}\left(\mathbf{I}_{\xi}-\mathbf{K} \mathbf{T}_{H}\right)^{T}+\mathbf{K} \mathbf{R}_{n} \mathbf{K}^{T}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\xi=6N+15\)</span>是协方差矩阵的维度</li></ul><p>审查EKF更新期间所需的操作的计算复杂性很有意思，残差 <span class="math inline">\(\mathbf{r}_{n}\)</span>以及矩阵<span class="math inline">\(\mathbf{T}_{H}\)</span>可以使用 Givens 旋转计算，操作复杂度是<span class="math inline">\(O\left(r^{2} d\right)\)</span>，而无需显式地计算Q1的形式。</p><p>另一方面，等式(31)包含了<span class="math inline">\(\xi\)</span>维度的方阵的乘法计算，是<span class="math inline">\(O\left(\xi^{3}\right)\)</span>的操作。因此，EKF更新的复杂度是<span class="math inline">\(\max \left(O\left(r^{2} d\right), O\left(\xi^{3}\right)\right)\)</span>。</p><p>另一方面，如果使用的残余向量<span class="math inline">\(\mathbf{r}_{o}\)</span>，而不将其投影在<span class="math inline">\(\mathbf{H}_{\mathbf{X} }\)</span>的range内，计算卡尔曼增益的计算成本是<span class="math inline">\(O\left(d^{3}\right)\)</span>，然而，通常<span class="math inline">\(d \gg \xi, r\)</span>，所以可知，使用残差<span class="math inline">\(\mathbf{r}_{n}\)</span>可以减少计算量。</p><h1 id="实验暂略">实验（暂略）</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/24/20210724205124.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;a-multi-state-constraint-kalman-filter-for-vision-aided-inertial-navigation&quot;&gt;A Multi-State Constraint Kalman Filter for Vision-aided
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>LT-mapper论文阅读</title>
    <link href="http://yoursite.com/2021/07/20/LT-mapper%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/07/20/LT-mapper%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-07-20T01:33:53.000Z</published>
    <updated>2021-07-20T01:56:02.029Z</updated>
    
    <content type="html"><![CDATA[<h1 id="lt-mapper-a-modular-framework-for-lidar-based-lifelong-mapping">LT-mapper: A Modular Framework for LiDAR-based Lifelong Mapping</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720094022.png"></p><h1 id="摘要">摘要</h1><p>本文开发了一个开源的、模块化的、现成的、基于lidar的城市站点lifelong mapping</p><p>这是通过将问题划分为连续的子问题来实现的：</p><p>- multi-session SLAM (MSS) - high/low dynamic change detection - positive/negative change management</p><p>所提出的方法利用MSS，并处理潜在的轨迹误差，因此，change检测不需要良好的初始对齐，我们的change管理方案保留了内存和计算成本的有效性，提供了从大规模点云图中自动分离对象的功能。通过对多个时间间隔(从一天到一年)的广泛实际实验，我们验证了该框架的可靠性和适用性，甚至在永久的年水平变化。</p><h1 id="介绍">介绍</h1><p>环境的变化如图1所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720094103.png"></p><p>为了更好地处理这种变化，lifelong mapping必须通过检测、更新和管理环境变化来解决自治的建图维护[1]</p><p>1）Integration to multi-session SLAM for scalability：一些研究认为，变化检测是比较多个预先构建的地图与时间上遥远和独立的后处理过程。在这项工作中，我们集成了多会话SLAM (MSS)，并将会话与锚节点[2]对齐，以在大型城市环境中执行变化检测，而不是在一个小房间，我们的框架包括一个基于激光雷达的多会话三维同步定位和映射(SLAM)模块，称为LT-SLAM。</p><p>2）Change detection under SLAM error：如果地图完全对齐，那么两个地图之间的变化检测就不重要了，早期的地图变化检测工作[5,3,6,7]依赖于全局对齐地图的强假设，没有错误，避免了处理这种模糊性问题。不幸的是，轨迹误差在现实中不可避免地发生。我们在变更检测期间调和了这种潜在的不对准，并使所提出的方法能够稳健地处理潜在的对准误差。为了处理模糊性，我们提出了一种具有投射可见性的scan-to-map方案，使用多个窗口大小的range-image，称为LT-removert</p><p>3）Compact place management：除了变更检测之外，我们还提出并证明了变更组合的概念，一旦检测到更改，就应该遵循地图维护的决定，以确定包含或排除什么，利用这一特性，我们不仅可以维护现有作品[1,3]等最新的地图，还可以提取具有较高placeness的稳定结构。因此，我们构建了一个可靠的3D地图，具有真正有意义的结构，用于其他任务，如跨模式定位[9]和长期定位[10]。这个最后的模块，称为LT-map.</p><p>总结，提出了一个新颖的基于激光雷达的lifelong mapping，称为LT-mapper. 框架中的每一个模块都可独立运行，基于file-based i/o 协议。与最近(但部分)提供的基于视觉的方法不同的是，3D LiDAR几乎没有实现统一和模块化的终身映射[11,12,13,14,15]。据我们所知，LT-mapper是第一个开放的模块化框架，支持基于lidar的复杂城市站点终身绘图，本文主要贡献：</p><p>- LT-SLAM 集成变化检测MSS，通过anchor node来解决会话恢复，只使用激光雷达在共享帧中缝合多个会话。 - LT-removert 利用时空轴上的remove-then-revert算法，克服了会话间的对齐模糊性。 - LT-map 能有效地生成最新地图(实时地图)和持久地图(元地图)，同时将更改存储为增量地图，通过增量建图，减少内存和储存的成本。</p><h1 id="相关工作">相关工作</h1><h1 id="概述">概述</h1><p>LT-mapper是完全模块化的，并支持上述三个功能。整个pipline有3个模块组成(图3)，顺序运行并且模块独立。不像现有的基于激光雷达的变化检测[21]，装备有昂贵的定位设备，我们的系统只需要一个激光雷达（可选的IMU）。</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720094454.png"></p><p>在真实的户外环境中，暂时不连接的场景之间的准确对齐是难以捉摸的，如图2(a)所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720094903.png"></p><p>在LT-SLAM模块中，我们利用多会话SLAM，联合优化多个会话，并使用基于lidar的全局定位器进行鲁棒的会话间闭环检测，在这个模块中，一个查询度量被配准到现有的中心地图中。</p><p>我们同样需要考虑观测的变化，如图2(b)，一个构建的点云图可能包含噪声，由于周围的运动物体(红点)，即使是精确的里程表。这些不稳定的物体对一个地方的显著性的贡献不如静止点，因此，在LT-removert模块中，这些高动态(HD)点应该在计算会话间差异之前预先删除。</p><p>在对齐查询帧和中心会话并移除高清点后，我们通过应用查询测量和中心地图之间的差分操作来检测变化，如图2(c)。我们称之为低动态变化(LD)，进一步分为新出现点(PD)和消失点(ND)两类。</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720095539.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;lt-mapper-a-modular-framework-for-lidar-based-lifelong-mapping&quot;&gt;LT-mapper: A Modular Framework for LiDAR-based Lifelong Mapping&lt;/h1&gt;
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>TEB局部路径规划论文阅读</title>
    <link href="http://yoursite.com/2021/07/19/TEB%E5%B1%80%E9%83%A8%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/07/19/TEB%E5%B1%80%E9%83%A8%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-07-19T09:03:53.000Z</published>
    <updated>2021-07-19T12:51:34.355Z</updated>
    
    <content type="html"><![CDATA[<h1 id="trajectory-modification-considering-dynamic-constraints-of-autonomous-robots">Trajectory modification considering dynamic constraints of autonomous robots</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719174333.png"></p><h1 id="摘要">摘要</h1><p>经典的“松紧带”使由全局规划器生成的路径相对于最短路径长度发生变形，为了避免与障碍物接触。它不直接考虑底层机器人的任何动态约束。本文贡献引入了一种名为“时间弹性带”的新方法，该方法明确地考虑了运动的时间方面的动态约束，如有限的机器人速度和加速度。“时间弹性带”问题用加权多目标优化框架表示。大多数目标是局部的，因为它们依赖于一些邻近的中间配置。这就得到了一个有效的大规模约束最小二乘优化方法存在的稀疏系统矩阵。</p><p>仿真和实际机器人的实验结果表明，该方法具有较好的鲁棒性和计算效率，能够实时生成最优机器人轨迹。“时间弹性带”将由一系列路径点组成的初始路径转换为明确依赖于时间的轨迹，从而实现对机器人的实时控制。由于其模块化的形式，该方法很容易扩展到包含额外的目标和约束。</p><h1 id="介绍">介绍</h1><p>在运动规划的背景下，本文侧重于局部路径修改，假设初始路径已由全局规划器生成[1]。特别是在服务机器人的环境中，由于动态环境可能是动态的，由于其固有的不确定性，修改路径是一种较好的方法。此外，由于局部、不完整的地图和动态障碍，环境模型可能会发生变化，此外，在实时应用中，大规模全局路径的(重新)计算往往是不可行的。这种观测结果导致了局部修改路径的方法，如[2,3]提出的“弹性带”，“松紧带”方法的主要思想是，将原来给定的路径视为受内外力影响的弹性橡皮筋，使其变形，而内外力相互平衡，使路径收缩，同时与障碍物保持一定距离。</p><p>后来这种方法被推广到非完整运动学[4,5,6]、多自由度[7]机器人系统和动态障碍[8]，然而，据我们所知，动态运动约束还没有被认为是对路径变形的一个目标。典型的方法是用样条曲线平滑路径，获得动态可行轨迹。</p><p>我们的方法称为“时间弹性带”，是一种新颖的方法，因为它明确地增加了“弹性带”的时间信息，从而允许考虑机器人的动态约束和直接修改轨迹而不是路径。图1展示了使用时间弹性带架构的机器人系统：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719175444.png"></p><p>通过考虑时间信息，“时间弹性带”也可以用来控制机器人的速度和加速度，这个方法也适用于高维的状态空间，尽管本文只考虑了差分驱动机器人的平面环境移动，有三个全局自由度和两个局部自由度。</p><h1 id="时间弹性带">时间弹性带</h1><p>经典的“弹性带”是用n个机器人的中间姿态序列来描述的，<span class="math inline">\(\mathbf{x}_{i}=\left(x_{i}, y_{i}, \beta_{i}\right)^{T} \in \R^{2} \times S^1\)</span>，下面记为位置(x_i,y_i)和旋转<span class="math inline">\(\beta_i\)</span>，如图2所示:</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719191833.png"></p><p><span class="math display">\[Q=\left\{\mathbf{x}_{i}\right\}_{i=0 \ldots n} \quad n \in \mathbb{N}\]</span></p><p>TEB由两个连续的配置之间的时间间隔来进行时间调整，因此一个序列中包含<span class="math inline">\(n-1\)</span>个<span class="math inline">\(\Delta T_i\)</span>：</p><p><span class="math display">\[\tau=\left\{\Delta T_{i}\right\}_{i=0 \ldots n-1}\]</span></p><p>每个时间差表示机器人从一个配置依次过渡到下一个配置的时间(图2)，因此TEB定义为元祖序列：</p><p><span class="math display">\[B:=(Q, \tau)\]</span></p><p>其关键思想是通过实时加权多目标优化，在配置和时间间隔两个方面进行调整和优化：</p><p><span class="math display">\[\begin{aligned}f(B) &amp;=\sum_{k} \gamma_{k} f_{k}(B) \\B^{*} &amp;=\underset{B}{\operatorname{argmin}} f(B)\end{aligned}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(B*\)</span>表示优化的TEB</li><li><span class="math inline">\(f(B)\)</span>记为目标函数，在本文中由多个加权成分<span class="math inline">\(f_k\)</span>组成，用于面对不同的方面，这是最基本的多目标优化方法，但它已经产生了非常好的结果。</li></ul><p>目标函数的大部分分量相对于B是局部的，因为它们只依赖于几个连续的配置，而不是整个可配置空间带。</p><p>TEB的这种局部性导致了一个稀疏系统矩阵，为其提供了专门的快速有效的大规模数值优化方法[11]。</p><p>TEB的目标函数分为两类：</p><ul><li>约束例如速度、加速度限制等惩罚项</li><li>目标项如最快、最短路径或者远离障碍等(等式8)</li></ul><p>因此，在“时间弹性带”的背景下，这些约束被表述为一个分段连续、可微的代价函数的目标，该函数会惩罚违反约束的行为：</p><p><span class="math display">\[e_{\Gamma}\left(x, x_{r}, \epsilon, S, n\right) \simeq \begin{cases}\left(\frac{x-\left(x_{r}-\epsilon\right)}{S}\right)^{n} &amp; \text { if } x&gt;x_{r}-\epsilon \\ 0 &amp; \text { otherwise }\end{cases}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(x_r\)</span>表示边界</li><li><span class="math inline">\(S,n,\epsilon\)</span> 影响近似的准确性</li><li>特别的，<span class="math inline">\(S\)</span>表示尺度缩放，<span class="math inline">\(n\)</span>表示阶数，<span class="math inline">\(\epsilon\)</span>是近似的一个位移小量</li></ul><p>图3展示了等式6的两个不同的实现。 Approximation 1 （n = 2, S = 0.1, <span class="math inline">\(\epsilon\)</span>= 0.1） ， Approximation 2 （n = 2, S = 0.05 and <span class="math inline">\(\epsilon\)</span> = 0.1） ， 这个例子展示了约束<span class="math inline">\(x_r=0.4\)</span>的一个近似。</p><p>使用多目标优化框架的一个明显优势是目标函数的模块化表达。目前TEB所采用的目标函数如下：</p><h2 id="way-points-and-obstacles">Way points and obstacles</h2><p>TEB同时考虑原始路径的中间路径点的到达和避免静态或者动态的障碍物。这两个目标函数相似，不同之处在于点吸引橡皮筋，而障碍物排斥它。</p><p>目标函数企图最小化TEB和way point的距离<span class="math inline">\(d_{min,j}\)</span>，如图4所示：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719194704.png"></p><p>对于way point的情况，其距离以最大的目标半径<span class="math inline">\(r_{pmax}\)</span>为界，这些约束由公式6中的惩罚函数实现：</p><p><span class="math display">\[\begin{aligned}f_{\text {path }} &amp;=e_{\Gamma}\left(d_{\min , j}, r_{p_{\max }}, \epsilon, S, n\right) \\f_{o b} &amp;=e_{\Gamma}\left(-d_{\min , j},-r_{o_{\min }}, \epsilon, S, n\right)\end{aligned}\]</span></p><p>由图3可知，必须将Eq. 8中<span class="math inline">\(d_{\min , j}\)</span>和<span class="math inline">\(r_{o_{min}}\)</span>交换符号来实现下界.</p><p>注意，这些目标函数的梯度可以解释为作用在弹性带上的外力</p><h2 id="velocity-and-acceleration">Velocity and acceleration</h2><p>机器人速度和加速度的动力学约束用与几何约束相似的罚函数来描述，图2展示了TEB的结构，线速度和角速度的均值使用两个连续配置<span class="math inline">\(x_i,x_{i+1}\)</span>之间的欧式距离和角距离和时间差<span class="math inline">\(\Delta T_i\)</span>来计算：</p><p><span class="math display">\[\begin{aligned}v_{i} &amp; \simeq \frac{1}{\Delta T_{i}}\left\|\left(\begin{array}{l}x_{i+1}-x_{i} \\y_{i+1}-y_{i}\end{array}\right)\right\| \\\omega_{i} &amp; \simeq \frac{\beta_{i+1}-\beta_{i}}{\Delta T_{i}}\end{aligned}\]</span></p><p>由于配置的临近，欧几里得距离是两个连续姿态之间的圆路径的真实长度的充分近似值。</p><p>加速度涉及两个连续的平均速度，因此考虑三个连续的构型，其中两个对应时间差：</p><p><span class="math display">\[a_{i}=\frac{2\left(v_{i+1}-v_{i}\right)}{\Delta T_{i}+\Delta T_{i+1}}\]</span></p><p>为了清楚起见，用上式两个相关的速度来代替这三个连续的配置，旋转的加速度计算类似。</p><p>考虑一个差分驱动的移动机器人，轮速和位移<span class="math inline">\(v_i\)</span>、旋转速度<span class="math inline">\(w_i\)</span>关于机器人中心点的关系如下：</p><p><span class="math display">\[\begin{aligned}v_{w_{r}, i} &amp;=v_{i}+L \omega_{i} \\v_{w_{l}, i} &amp;=v_{i}-L \omega_{i}\end{aligned}\]</span></p><p>其中，L为机器人轮距的一半</p><p>将公式12和公式13（即上面两个式子）对时间进行微分就得到了相应的车轮加速度。车轮的速度和加速度是有界的，可根据制造商的规格获取。机器人的平移和转动惯量可以以一种明显的方式包括在内，但在这第一个实现中，我们还没有这样做。</p><h2 id="non-holonomic-kinematics非完整约束运动学方程">Non-holonomic kinematics（非完整约束运动学方程）</h2><p>差动驱动的机器人只有两个局部自由度，因此，它们只能在机器人当前航向的方向执行运动，这种运动学约束导致了由圆弧段组成的平滑路径。</p><p>因此，两个相邻的构型需要位于一个常曲率的公共圆弧上，如图5所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719200953.png"></p><p>初始配置<span class="math inline">\(x_i\)</span>和方向<span class="math inline">\(d_{i,i+1}\)</span>之间的角度<span class="math inline">\(\vartheta_{i}\)</span>必须等于配置<span class="math inline">\(x_{i+1}\)</span>和方向<span class="math inline">\(d_{i,i+1}\)</span>之间的夹角，即：</p><p><span class="math display">\[\vartheta_{i}=\vartheta_{i+1}\]</span></p><p>根据二维叉积（<span class="math inline">\(A \times B=|A|·|B|·\sin \alpha\)</span>），有：</p><p><span class="math display">\[\Leftrightarrow\left(\begin{array}{c}\cos \beta_{i} \\\sin \beta_{i} \\0\end{array}\right) \times \mathbf{d}_{i, i+1}=\mathbf{d}_{i, i+1} \times\left(\begin{array}{c}\cos \beta_{i+1} \\\sin \beta_{i+1} \\0\end{array}\right)\]</span></p><p>其中，机器人的绝对旋转为<span class="math inline">\(\beta_i\)</span>，位移方向向量为：</p><p><span class="math display">\[\mathbf{d}_{i, i+1}:=\left(\begin{array}{c}x_{i+1}-x_{i} \\y_{i+1}-y_{i} \\0\end{array}\right)\]</span></p><p>因此，对应的目标函数为：</p><p><span class="math display">\[f_{k}\left(\mathbf{x}_{i}, \mathbf{x}_{i+1}\right)=\left\|\left[\left(\begin{array}{c}\cos \beta_{i} \\\sin \beta_{i} \\0\end{array}\right)+\left(\begin{array}{c}\cos \beta_{i+1} \\\sin \beta_{i+1} \\0\end{array}\right)\right] \times \mathbf{d}_{i, i+1}\right\|^{2}\]</span></p><p>惩罚违反此约束的二次误差。一个潜在的180方向的变化用一个额外的项来处理</p><h2 id="fastest-path">Fastest path</h2><p>以往的“松紧带”方法通过收缩松紧带的内力获得最短路径。由于我们的方法考虑时间信息作为最短路径的目标，我们可以选择用最快路径的目标代替最短路径的目标，或者将这些目标结合起来。</p><p>最快路径的目标很容易通过最小化所有时间差的和的平方来实现：</p><p><span class="math display">\[f_{k}=\left(\sum_{i=1}^{n} \Delta T_{i}\right)^{2}\]</span></p><p>这一目标导致了一种最快的路径，其中中间配置在时间上均匀分离，而不是在空间上</p><h2 id="实现">实现</h2><p>图6展示了实现TEB的控制流程，在初始化阶段，初始路径被增强为初始轨迹，方法是根据动力学和运动学约束添加默认的时间信息。</p><p>在我们的例子中，初始轨迹是由带有纯旋转和平移的分段线性分段组成的，这种以多边形表示的路径通常由概率路线图规划者提供[9]，另外，reed - shepp路径很容易被增强为允许的轨迹[10]。</p><p>在每一次迭代中，算法动态地添加新的结构或删除以前的结构，以调整空间和时间分辨率以适应剩余的轨迹长度或规划水平。</p><p>一个滞后被实施以避免振荡。将优化问题转化为一个超图，用包含在“g20 -框架”中的大规模稀疏系统优化算法求解[11]</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719204210.png"></p><p>所要求的超图是一条边的连接节点数量不受限制的图,因此，一条边可以连接两个以上的节点。</p><p>TEB问题(Eq. 4)可以转化为一个以配置和时间差为节点的超图。它们与表示给定目标函数fk或约束函数的边相连，图7展示了一个两配置一个时间差和一个点状障碍物的超图，</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719204524.png"></p><p>速度边界目标函数要求的平均速度与两个配置之间的欧氏距离和所需的时间有关。因此它形成一条连接B的那些状态的边。</p><p>障碍物需要一条与最近的配置相连的边，表示障碍物的节点是固定的(双圆)，因此优化算法无法改变其参数(位置)</p><p>在验证优化后的TEB后，可以通过计算控制变量v和ω来直接命令机器人驱动系统。</p><p>在每一次新的迭代之前，重新初始化阶段将检查新的和变化的way-points，这将会比较有用如果way-points是在分析相机或者激光数据之后才收到的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;trajectory-modification-considering-dynamic-constraints-of-autonomous-robots&quot;&gt;Trajectory modification considering dynamic constraint
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>Hector-SLAM论文阅读</title>
    <link href="http://yoursite.com/2021/07/11/Hector-SLAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/07/11/Hector-SLAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-07-11T06:03:53.000Z</published>
    <updated>2021-07-11T13:56:26.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="a-flexible-and-scalable-slam-system-with-full-3d-motion-estimation">A Flexible and Scalable SLAM System with Full 3D Motion Estimation</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/11/20210711214658.png"></p><h1 id="摘要">摘要</h1><p>在许多应用场景中，比如城市搜救和搜索（USAR）机器人，需要去获取未知环境的地图。我们提出了一个快速在线学习占用栅格地图、占用较少计算资源的系统。它利用激光雷达系统与基于惯性传感器的3D位姿估计系统进行融合，实现了一种鲁棒的扫描匹配方法。通过地图变化的快速近似和多分辨率栅格地图，在各种有挑战性的环境中实现了可靠的定位与建图。提供了多种数据集以适应嵌入式手持建图系统。我们表明，该系统是足够准确的，在我们考虑的应用场景中，不需要显式闭环检测技术。该软件可作为ROS的开源代码包。</p><h1 id="介绍">介绍</h1><p>学习环境模型并定位自身是一个真正的机器人在真实世界运行的最重要的能力。在本文中，我们提出了一种灵活的、可升级的系统来解决SLAM问题，已成功的运用在了UGV、USV和一个小型的室内导航系统上。该方法消耗的计算资源较少，故可以应用于低成本、低功耗的嵌入式系统。该方法是在ROS上实现的开源软件。它适应ROS上的API和导航stack，并可以在ROS的生态中替代其他SLAM方法。</p><p>本文介绍的系统旨在保证计算力要求低的前提下，实现足够精确的环境感知和自我定位。它可以应用在小尺度的、不必做大的闭环的系统中，并需要使用高更新速率的激光雷达系统。类似的场景包括RoboCup搜救比赛，可能需要在模拟的地震场景中找到受害者，因此需要对车辆在6Dof上进行姿态估计。或者，比地面机器人更灵活的室内飞行器的导航。有关USAR的结果和模型在参考【2】中可以找到。</p><p>我们的方法结合了2D SLAM（基于激光雷达的平面地图）和3D导航（基于IMU）融合了2D的SLAM信息作为辅助（FIG.I）。SLAM过程通常是由激光雷达的数据更新来触发的，而整个3D导航解决方案是需要实时计算的，构成车辆控制系统的一部分。</p><h1 id="相关工作">相关工作</h1><p>近些年已经有大量的研究关于SLAM的问题，例如作为开源软件的gmapping使用的是Rao-Blackwellized粒子过滤器，可以可靠在的典型办公室室内场景使用。然而，这些解决方案的工作最好在平面环境，依赖于现有的，足够精确的航迹以及不利用现代雷达系统提供的高更新率。对于非结构化环境，会导致载体的显著滚转和俯仰运动，或在空中平台上实现这种系统不适用或必须进行显著修改。</p><p>一个SLAM的前端和后端系统之间的区别。在大满贯的前端，用于实时在线估计机器人运动，后端用于优化位姿图，和在使用的前端产生的位姿之间的约束。本文提出的方法可以作为一个SLAM的前端和不提供姿势图优化像[ 4 ]和[ 5 ]提出的解决方案。然而，我们表明，在许多情况下，这种优化并不需要在现实中的一些条件，因为这种方法是足够准确的，可用于机器人来执行他们的任务。</p><p>基于激光扫描匹配的室内导航系统提出了对旋翼无人机使用[ 6 ]、[ 7 ]、[ 8 ]。在这里，采用两阶段的方法，前端快速扫描的位姿估计，和用于在后台或远程计算机上进行的较慢的后端建图步骤。从雷达扫描对准的位姿估计不直接纳入车辆的控制回路，因此他们只在低速行驶。</p><p>在[ 9 ]和[ 10 ]中描述了移动机器人上使用的其他前端系统。在本文的对比，他们没有提供完整的六自由度位姿估计和开源软件。</p><p>使用扫描匹配进行定位的工作始于ICP[ 11 ]，它起源于注册三维点云的一般方法。许多基于ICP的方法的主要缺点是对点对应的高复杂度搜索，这必须在每次迭代中进行。极坐标扫描匹配（PSM）[ 12 ]避免了利用激光扫描的自然极坐标系统来估计它们之间的匹配的对应搜索。扫描进行预处理，可用于极性扫描匹配。实时相关扫描匹配方法[ 13 ]采用穷举抽样方法进行扫描匹配。通过多种优化，这种方法能够实时应用。基于正态分布变换（NDT）[ 14 ]的扫描匹配将扫描对齐到代表前扫描的正态分布混合。</p><p>对于沿海的情况，有研究使用昂贵的多传感器扫描仪[ 15 ]，但据笔者的知识，没有单发射器激光雷达为基础的SLAM方法，可在现实世界中的条件下进行测试。</p><h1 id="系统概述">系统概述</h1><p>相对许多其他基于网格的2D-SLAM来说，本文提供了一种可用的，具有完整的6自由度运动的平台。我们的系统可以预测6自由度的平移和旋转状态。为了实现这一点，我们的系统由两个主要部分组成，导航滤波器融合了来自于惯性传感器和其他可用的传感器到一个可用的3D数据，而2D SLAM则提供平面的位姿信息。这两部分的更新都是单独的，是松耦合系统，他们会定时保持同步。</p><p>我们定义导航坐标系统是一个右手系统，它的原点在平台的起点上，Z轴指向上方，X轴在启动时指向平台的朝向。</p><p>完整的3D状态表示为<span class="math inline">\(\mathbf{x}=\left(\begin{array}{lll}\mathbf{\Omega}^{\mathrm{T}} &amp; \mathbf{p}^{\mathrm{T}} &amp; \mathbf{v}^{\mathrm{T}}\end{array}\right)^{\mathrm{T}}\)</span>，其中，<span class="math inline">\(\mathbf{\Omega}=(\phi, \theta, \psi)^{\mathrm{T}}\)</span>表示欧拉角的roll，pitch和yaw，<span class="math inline">\(\mathbf{p}=\left(p_{x}, p_{y}, p_{z}\right)^{\mathrm{T}}\)</span>和<span class="math inline">\(\mathbf{v}=\left(v_{x}, v_{y}, v_{z}\right)^{\mathrm{T}}\)</span>分别表示在导航坐标系下的位置和速度。</p><p>惯性测量使用<span class="math inline">\(\mathbf{u}=\left(\begin{array}{ll}\omega^{T} &amp; \mathbf{a}^{T}\end{array}\right)^{T}\)</span>来表示，其中<span class="math inline">\(\mathbf{a}=\left(a_{x}, a_{y}, a_{z}\right)^{\mathrm{T}}\)</span>和<span class="math inline">\(\mathbf{w}=\left(w_{x}, w_{y}, w_{z}\right)^{\mathrm{T}}\)</span>分别表示加速度和角速度。任意刚体的运动可以表示为如下非线性微分方程（简化版）：</p><p><span class="math display">\[\begin{aligned}\dot{\mathbf{\Omega}} &amp;=\mathbf{E}_{\Omega} \cdot \boldsymbol{\omega} \\\dot{\mathbf{p}} &amp;=\mathbf{v} \\\dot{\mathbf{v}} &amp;=\mathbf{R}_{\Omega} \cdot \mathbf{a}+\mathbf{g}\end{aligned}\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;a-flexible-and-scalable-slam-system-with-full-3d-motion-estimation&quot;&gt;A Flexible and Scalable SLAM System with Full 3D Motion Estimati
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>T-LOAM论文阅读</title>
    <link href="http://yoursite.com/2021/06/11/T-LOAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/06/11/T-LOAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-06-11T06:03:53.000Z</published>
    <updated>2021-06-14T10:18:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="t-loam-truncated-least-squares-lidar-only-odometry-and-mapping-in-real-time">T-LOAM: Truncated Least Squares LiDAR-Only Odometry and Mapping in Real Time</h1><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611150206.png"></p><h1 id="摘要">摘要</h1><p>提出了一个基于截断最小二乘法的新颖的、计算效率高、鲁棒的纯激光里程计。我们的方法侧重于减轻异常值的影响，允许在退化发生的稀疏，嘈杂或杂乱的情况下允许强大的导航。</p><p>作为预处理，使用了多区域的地面提取，动态曲率体素的聚类方法来完成3D点云的分割和滤除不稳定目标的工作。</p><p>提出一个新颖的特征提取模块，用于区分：边缘特征、球形特征、平面特征、地面特征。</p><p>作为前端，是一个基于分层特征的纯激光雷达里程计，通过截断最小二乘法来直接处理不同的特征以进行精确地运动估计。预处理模型和运动估计精度已经在 KITTI 里程计基准以及各种校园场景中进行了评估。 实验结果证明了所提出的<code>T-LOAM</code>的实时能力和优越的精度优于其他最先进的算法。</p><h2 id="变量约定">变量约定</h2><ul><li><p><code>DCVC</code>聚类: Dynamic curved-voxel clustering</p></li><li><p><code>MRGE</code>地面提取：Dultiregion ground extraction</p></li><li><p><code>TLS</code>截断最小二乘：Truncated least squares</p></li><li><p><span class="math inline">\(t_k\)</span>: 第k帧激光扫描结束的时间</p></li><li><p><span class="math inline">\(L_k\)</span>: 时间<span class="math inline">\(t_k\)</span>对应的LiDAR body坐标系</p></li><li><p><span class="math inline">\(Q_i,S_j\)</span>: <code>MRGE</code>的第i个象限，第j部分</p></li><li><p><span class="math inline">\(\mathbb{F}_{k}^{v}, \mathbb{F}_{k}^{b}\)</span>: 时间<span class="math inline">\(t_k\)</span><code>MRGE</code>的前景和背景</p></li><li><p><span class="math inline">\(\mathbb{F}_{k}^{d}\)</span>： 在时间<span class="math inline">\(t_k\)</span>由<span class="math inline">\(\mathbb{F}_{k}^{v}\)</span>输出的<code>DCVC</code>模型</p></li><li><p><span class="math inline">\(\mathbb{F}_{k}\)</span>: 在时间<span class="math inline">\(t_k\)</span>对应<span class="math inline">\(L_k\)</span>坐标系的所有特征</p></li><li><p><span class="math inline">\(F_{k}^{e}, F_{k}^{s}, F_{k}^{p}, F_{k}^{g}\)</span>：边缘、球形、平面、地面特征</p></li><li><p><span class="math inline">\(\mathbb{M} \mathbb{l}_{k}^{w}\)</span>：时间<span class="math inline">\(t_k\)</span>的世界坐标系中的子图第k特征点</p></li><li><p><span class="math inline">\(x_{k}^{w}\)</span>：时间<span class="math inline">\(t_k\)</span>对应的机器人在全局坐标系的状态</p></li></ul><h1 id="介绍">介绍</h1><p>在本文中，已经实施了一系列改进，以解决当前激光SLAM框架中存在的缺陷。 截断最小二乘（TLS）方法创新的应用于扫描匹配，使<code>T-LOAM</code>对于异常值更加鲁棒，并减轻其对初始值解决方案的依赖性。 原始点云通过精心设计的预处理模块进行了分割，以提取四个独特的功能：边缘特征，球形特征，平面特征和地面地面。 为了实现更加平滑的里程计，通过TLS方法构建了与不同特征相关的三种残差函数。 最后，构建如图1所示全局一致的地图。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613122201.png"></p><p>使用了KITTI Odomerty数据集来与其他state-of-art的方法相比，如<code>F-LOAM</code>,<code>A-LOAM</code>,<code>SuMa</code>等，实验结果表明<code>T-LOAM</code>在大多数序列实验场景中达到更优越的性能。</p><p>本文主要贡献：</p><ul><li>提出一个基于截断最小二乘TLS的高效率鲁棒激光里程计，用于进行导航和构建全局一致性高的点云地图。</li><li>利用MRGE地面提取和DCVC聚类来实现一系列预处理步骤，提高点云分割的精度</li><li>首次使用<code>Open3D</code>进行激光SLAM框架的开发，并开源</li></ul><p>本方法受<code>graduated nonconvexity(GNC)</code>方法的启发，这在计算机视觉字段中执行了各种匹配和优化任务，已经证明有效。根据Convex优化理论基于TLS构建一种新颖的姿态优化函数，以估算6自由度的变换。</p><p>该实现遵循如图2所示，并通过多线程和OpenMP [28]设置并行化以保证算法的整体运行效率，另外还使用了ROS社区的<code>nodelet</code>功能包来实现<code>Zero copy</code>.</p><h1 id="数据集与系统硬件介绍">数据集与系统硬件介绍</h1><p>本文中使用的TX2机器人是远程化的车辆,这是由双无刷电机驱动，由670-WH电池供电。LIDAR的安装高度设置为0.75米，以保证全向扫描。 通过遥控记录实验数据集。 此外，LIDAR采样频率设定为10 Hz。</p><p>采用两台计算机评估所提出的框架的实时能力，包括NVIDIA Jetson TX2(ARM Cortex-A57 CPU)以及2.5-GHz i7-10750H CPU的笔记本电脑。</p><p>所有模块由C ++实现，并将Nodelet包作为Ubuntu 18.04 Linux中的唯一节点集成到ROS [29]中，以完成它们之间的零拷贝传输。 本文中提出的实验仅利用这些系统中的CPU运行。</p><h1 id="预处理模块">预处理模块</h1><h2 id="框架总览">框架总览</h2><p><code>T-LOAM</code>框架如图2所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613122918.png"></p><p>首先，对激光扫描进行关于旋转的畸变矫正，然后，将点云输入到<code>MRGE</code>模块中以获取前景<span class="math inline">\(\mathbb{F}_{k}^{v}\)</span>和背景<span class="math inline">\(\mathbb{F}_{k}^{b}\)</span>。随后，使用<code>DCVC</code>方法来对前景进行分割，并滤除不稳定的类别，得到<span class="math inline">\(\mathbb{F}_{k}^{d}\)</span>。</p><p>此外，通过特征提取模块来分别得到特征点<span class="math inline">\(\mathbb{F}_{k}=\left\{F_{k}^{e}, F_{k}^{s}, F_{k}^{p}, F_{k}^{g}\right\}\)</span>，通过预处理扫描，关联的特征点通过位姿优化模块配准到子图上，得到全局一致性地图。此外，基于连续的激光扫描位姿优化得到的平移，将用于对当前帧扫描特征点云进行平移量的矫正。</p><h2 id="多区域地面提取">多区域地面提取</h2><p>地面点云通常占据了车载激光雷达扫描的大部分，并通常具有较简单的数学模型，有利于处理。</p><p>特别的，这些特征可以直接用于构造位姿优化的约束。但是，仅使用单个平面模型不足以准确表示复杂地形中的分布，因此，采用了<code>MRGE</code>方法来提高分割精度。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613224514.png"></p><p>在算法一中，原始的激光扫描首先根据极坐标分为多个象限（默认是4），如图4(a)所示。其次，对于每一个象限，都将继续分为多个子区域（默认为3），如图4(b)所示，其中，每个子区域的边界如下计算：</p><p><span class="math display">\[\theta_{i}=\theta_{s}+\frac{n}{b} \alpha \cdot k_{i}\]</span></p><p><span class="math display">\[\lambda_{i}=\left\{\begin{array}{ll}h\left(\frac{1}{\tan \left(\theta_{i}\right)}\right), &amp; \text { if } i \geq 1 \\0, &amp; \text { if } i=0\end{array}\right.\]</span></p><p>其中，<span class="math inline">\(\theta_s\)</span>是激光扫描的起始仰角，<span class="math inline">\(b\)</span>是子区域数，<span class="math inline">\(n\)</span>是可能包含地面点的激光线束数总和。特别的，<span class="math inline">\(\alpha\)</span>代表激光雷达的垂直角分辨率，<span class="math inline">\(h\)</span>表示激光雷达的安装高度，<span class="math inline">\(k,i\)</span>分别表示区域的系数和索引。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613175331.png"></p><p>对于自动驾驶车辆来说，地面点的高度坐标分量通常位于最低位置，因此，可以利用这个先验信息来根据高度值对区域点云进行整理。</p><p>种子点在指定的阈值<span class="math inline">\(\tau\)</span>内进行选择，主要用于拟合初始的平面模型。为了提高模型精度，多轴线性回归方法量身定制以计算相关参数，主要方向被加权，以减轻异常值的影响。线性平面模型被量身定制以反映子区域的分布，通过如下：</p><p><span class="math display">\[\begin{aligned}a x+b y+c z+d &amp;=0 \\n^{T} \mathrm{p} &amp;=-d\end{aligned}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(n=\left[\begin{array}{lll}a &amp; b &amp; c\end{array}\right]^{T}\)</span>，表示平面法向量</li><li><span class="math inline">\(\mathrm{p}=\left[\begin{array}{lll}x &amp; y &amp; z\end{array}\right]^{T}\)</span>，表示平面上的点</li></ul><p>特别的，协方差矩阵M用于获取对应的每个子区域中指定的种子点的分散性：</p><p><span class="math display">\[\mathcal{M}=\sum_{i=1}^{|s|}\left(s_{i}-\bar{s}\right)\left(s_{i}-\bar{s}\right)^{T}=\left(\begin{array}{lll}a_{1} &amp; a_{2} &amp; a_{3} \\b_{1} &amp; b_{2} &amp; b_{3} \\c_{1} &amp; c_{2} &amp; c_{3}\end{array}\right)\]</span></p><p>其中，<span class="math inline">\(\bar{s} \in \mathbb{R}^{3}\)</span>记为子区域中的所有点<span class="math inline">\(s_{i} \in S\)</span>的均值。</p><p>下一步，计算出三个主方向：</p><p><span class="math display">\[\begin{array}{l}v_{x}=\left[\begin{array}{l}b_{2} c_{3}-b_{3} b_{3} \\a_{3} b_{3}-a_{2} c_{3} \\a_{2} b_{3}-a_{3} b_{2}\end{array}\right], \quad v_{y}=\left[\begin{array}{l}a_{3} b_{3}-a_{2} c_{3} \\a_{1} c_{3}-a_{3} a_{3} \\a_{2} a_{3}-a_{1} b_{3}\end{array}\right] \\v_{z}=\left[\begin{array}{l}a_{2} b_{3}-a_{3} b_{2} \\a_{2} a_{3}-a_{1} b_{3} \\a_{1} b_{2}-a_{2} a_{2}\end{array}\right]\end{array}\]</span></p><p>为了减轻异常值对法向量估计的影响，每个主方向加权，并应用线性回归来细化正常向量。 权重值是所示的每个主方向的二范数，如下：</p><p><span class="math display">\[\begin{aligned}n &amp;=\sum_{k \in\{x, y, z\}} w_{k} v_{k} \\w_{x} &amp;=v_{x}[0]^{2}, \quad w_{y}=v_{y}[1]^{2}, w_{z}=v_{z}[2]^{2}\end{aligned}\]</span></p><p>其中，<span class="math inline">\([]\)</span>操作符表示取该向量对应的元素</p><p>参数<span class="math inline">\(d\)</span>可以根据平面方程和<span class="math inline">\(p,\bar{s}\)</span>计算获得，即当法向量<span class="math inline">\(n\)</span>获取后，将一个点代入最终的平面模型即可计算得到。</p><h2 id="动态曲率体素聚类">动态曲率体素聚类</h2><p>为了准确高效对点云进行分割，提出了<code>DCVC</code>方法。改进的空间体素类型满足[24]中描述的三个重要方面，以及与点云的空间分布更加对应。</p><ul><li>定义1： 第i，j，k个动态弯曲的voxel表示一个3D空间的体素单元，其根据配置可以如下计算：</li></ul><p><span class="math display">\[\begin{aligned}D C V_{i, j, k}=\left\{P(\rho, \theta, \phi)=\mid \rho_{i}\right.&amp; \leq \rho&lt;\rho_{i}+\Delta \rho_{i} \\\theta_{j} &amp; \leq \theta&lt;\theta_{j}+\Delta \theta_{j} \\\phi_{k} &amp;\left.\leq \phi&lt;\phi_{k}+\Delta \phi_{k}\right\}\end{aligned}\]</span></p><p>其中，每个<span class="math inline">\(P(\rho, \theta, \phi)\)</span>是极坐标系中的极径<span class="math inline">\(\rho\)</span>、方位角（俯仰）<span class="math inline">\(\theta\)</span>，极角（水平）<span class="math inline">\(\phi\)</span>。特别的，<span class="math inline">\(\Delta \rho_i,\Delta \theta_j, \Delta \phi_k\)</span>表示根据点云的稀疏性和距离值调整的每个体素单元的边界上下界之差（即体素的尺寸）。</p><p>另外，每个方向上的体素的增量可以计算如下：</p><p><span class="math display">\[\begin{array}{l}\Delta \rho_{i}=\rho_{s}-\left(a \times s_{i}+b\right) \times \Delta \rho \\\Delta \theta_{j}=s_{j} \times \Delta \theta \\\Delta \phi_{k}=s_{k} \times \Delta \phi\end{array}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\rho_s\)</span>表示起始的体素极径</li><li><span class="math inline">\(s\)</span>表示步长</li><li><span class="math inline">\(\Delta \rho,\Delta \theta, \Delta \phi\)</span>分别对应极径、方位角、极角的增量</li><li><span class="math inline">\(a,b\)</span>是体素系数，其可以根据各种光束的点云和3D LIDAR的密度来确定</li></ul><p>为了更好地理解实际分布，在图5中可视化两个相邻的空间动态体素。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613220812.png"></p><blockquote><p>这个图右图有点问题，图中的z轴应该是y轴才对，因为右图是俯视图</p></blockquote><p>稀疏系数可用于调节极径方向上不同距离的体积，这有利于防止对空间中相邻物体区分失败。</p><p><code>DCVC</code>方法总结成算法2。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613232018.png"></p><p>我们首先将笛卡尔坐标系的点云转换到极坐标系中，同时建立弯曲的体素。</p><p>然后，将非空的动态体素构建成哈希表，以提高搜索效率。</p><p>随后，根据哈希表映射关系，搜索当前点所在体素的周围体素，并将它们合并到统一标签中。</p><p>最后，我们返回每个点的类别信息，并提供合成判断，以滤除一组或潜在的动态目标，以获得最终的点云<span class="math inline">\(\mathbb{F}_{k}^{d}\)</span>。以这种方式，当各种对象彼此依然相邻时，显著抑制了不正确的分割分类的发生。</p><h2 id="特征提取">特征提取</h2><p>很明显，<code>LOAM</code>[10]中呈现的算法容易受几何退化场景的影响，这将直接衰减激光雷达测量仪的精度甚至使其失败。</p><p>为了提高纯激光雷达里程计的稳定性，<code>T-LOAM</code>将从<span class="math inline">\(\mathbb{F}_{k}^{d}\)</span>和<span class="math inline">\(\mathbb{F}_{k}^{b}\)</span>提取4个可区分的几何特征，包括边缘特征<span class="math inline">\(F_{k}^{e}\)</span>，球面特征<span class="math inline">\(F_{k}^{s}\)</span>，平面特征<span class="math inline">\(F_{k}^{p}\)</span>和地面特征<span class="math inline">\(F_{k}^{g}\)</span>。</p><p>背景<span class="math inline">\(\mathbb{F}_{k}^{b}\)</span>首先进行降采样，边缘则使用<code>LOAM</code>[10]方法，根据平滑度提取：</p><p><span class="math display">\[c=\frac{1}{|s| \cdot\left\|p_{i}\right\|}\left\|\sum_{m \in s, m \neq i}\left(p_{m}-p_{i}\right)\right\|\]</span></p><p>其中，<span class="math inline">\(s\)</span>表示同一激光束的连续10个点的集合，即包含点<span class="math inline">\(p_i\)</span>左右两侧的5个点</p><p>另外，球面特征<span class="math inline">\(F_{k}^{s}\)</span>和平面特征<span class="math inline">\(F_{k}^{p}\)</span>的垂直部分可以通过PCA算法获取。它用于捕捉局部领域的描述，包括曲率、主方向、次方向、法向量以及相应的特征值。为了实现上述，需要计算的协方差矩阵如下计算：</p><p><span class="math display">\[\mathcal{M}=\frac{1}{k} \sum_{i=1}^{k}\left(p_{i}-\overline{p_{k}}\right)\left(p_{i}-\overline{p_{k}}\right)^{T}\]</span></p><p>其中，<span class="math inline">\(k\)</span>表示总的点数，<span class="math inline">\(\bar{p_k}\)</span>表示点集的均值。</p><p>进一步的，对协方差矩阵M执行了SVD分解，以获取特征值<span class="math inline">\(\lambda\)</span>和特征向量<span class="math inline">\(v\)</span>。<strong>可以通过协方差矩阵的特征向量来区分各种特征，这些特征向量与最显著的数据方差方向相关联</strong>。一些分散点主要根据特征值近似平等的方法来进行过滤。然后，点云的平坦度<code>flatness</code>和球面度<code>sphericity</code>[30]可以计算如下：</p><p><span class="math display">\[\begin{aligned}\gamma &amp;=\frac{\lambda_{3}}{\lambda_{1}+\lambda_{2}+\lambda_{3}} \\\sigma &amp;=\frac{\lambda_{2}-\lambda_{3}}{\lambda_{1}}, \quad \psi=\frac{\lambda_{3}}{\lambda_{1}}\end{aligned}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\gamma\)</span>是曲率</li><li><span class="math inline">\(\sigma\)</span>是平坦度</li><li><span class="math inline">\(\psi\)</span>是球面系数</li><li>需要注意的是，特征值以降序的顺序给出</li></ul><p>如算法3展示，如果平坦度系数<span class="math inline">\(\sigma\)</span>比平面特征阈值<span class="math inline">\(\tau\)</span>更大，然后将点作为平面特征的垂直部分提取。对于非平面部分，如果球面系数<span class="math inline">\(\psi\)</span>比球面特征阈值<span class="math inline">\(\pi\)</span>更大，点将会添加到球面特征中。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/14/20210614181724.png"></p><p>来自<code>HDL-64E</code>激光雷达的不同特征的提取示意图如图6所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/14/20210614181433.png"></p><p>进一步的，主方向和法向量将会在后续残差计算中继续使用，以进一步细化水平和垂直分布的关联特征。</p><h1 id="位姿优化">位姿优化</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;t-loam-truncated-least-squares-lidar-only-odometry-and-mapping-in-real-time&quot;&gt;T-LOAM: Truncated Least Squares LiDAR-Only Odometry and
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>LION论文阅读</title>
    <link href="http://yoursite.com/2021/06/09/LION%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/06/09/LION%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-06-09T02:03:53.000Z</published>
    <updated>2021-06-11T02:53:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="lion-lidar-inertial-observability-aware-navigator-for-vision-denied-environments">LION: Lidar-Inertial Observability-Aware Navigator for Vision-Denied Environments</h1><p><a href="https://youtu.be/Jd-sqBioarI" target="_blank" rel="noopener">Video</a></p><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609104338.png"></p><h1 id="摘要">摘要</h1><p>在GPS拒绝和感知的环境中导航的机器人的状态估计，例如地下隧道，矿区和行星子表面空隙[1]，在机器人中仍然具有挑战性。</p><p><code>LION</code>通过融合来自IMU的高频惯性数据和通过固定滞后滑动窗口的LIDAR低频相对位姿观测来提供高频的里程估计。同时也不需要事先获取LiDAR和IMU外参，进行了实时在线外参标定，此外，还使用了可观测性的度量来评估位姿估计是否是几何病态的。</p><h1 id="介绍">介绍</h1><p>提出了LiDAR-Inertial的可观测性算法，用于在感知性降低的环境中，这是<code>CoSTAR</code>团队在第一届<code>DARPA Subterranean</code>挑战中的方法。</p><p>我们的解决方案依赖于IMU预积分和<code>scan-to-scan</code>的ICP，以及固定滞后的滑动窗口。并且该方法实时在线标定LiDAR和IMU的外参，为了解决潜在的可观测性问题，我们使用了几何可观测性分数[23]，使得<code>LION</code>预测其输出中的观测场景的几何结构。通过该输出的得分，基于监督算法(如HeRO)，我们可以切换到不同的状态估计算法（如轮速编码、视觉惯性等）。该方法保证了状态估计的连续性、可靠性以及重力对齐的特点，为后续级联规划和控制算法提供保障。</p><h1 id="具体方法">具体方法</h1><h2 id="激光惯性里程计">激光惯性里程计</h2><p><code>LION</code>是基于滑动窗口的LIO，分为两个部分，前端由LO和IMU预积分、可观测性检测模块组成，后端是因子图优化，如图2所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609151034.png"></p><p>在下面的叙述中，遵循如下约定：</p><ul><li>里程计世界坐标系<span class="math inline">\(W\)</span></li><li>载体坐标系<span class="math inline">\(B\)</span>，即IMU坐标系</li><li>激光雷达坐标系<span class="math inline">\(L\)</span></li><li>一个点聪坐标系A到坐标系B的表示为4x4的矩阵：<span class="math inline">\(_B\mathbf{T}_A\)</span>，由旋转矩阵<span class="math inline">\(_B\mathbf{R}_A\)</span>和平移向量<span class="math inline">\(_Bt_A\)</span>组成</li></ul><p>LO模块使用<code>GICP</code>[31]来获取两帧激光扫描之间的相对位姿变换<span class="math inline">\(_{L_{k-1}} \mathbf{T}_{L_{k}}\)</span>。为了简化ICP算法的收敛性，对于每一帧激光扫描到达，首先进行重力对齐（使用IMU对其进行旋转部分的坐标表变换作为ICP的初始值）。</p><p>IMU预积分模块利用state-of-art的流形积分理论来将关键帧之间的IMU整合成单个运动约束[22,32]。</p><p>或者，基于scan-to-scan的前端可以使用<code>LOCUS</code>[33]框架替换，它额外地将新的激光扫描与局部地图进行对齐，以执行一个refinement步骤。</p><p>在后端，由前端产生的相对位姿观测与IMU测量结合使用，图3展示了因子图中的状态和因子，其中，在第j个时间步的状态<span class="math inline">\(x_j\)</span>表示如下：</p><p><span class="math display">\[\mathbf{x}_{j}:=\left\{_{W} \mathbf{T}_{B}, _W \mathbf{v},{ }_{B} \mathbf{b}^{a},{ }_{B} \mathbf{b}^{g},{ }_{B} \mathbf{T}_{L}\right\}_{j}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(_W{\mathbf{T}}_{B}\)</span>是IMU$到世界坐标系的变换</li><li><span class="math inline">\(_Wv\)</span>是线速度</li><li><span class="math inline">\(_Bb^a,_Bb^g\)</span>是IMU的加速度计bias和陀螺仪bias</li><li><span class="math inline">\(_BT_L\)</span>是激光雷达到IMU的变换</li></ul><p>遵循[32]，记<span class="math inline">\(\mathcal{K}_{k}:=\{k-m+1,\dots,k\}\)</span>为滑动窗口中的m个时间步，并记<span class="math inline">\(\mathcal{X}_{k}:={\mathbf{x}_j}_{j\in\mathcal{K}_k}\)</span>和<span class="math inline">\(\mathcal{Z}_k\)</span>分别记为滑动窗口中的状态和观测。</p><p>因子图优化旨在求解如下[32]函数：</p><p><span class="math display">\[    \mathcal{X}_k^{*}:= \arg \min_{\mathcal{X}_k}(-\log_c p(\mathcal{X}_k | \mathcal{Z}_k))\]</span></p><p>其中，<span class="math inline">\(p(\mathcal{X}_k | \mathcal{Z}_k)\)</span>是一个后验概率分布。</p><p>我们使用GTSAM[29]作为后端，并使用iSAM2[28]求解。</p><h2 id="可观测性度量">可观测性度量</h2><p>在地下场景，确定场景的几何特性是否有利于求解平移方向的位移是至关重要的。遵循[23,34]，假设旋转是小量的，那么<code>Point-to-Plane</code>ICP的cost的Hessian使用<span class="math inline">\(2A\)</span>来近似，其中，</p><p><span class="math display">\[\begin{aligned}    A:=\sum_{i=1}^{M} H_i^TH_i:=    \begin{bmatrix}    A_{rr} &amp; A_{rt} \\    A_{rt}^{T} &amp; A_{tt}    \end{bmatrix}\end{aligned}\]</span></p><p>且有</p><p><span class="math display">\[    H_i:=[-(p_i \times n_i)^{\mathbf{T}},-n_i^{\mathbf{T}}]\]</span></p><p>其中，<span class="math inline">\(n_i\)</span>是与点<span class="math inline">\(p_i\)</span>对应的平面单位法向量。</p><p>因此，矩阵<span class="math inline">\(A\)</span>的最小特征值即为最小可观测性的方向，平移部分通常是位姿估计具有挑战性的部分，主要是环境中可能存在长廊等场景。</p><p>因此，我们使用条件<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right):=\frac{\left|\lambda_{\max }\left(\boldsymbol{A}_{t t}\right)\right|}{\left|\lambda_{\min }\left(\boldsymbol{A}_{t t}\right)\right|}\)</span>作为可观测性的度量。</p><p>当<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>越大，优化问题中在平移部分的约束越差，当高于阈值时，<code>LION</code>向切换逻辑的<code>HeRO</code>[4]模块发出警告，以便可以使用其他更加准确的里程计来源。</p><h1 id="实验">实验</h1><h2 id="隧道竞赛">2019隧道竞赛</h2><p>我们首先在美国匹兹堡的Niosh实验矿山举行的两种不同赛道中的两次不同赛道中的两次不同赛道的两次不同。</p><p>激光里程计计算频率为10Hz，其中IMU和<code>LION</code>输出可以高达20Hz。<code>LION</code>使用的滑动窗口保持在3s内，并且后端被调整为使用<code>i7 Intel NUC</code>核心的30%。</p><p>对于参考，我们将其性能与：轮速惯性里程计（通过扩展卡尔曼滤波器融合的轮速惯性里程计）、scan-to-scan的里程计（<code>LION</code>的相对姿势输入）进行比较，并使用<code>LAMP</code>[35]作为ground-truth。</p><p>结果总结在表1中。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611102040.png"></p><p>我们可以看到，融合惯性数据与前端的里程计中显着降低了纯激光雷达里程计方法的漂移（scan-to-scan）。这从沿图4所示的z轴的z轴尤其明显，另外，<code>LION</code>可靠的估计了机器人的姿态（如图5所示），实现了小的roll和pitch误差</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611101500.png"></p><blockquote><p>没看出来.</p></blockquote><p>为了展示<code>LION</code>的自动校准能力，我们在模拟中生成了一个数据集，其中LIDAR沿IMU的Y轴相对于IMU转换为0.1米。 在图7中，我们观察到大约20秒后，<code>LION</code>（连续线）估计正确的外参（虚线）</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611101847.png"> &gt; 精度不咋地。</p><h2 id="可观测性模块">可观测性模块</h2><p>我们首先展示<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>如何检测几何上的无约束场景。 要测试这一点并建立直觉，我们首先使用<code>JPL-Corlidor</code>数据集，在JPL的办公室录制，其中主要的挑战是该环境沿长廊方向缺少几何特征。</p><p>此外，我们也使用了<code>Arch-Coal-Mine</code>数据集进行测试，它由一条直隧道和一个交叉路口组成。</p><p>将<code>JPL-Corlidor</code>数据集中的<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>以及矩阵特征值进行可视化，如图8所示：</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611102825.png"></p><blockquote><p>某个方向特征值越小，轴越短，不确定性越小，<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>越大，约束越少。</p></blockquote><p>在走廊的开始和末尾，所有方向都有足够的特征，条件号是<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>≈2。但是，在走廊的中间，条件号达到值<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>&gt; 13，同时特征向量沿着隧道的方向相关的特征值很小</p><p>将运行<code>Arch-Coal-Mine</code>数据集时的<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>以及矩阵特征值进行可视化，如图9所示：</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611103243.png"></p><p>使用此<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>作为可观察性度量标准，<code>HeRO</code>可以决定在没有足够的LIDAR功能时切换到不同于<code>LION</code>（如WIO）的其他里程计源。这种行为如图10所示。对于在办公室的环境中进行的真实实验，其中第一走廊的一部分没有足够的激光雷达特征。</p><p>如果未使用可观察性模块，则走廊中的这种特征缺乏会造成<code>lidar slip</code>，即无法观测到机器人的运动，这产生了9m的误差（机器人回到原点）。</p><p>当使用可观测性模块后，这种特征被检测到，将会切换到WIO模块来暂时代替<code>LION</code>，减少误差，此时误差为1m。</p><h1 id="总结">总结</h1><p><code>Local state estimator</code>: <code>LION</code>的主要目标是提供高速、连续、平滑的输出给下游算法。因此，<code>LION</code>并不构建任何地图，也不执行回环检测，因此其参考坐标系会逐渐偏移。这个偏移会由<code>LAMP</code>[35]中进行补偿。</p><p><code>Loosely-coupled architecture</code>： 与其他先进工作相比[16-18]，<code>LION</code>的前后端是松耦合的（即估计的状态x不包含特征点或激光扫描），目的是与建图模块[35]共享计算资源。此外，这种架构通过在选择前端/后端算法中的模块化时决定，并在单个估计引擎的情况下分配几个估计引擎之间的风险以消除单点故障。</p><p><code>Not feature-based</code>： 不同于[18],<code>LION</code>并没有使用特征点进行匹配，原因有两个，以是特征提取耗费计算资源，这将会降低板载资源的运行性能，另外，<code>LION</code>旨在探索完全未知的环境，在那里可以有人类制造的结构（充满了角点，平幔和线条）或完全非结构化的地形。这种不确定环境的特征提取的使用造成了在环境中的先验，因此引起风险或故障。</p><p><code>Automatic extrinsic calibration</code>：激光雷达与IMU的标定非常重要，特别是旋转方面，由于小误差可以快速地积累并导致大漂移。离线标定的方法通常需要校准目标[36]或特定运动序列[37]。</p><p><code>Supervisory Algorithm</code>：<code>LION</code>被设计成由<code>HeRO</code>[4]调用的多种里程计源之一，并且根据可观测性度量进行自主切换。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;lion-lidar-inertial-observability-aware-navigator-for-vision-denied-environments&quot;&gt;LION: Lidar-Inertial Observability-Aware Navigator
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>UPSLAM论文阅读</title>
    <link href="http://yoursite.com/2021/06/08/UPSLAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/06/08/UPSLAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-06-08T02:03:53.000Z</published>
    <updated>2021-06-09T03:00:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="upslam-union-of-panoramas-slam">UPSLAM: Union of Panoramas SLAM</h1><p><img src="http://s1.nsloop.com:59080/images/2021/06/08/20210608101046.png"></p><blockquote><p>此文表达，晦涩难懂</p></blockquote><h1 id="摘要">摘要</h1><p>提出一种基于全景深度图的建图系统，全景图像可有效捕获由旋转激光雷达获取的范围测量，在大约数百万立方米的空间地图中记录厘米级别的细节。通过手持传感器收集的数据并同时运行建图程序来证明系统的灵活性，在<code>NVIDIA-Jestion-AGX-Xavier</code>系统中，在线的3D建图更新时间小于10毫秒。</p><h1 id="介绍">介绍</h1><p>SLAM中的挑战：</p><ul><li>运动：运动是否快速？是地面行驶还是空中飞行器？地面是否平坦？</li><li>几何：目标环境是否有足够的特征（平面、角点、边缘）？</li><li>性能：准确性、计算效率、环境规模扩展性</li></ul><p>本文提出一种尽可能多的利用传感器数据的方法，并通过以下实验来验证提出方法的鲁棒性，其中包括：行走、跑步、小型轮式机器人、腿式机器人平台、无人机平台，以及汽车从室内杂乱的环境中到地下隧道再到野外森林再到公路。</p><p>对实验结果进行量化分析，其轨迹误差仅为轨迹长度的0.05%，其在15W功率的平台上运行，轨迹长度为4.4公里。</p><p>在这项工作中，环境被代表为一系列全景深度图，这个选择允许我们使用二维数组的来表示我们的局部地图，我们的实施能够有效利用这种表示所提供的常规内存访问模式和并行性，结果表明，即使在嵌入式GPU平台上也运行很快。</p><p><strong>另外一个重要的结论是</strong>，我们能够利用所有可用的激光测量数据，并且避免了早期关于特征的决定，或者说，利用所有数据提供了额外的鲁棒性，使得提出的方法可以适用于各种环境。</p><h1 id="具体方法">具体方法</h1><p><code>UPSLAM</code>的地图是一个<code>graph</code>，其中，该图以平面法向量估计增强的全景深度图作为节点，并以这些深度图关键帧之间的相对位姿关系作为边。</p><p>关键帧可能具有不同的分辨率和扩展区，而不是原始传感器数据，我们通常使用比传感器捕获更宽的关键帧垂直视野进行操作。随着每个帧的LIDAR扫描更新，系统会跟踪当前的关键帧。这里描述的实验使用<code>Ouster os1-64</code>进行，33<span class="math inline">\(^\circ\)</span>垂直视场角，10Hz。另外，由IMU提供100Hz的加速度、角速度信息。</p><h2 id="数据输入">数据输入</h2><p>在快速运动期间，LIDAR扫描的100ms周期可包括显着的传感器运动，因此首先在CPU上进行运动畸变矫正。</p><p>接下来的所有步骤都在GPU上进行。</p><p>经过矫正后的点将投影到一个全景图像中，图像结构有助于计算一个大致的平面法向量估计，即利用某个点与其他两个最近点的叉乘来计算得到。这些表面法向量估计再次使用图像来表示，但是噪声非常大，可以使用如<code>à trous wavelet filter</code>滤波器等边缘敏感的平滑算法进行有效地平滑[12]。</p><h2 id="运动估计">运动估计</h2><p>对于每一帧新来的深度图和平面法向量估计，都使用基于点-面距离误差衡量的ICP进行配准[13,14]，如下式所示：</p><p><span class="math display">\[\mathbf{E}(\mathrm{T})=\sum_{\left(s_{1}, s_{2}\right) \in C \wedge \Omega_{d, \theta}\left(s_{1}, s_{2}\right)}\left|\left(\mathrm{T} s_{2}-s_{1}\right) \cdot N\left(s_{1}\right)\right|\]</span></p><p>其中，我们定义一个位姿变换<span class="math inline">\(T\)</span>，旨在最小化投影点对<span class="math inline">\(C\)</span>之间的距离。另外，还使用了一个相似性滤波器，用于拒绝距离较大、平面法向量夹角相差较大的关联点对。这个方法很容易在GPU上实现[15]。</p><p>扫描的表面正常估计可能略微通过<code>à trous wavelet filter</code>滤器平滑，但这种平滑有助于优化收敛，特别的，每次扫描都会更新关键帧表面法线估计，从而恢复因平滑而丢失的一些细节。</p><p>在建图系统的鲁棒性方面，一个重要因素是有多少个点参与到优化中。图2显示了典型实验的有效点的数量，其中有效点的定义为：从激光雷达有效测距范围内的点且点的强度值大于给定阈值。在图中所示的示例中，传感器在隧道中的墙壁附近开始，但是一旦出现进入开放环境时，在10Hz扫描速率下达到了40,000~50,000个点。</p><h2 id="关键帧更新">关键帧更新</h2><p>优化的变换矩阵为最后的投影关联集合提供了基础，其中，每个关键帧像素都被投影到扫描的深度图上。使用相似性过滤器来确定新的扫描数据是否应该对关键帧像素的进行平均。</p><p>在更新关键帧时使用加权平均值：每个关键帧像素跟踪已结合到其中的样本数量，当大于10时，将被用于确定加权权重。</p><p>移动物体可以在单个关键帧中应对平滑和响应性的平衡，但考虑多个关键帧重建3D占用率的潜在多假设模型，这是一种从地图中过滤移动对象的有效工具。当切换关键帧时，执行关键帧之间的相互一致性检查，以减少移动对象的影响。</p><p>更新后，将决定是否应该继续使用当前关键帧。如果配准的得分定义为ICP配准到深度图上的内点比例低于阈值，则会寻找新的关键帧。并且，会根据pose graph来对最近的节点进行闭环检测。</p><p>由于在显着漂移的影响下可能发生闭环配准，因此使用低分辨率的投影配准的方法对数百个网格搜索，得到候选闭环。然后使用full-ICP来进行配准，如果配准的质量高于创建一个新的关键帧的阈值，那么将会增加一条边，并且该闭环候选作为新的关键帧。除此以外，如果内点比例大于75%，将会使用full-ICP的配准结果对pose graph进行更新（反向传播更新），我们将这些分为strong和weak的闭环：这两个闭环都提高了地图的全局准确性，但是强闭合还会减少关键帧的数量。</p><p>Pose graph使用了GTSAM，CPU上运行，另外，使用了基于IMU的互补滤波器，用于估计重力向量并对每一个关键帧添加约束到GTSAM，这种观测信息将有助于减少pitch方向的误差。</p><h1 id="自带数据集实验">自带数据集实验</h1><h2 id="校园室内">校园——室内</h2><p>mapping的第一次测试是在实验室空间散步145米，以1-2米/秒的速度移动。 使用50个关键帧图像捕获80个MIB在磁盘上捕获图3所示的地图。 虽然这里的运动是良性的，但是图底部围绕着角截面的玻璃，以及几个狭窄的通道，由于传感器无法可靠地检测玻璃或墙壁近50厘米的玻璃或墙壁，提供了一些挑战</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609093332.png"></p><h2 id="校园室外">校园——室外</h2><p>手持传感器，沿着375米的循环沿大学校园内的建筑物群，在2-3米/秒。 所得到的点云，如图4所示，当传感器在其在图4A的下部附近的树木附近的起始位置约为10米的单环闭合时，从单环闭合所形成的。 选择运行的步伐，以强调由于摇动和弹跳而具有高幅度旋转的非平面运动</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609093611.png"></p><h2 id="农场">农场</h2><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609094032.png"> <img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609094057.png"></p><h2 id="地下">地下</h2><p>Robot沿着220米的轨迹在一座古老的煤矿中散步，采集了如图6中所示的底图，共有44个全景图像。该实验提供了与前面描述的运动模型的实际不同的运动模型，因为矿井的湿法砾石地板意味着机器人必须不断地工作以保持其平衡。</p><p>滴水，雾和灰尘对单独的激光雷达扫描有很大的噪音 <img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609094248.png"></p><h2 id="空中">空中</h2><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609094528.png"></p><h2 id="车载">车载</h2><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609094459.png"></p><p><strong>前面几个实验的参数</strong></p><table><thead><tr class="header"><th>参数</th><th>值</th></tr></thead><tbody><tr class="odd"><td>激光雷达</td><td>Ouster OS1-64</td></tr><tr class="even"><td>速度</td><td>3m/s</td></tr></tbody></table><p><strong>车载实验的参数</strong></p><table><thead><tr class="header"><th>参数</th><th>值</th></tr></thead><tbody><tr class="odd"><td>激光雷达</td><td>Ouster OS1-64</td></tr><tr class="even"><td>速度</td><td>13.8m/s</td></tr><tr class="odd"><td>关键帧数量</td><td>1045</td></tr><tr class="even"><td>图片占用</td><td>1220MiB</td></tr><tr class="odd"><td>生成地图</td><td>20GiB</td></tr></tbody></table><h1 id="公开数据集实验">公开数据集实验</h1><p>数据集使用了<code>Newer College Dataset</code>[18]</p><h2 id="轨迹精度">轨迹精度</h2><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609095605.png"></p><p>ATE: 1.4km， 0.77m， 0.05%</p><h2 id="算法效率">算法效率</h2><p>使用平台：NVIDIA Jetson AGX Xavier</p><p>考虑高分辨率，2048x256和低分辨率，1024x128的关键帧</p><p>在图13中的每种配置中显示了将新LIDAR扫描到地图中所花费的时间。与Xavier的最大功率预算和高分辨率关键帧采取的中位时间为6毫秒</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609100109.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;upslam-union-of-panoramas-slam&quot;&gt;UPSLAM: Union of Panoramas SLAM&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com:59080/images/2021/06/08/20210
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>Cross_view_slam论文阅读</title>
    <link href="http://yoursite.com/2021/06/05/Cross_view_slam%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/06/05/Cross_view_slam%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-06-05T10:03:53.000Z</published>
    <updated>2021-06-07T11:16:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="any-way-you-look-at-it-semantic-crossview-localization-and-mapping-with-lidar">Any Way You Look at It: Semantic Crossview Localization and Mapping With LiDAR</h1><p><img src="http://s1.nsloop.com:59080/images/2021/06/05/20210605180434.png"></p><h1 id="摘要">摘要</h1><p>GPS是迄今为止最受欢迎的全球本地化方法。 但是，在所有环境中，它并不总是可靠或准确。 SLAM方法使能局部估计能够提供将本地地图注册到全局的局部估计，这对于机器人间协作或人类互动可能是重要的。</p><p>在这项工作中，提出了一种利用语义的实时方法，仅使用以自我为中心的 3D 语义标记的 LiDAR 和 IMU 以及从卫星或空中机器人获得的自上而下的 RGB 图像来全局定位机器人。</p><h1 id="介绍">介绍</h1><p>提出一种将地面机器人的局部观测信息与来自卫星或无人机的大范围全局地图进行匹配的定位方法，这种方法比传统的基于已知地图定位方法更加具有挑战性，因为对于大多数机器人而言，<code>the overhead views</code> 与 以自我为中心的观测数据有较大不同。因此，在鸟瞰图中直接应用基于特征的配准方法对机器人进行定位通常是无效的。此外，卫星图通常在不同的时间拍摄，这意味着配准方法必须适应季节的变化。</p><p>近年来，使用人工神经网络（ANNS）的图像语义分割已成为成熟的技术。地图语义对于<code>cross-view registration</code>是理想的，因为在足够多样化的训练数据中，可提取具有视角和季节不变性的语义信息。此外，从鸟瞰图中提取粗略的语义信息比从<code>the overhead views</code> 中提取更加简单，无论是手工提取还是基于ANN的方法。</p><p>与此同时，Lidars在光束密度增加同事，价格和重量下降，因此将它们放在小型移动机器人上现在是实用的。同时应用两种传感技术将可以产生稠密的语义点云。</p><p>最近的工作使用了用于<code>cross-view</code>定位的图像语义信息[15]，[16]，但通常没有只有很少的利用了深度信息或环境的有力的结构假设。如基于几何方法如 [3] 通常在没有足够几何结构的环境中失效，相反，我们引入了一种结合这两种信息来源的方法，以实现更强大的语义<code>cross-view</code>定位系统。</p><p>主要贡献如下：</p><ul><li>提出一种实时的<code>cross-view</code>定位和建图框架，如果鸟瞰地图未知但有界，我们的方法也能够估计它的比例。</li><li>我们使用Semantickitti [17]以及我们自己的数据集以及在包括农村和城市环境的各个地点，验证我们所提出的方法。</li><li><a href="https://github.com/iandouglas96/cross_view_slam" target="_blank" rel="noopener">开源了代码</a></li></ul><h1 id="相关工作">相关工作</h1><h2 id="图像匹配">图像匹配</h2><p><code>Cross-view localization</code>问题定义为图像匹配问题[12]：给定一个由图片构成的数据库作为全局地图，以及一张待查询的图片作为局部观测。问题可描述为该问题通过获取一些描述符，使得来自多个视图的相同位置的图像在一些潜在的空间中接近。</p><p>早期的工作如[18]，[19]，使用局部特征描述符。 Majdik等[11]使用来自Google Street View的图像特征来与四旋翼无人机飞行拍摄图来进行匹配。然而，这些方法往往在极端视角的情况下失效。</p><p>这个限制导致了最近兴起的使用孪生神经网络[20], [21]方向，这些网络通过对不同视点的图像使用权重共享的分支网络结构在潜在空间进行交叉关联，来决定两个图像的相似度。整个网络在已知的图像对上训练，在线处理图像对时，这些网络相对较慢，因此不能在机器人应用程序中实时运行，其中必须同时查询许多可能的状态。</p><h2 id="基于视觉">基于视觉</h2><p>图像匹配算法提供了一种将图像与提供的现有图像数据库进行比较的方法，但它们并未明确寻求估计机器人或传感器的位置。定位需要具有<code>pose label</code>或整个航拍地图的图像数据库，如[22]中的描述。在这项工作中，作者使用鸟瞰坐标系中的一系列边缘来代表全局地图，然后在粒子滤波框架中与地面图像进行边缘匹配。但是，通过将全局地图减少为一系列边缘，将会丢弃大量的有用信息。</p><p>其他工作如[23]，使用立体图像来生成RGBD图像，然后将它从鸟瞰角度进行渲染生成，然后使用<code>chamfer matching</code>与已知地图进行匹配，尽管如此，这种方法未能解决由季节变化或环境中的动态物体（如人或汽车）等因素引起的摄影变化。</p><p>为了更好地应对季节性变化和更加极端的观点变化，最近的工作越来越多地分析了语义的定位使用。Castaldo等 [15]对地面图像分割，并使用<code>homography</code>和地平面将语义特征投影到自上而下的视角。然后开发出一种针对分割图像的语义描述符，并与已知地图进行比较，以在相机位置集合生成对应的热图(heatmap)。然而，改方法并没有利用时间或深度信息，导致大规模定位系统的收敛速度较慢。另外，他们的方法在<code>homography</code>不成立的情况下失效，如路面并非平坦的工况。</p><p>类似的工作还有Gawel et al. [16]，为空中图和地面视图生成语意图表示，然后构建描述符以匹配各种合成数据集。</p><h2 id="基于激光雷达">基于激光雷达</h2><p>Wolcott等人 [24]通过从各种透视图渲染一个已知的高分辨率点云并最大化互相关信息来对相机进行定位。Gawel等人 [3]通过从两个角度构建点云地图并使用几何描述符匹配它们来交叉定位地面和空中机器人。Barsan等人 [25]使用孪生网络实现厘米级定位。虽然上述方法准确和强大，这些方法需要相关环境的预先存在点云图，而我们只需要单个航拍图像。</p><p>与基于视觉的方法一样，最先进的基于Lidar的方法利用环境的语义结构。在早期的工作中，Matei等人[26] 从无人机采集的点云来拟合一个最接近的建筑模型，用于创建与地面图像相比的地面预测。</p><p>……</p><p>与这些工作相比，我们的框架只需要一个卫星或环境的鸟瞰图，这适合更多的应用。</p><h1 id="具体方法">具体方法</h1><p>我们的方法由两个主要组件组成：基于ICP的LIDAR SLAM系统 - Panoramas Slam [32]（Upslam） - 以及基于粒子滤波器的语义定位器，如图2所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/06/20210606185558.png"></p><h2 id="定位">定位</h2><p>粒子滤波器特别擅长处理多模态分布，这在机器人定位问题中经常出现，这将带来计算量的问题，我们通过使用优化策略来减少计算成本：</p><h3 id="问题描述">问题描述</h3><p>对于2D地图的定位，我们系统状态<span class="math inline">\(x\)</span>由<code>tuple</code>(<span class="math inline">\(p\in SE(2) , s \in [S_{min},S_{max}]\)</span>)构成，其中，<span class="math inline">\(p\)</span>表示机器人位姿，<span class="math inline">\(s\)</span>表示地图尺度(px/m)。我们还有控制输入量<span class="math inline">\(u\in SE(3)\)</span>，从上一帧到当前帧的相对位姿变换，该值来源于<code>UPSLAM</code>或者其他里程计。</p><p>因此，对于每个时间步t，都有一个运动模型<span class="math inline">\(P(x_t|x_{t-1},u_{t-1})\)</span>。此外，在每个时刻t，我们都有来自LiDAR和cameras的语义扫描<span class="math inline">\(z_t\)</span>，为了定义我们的粒子滤波器，我们必须先定义观测模型<span class="math inline">\(P(z_{t}|x_{t})\)</span></p><h3 id="运动模型">运动模型</h3><p>实际上，我们并不能获取真正的控制输入量，而是从frame-to-frame的估计中获取一个近似。</p><p>因为<code>UPSLAM</code>在3D空间中操作，因此我们首先将帧间运动投影到local的x-y地平面上，记为<span class="math inline">\(\operatorname{proj}(u) \in SE(2)\)</span>。此外，我们只使用<code>UPSLAM</code>的基于ICP的初始解作为运动估计，忽略了来自闭环后的优化位姿（这是为了避免里程的运动不连续性）。</p><p>最后，我们假设分布具有恒定的协方差，在<code>log-space</code>中具有尺度噪声，因此，有：</p><p><span class="math display">\[\begin{aligned}P\left(x_{t} \mid x_{t-1}, u_{t-1}\right)=(&amp;\left[\operatorname{proj}\left(u_{t-1}\right)+\mathcal{N}\left(0, \Sigma_{p}\right)\right] * p_{t-1},\left.\mathcal{N}\left(1, \Sigma_{s}\right) * s_{t-1}\right)\end{aligned}\]</span></p><p>另外，我们使用到起点位置的距离的inverse来对协方差<span class="math inline">\(\Sigma_s\)</span>进行缩放，以使得可以自然的收敛。一旦尺度方差低于阈值，我们则可固定尺度s。</p><h3 id="观测模型">观测模型</h3><p>我们的观测<span class="math inline">\(z\)</span>是语义的点云，我们可以表示为在机器人坐标系中具有相关性的标签：<span class="math inline">\(z=\{ (p_1,l_1),(p_2,l_2),\dots,(p_n,l_n) \}\)</span>。因此，我们可以将这些点投影到地平面上。</p><p>进一步的，对于任意给定的粒子状态，我们可以对机器人坐标系中的点在俯视的空中地图<span class="math inline">\(L\)</span>中查询，比较地图上的类别与预期类别，在给定粒子状态(位姿为<span class="math inline">\(d\)</span>)的条件下，一个简单的计算cost方法如下：</p><p><span class="math display">\[C^{\prime}=\sum_{i \in[1, n]} \mathbf{1}\left(L\left(d * p_{i}\right) \neq l_{i}\right)\]</span></p><p>为了通过扩大局部最小来提高收敛性，我们选择一个更柔软的成本函数。因此，我们不使用以二进制方式评估cost，而是以机器人坐标系中的点对应的类别为目标类别，在空中地图中查询相同类别的最近点，以两点距离作为cost：</p><p><span class="math display">\[C=\sum_{i \in[1, n]} \min _{\left\{p \mid L(p)=l_{i}\right\}}\left(\left\|p-p_{i}\right\|\right)\]</span></p><p>最后，我们通过对<span class="math inline">\(C\)</span>求逆和正则化来计算一个<code>ad-hoc probability</code>，另外，对于每一个类别的点，可以设置一个权重因子<span class="math inline">\(\alpha_l\)</span>：</p><p><span class="math display">\[P\left(z_{t} \mid x_{t}\right) \approx \frac{n}{\sum_{i \in[1, n]} \min _{\left\{p \mid L(p)=l_{i}\right\}}\left(\alpha_{l_{i}}\left\|p-p_{i}\right\|\right)}+\gamma\]</span></p><p>其中，<span class="math inline">\(\gamma\)</span>是正则化常数项，用于slow convergence??? 所有粒子的概率最后都要进行归一化，以使得所有粒子对应的观测模型概率之和为1。</p><p>可以注意到，这是一个<code>ad-hoc</code>观测，在实践中可以通过实验对常数进行调整，然而，这对于基于蒙特卡罗的定位方法并不少见，例如 [29]。</p><h3 id="性能优化">性能优化</h3><p>如果单纯的实现<span class="math inline">\(C^{\prime}\)</span>的cost计算，将会导致较大计算成本，因为它总结了所有的非凸最小值的情况。我们通过预先计算空中语义地图关于类别的截断场(Truncated Distance Field, TDF)来优化这个问题，这个计算非常直观，大概需要1分钟，但是只需要进行一次。</p><p>这个TDF地图对每一个点到其同类别的最近点的距离阈值进行编码，将最小化<span class="math inline">\(C^{\prime}\)</span>变成了一个简单的查表。另外，相比于简单的对所有点的结果进行求和，使用了如下方法优化：首先将语义激光扫描分散到极坐标系中，统计在每个极坐标系分割中每个类别的点数。然后局部的类别TDF场使用同种方式渲染显示。然后通过对两个图像的元素进行乘法，得到关于每个类别的内积，来近似<span class="math inline">\(C^{\prime}\)</span>的计算，提高计算效率。</p><p>我们发现，对于<code>100x25</code>像素的极坐标地图，每个粒子大概需要<code>500μs</code>来计算，几乎是用于查表的时间，如图3所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/07/20210607101819.png"></p><p>另外，地图的旋转在极坐标中表示为索引的偏移，计算非常快，可用于初始化阶段。我们随机采样了地图上的道路点，因为我们有强壮的先验信息：因为是从道路上开始的。对于每一个点，我们初始化<span class="math inline">\(k_s\)</span>个在<span class="math inline">\(s_{min}\)</span>和<span class="math inline">\(s_{max}\)</span>(1~10 px/m)尺度内均匀分布的粒子。对于每一个粒子，我们采样<span class="math inline">\(k_{\theta}\)</span>个可能的观测，使得我们可以高效的对索引进行偏移（可参考scan-context），然后选取最佳的观测作为初始粒子。</p><p>为了进一步加速算法，我们使用CPU并行计算每个粒子的cost，我们还基于高斯混合模型（GMM）的协方差的区域的总和适应粒子分布的基础上的粒子数</p><h2 id="语义分割">语义分割</h2><h3 id="卫星分割">卫星分割</h3><p>我们使用在 ImageNet 上预训练的 ResNet-34 [34] 骨干训练两个稍微修改过的完全卷积网络 (FCN) [33] 版本以分割卫星图像，图像直接从<code>Google-Earth</code>中获取。</p><p>我们使用了4个类别：</p><ul><li>road（道路）</li><li>terrain（地面）</li><li>vegetation（植被）</li><li>building（建筑）</li></ul><p>为了分析卫星图像，256×256 PX RGB图像被传递为图像分割网络的输入，该网络由3个手工标注的卫星图像数据训练得到，如表1所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/07/20210607153338.png"></p><p>图像是随机缩放，旋转，裁剪的，并翻转以产生更多的训练样本，卫星图像的随机缩放也允许模型在从多个高度收集的图像上概率更好地泛化。</p><p>网络输出例子和训练数据如图5所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/07/20210607153818.png"></p><p>令人惊讶的是，尽管模型只在3张图片上训练，但是每张图片都包含了许多对象实例</p><h3 id="激光扫描分割">激光扫描分割</h3><p>我们使用图 2 中绿色方框中显示的两个不同的pipline，用于根据数据集生成语义点云。通过使用不同的分割方法进行测试，展示了我们的方法可以适用于不同的传感系统，提供点级别的标签点云。</p><p><strong>PC</strong> 对于kitti数据集，使用了与卫星图像分割同样的FCN结构，对于激光扫描，使用带有X,Y,Z,Depth通道的64x2048的<code>2D-Poloar-Grid-Map</code>来表示(没有利用强度信息)。</p><p>我们在<code>SemanticKITTI</code>数据集上训练，然后使用序列{10}和{00,02,09}作为验证集和测试集。另外，我们添加地面车辆转化为road类别。</p><p><strong>RGB</strong></p><p>对于我们自己的Morgantown和Ucity数据集，我们使用不同于Kitti的激光雷达（Ouster OS-1），因此不能使用Semantickitti进行训练。因此，我们使用HRNets[13]来对RGB图像进行分割，然后校准到激光扫描中。利用外参信息，可以将激光点云投影到相机图像帧，然后根据RGB的分割对点云进行分配。</p><h2 id="建图">建图</h2><p>我们使用<code>UPSLAM</code>，但此处给出了在本工作中的一些更改的概况。对于这项工作，我们扩展了<code>UPSLAM</code>，整合从图像中提取的语义标签，以形成语义全景。需要注意的是，<code>UPSLAM</code>并没有使用语义信息来进行<code>scan-matching</code>，只是简单的使用刚体变换信息来整合语义信息。因此，由于我们不需要每一帧扫描的语义数据，我们可以以低于LIDAR的速率来运行推断，而无需删除ICP数据，以提高地图质量。深度、法向量、语义全景如图4所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/07/20210607164532.png"></p><p>除了使用<code>UPSLAM</code>估计的粒子过滤器运动模型之外，我们还计算每个更新的后粒子滤波器估计的协方差和均值。一旦协方差低于阈值<span class="math inline">\(\Sigma_t\)</span>，我们使用粒子滤波器估计的位置作为位子图中对应状态节点的先验，最终得到如图2所示的pose graph。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/06/20210606185558.png"></p><p>通过这样的方式，我们使得图优化结果与地理保持对齐，我们的实验表明，添加这些语义约束边可去除漂移，并有效地构建语义闭环约束。这使得建图器在没有闭环的情况下可以应对更大规模的轨迹，并保持全局一致性。</p><h1 id="实验">实验</h1><p><img src="http://s1.nsloop.com:59080/images/2021/06/07/20210607185914.png"></p><h1 id="参考文献">参考文献</h1><p>[1] W.Maetal.,“Findyourwaybyobservingthesunandothersemanticcues,”in Proc. IEEE Int. Conf. Robot. Automat., May 2017, pp. 6292–6299. [2] T. Dang et al., “Autonomous search for underground mine rescue using aerial robots,” in Proc. IEEE Aerosp. Conf., 2020, pp. 1–8. [3] A. Gawel et al., “3D registration of aerial and ground robots for disaster response: An evaluation of features, descriptors, and transformation esti- mation,” in Proc. IEEE Int. Symp. Saf., Secur. Rescue Robot., Oct. 2017, pp. 27–34. [4] J. Peterson et al., “Online aerial terrain mapping for ground robot naviga- tion,” Sensors, vol. 18, no. 2, Feb. 2018, Art no. 630. [5] N. Michael et al., “Collaborative mapping of an earthquake-damaged building via ground and aerial robots,” J. Field Robot., vol. 29, no. 5, pp. 832–841, 2012. [6] X. Liang, H. Wang, Y. Liu, W. Chen, and T. Liu, “Formation control of non- holonomic mobile robots without position and velocity measurements,”IEEE Trans. Robot., vol. 34, no. 2, pp. 434–446, Apr. 2018. [7] A. Franchi, G. Oriolo, and P. Stegagno, “Mutual localization in multi- robot systems using anonymous relative measurements,” Int. J. Robot. Res., vol. 32, no. 11, pp. 1302–1322, 2013. [8] A. Howard, “Multi-robot simultaneous localization and mapping using particle filters,” Int. J. Robot. Res., vol. 25, no. 12, pp. 1243–1256, 2006. [9] S. Wang et al., “A novel approach for lidar-based robot localization in a scale-drifted map constructed using monocular slam,” Sensors, vol. 19, no. 10, 2019, Art. no. 2230. [10] F. Dellaert, D. Fox, W. Burgard, and S. Thrun, “Monte Carlo localization for mobile robots,” in Proc. IEEE Int. Conf. Robot. Automat., vol. 2, 1999, pp. 1322–1328 vol.2. [11] A. L. Majdik, Y. Albers-Schoenberg, and D. Scaramuzza, “MAV urban localization from google street view data,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., Nov. 2013, pp. 3979–3986. [12] X. Gao, S. Shen, Z. Hu, and Z. Wang, “Ground and aerial meta-data integration for localization and reconstruction: A review,” Adv. Visual Corresp.: Models, Algorithms Appl., Pattern Recognit. Lett. vol. 127, pp. 202–214, 2019. [13] J. Wang et al., “Deep high-resolution representation learning for visual recognition,” IEEE Trans. Pattern Anal. Mach. Intell., early access: Apr. 01, 2020, doi: 10.1109/TPAMI.2020.2983686. [14] M. Wu, C. Zhang, J. Liu, L. Zhou, and X. Li, “Towards accurate high resolution satellite image semantic segmentation,” IEEE Access, vol. 7, pp. 55 609–55 619, 2019. [15] F. Castaldo, A. Zamir, R. Angst, F. Palmieri, and S. Savarese, “Semantic cross-view matching,” in Proc. IEEE Int. Conf. Comput. Vis. Workshops, Dec. 2015, pp. 9–17. [16] A. Gawel, C. D. Don, R. Siegwart, J. Nieto, and C. Cadena, “X-view: Graph-based semantic multi-view localization,” IEEE Robot. Automat. Lett., vol. 3, no. 3, pp. 1687–1694, Jul. 2018. [17] J. Behley et al., “SemanticKITTI: A dataset for semantic scene under- standing of LiDAR sequences,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 9297–9307. [18] D.M. Chen et al., “City-scale landmark identification on mobile devices,”in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2011, pp. 737–744. [19] Y. Li, N. Snavely, and D. P. Huttenlocher, “Location recognition using prioritized feature matching,” Computer Vision – ECCV, K. Daniilidis, P. Maragos, and N. Paragios, eds. Springer Berlin Heidelberg, 2010, pp. 791–804. [20] D. Kim and M. R. Walter, “Satellite image-based localization via learned embeddings,” in Proc. IEEE Int. Conf. Robot. Automat., May 2017, pp. 2073–2080. [21] Y. Tian, X. Deng, Y. Zhu, and S. Newsam, “Cross-time and orientation- invariant overhead image geolocalization using deep local features,” in Proc. IEEE Winter Conf. Appl. Comput. Vis., 2020, pp. 2512–2520. [22] K. Y. K. Leung, C. M. Clark, and J. P. Huissoon, “Localization in urban environments by matching ground level video images with an aerial image,” in Proc. IEEE Int. Conf. Robot. Automat., May 2008, pp. 551–556. [23] T. Senlet and A. Elgammal, “A framework for global vehicle localization using stereo images and satellite and road maps,” in Proc. IEEE Int. Conf. Comput. Vis. Workshops, Nov. 2011, pp. 2034–2041. [24] R. W. Wolcott and R. M. Eustice, “Visual localization within lidar maps for automated urban driving,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., Sep. 2014, pp. 176–183. [25] I. A. Barsan, S. Wang, A. Pokrovsky, and R. Urtasun, “Learning to localize using a lidar intensity Map,” in Proc. 2nd Conf. Robot Learn., Proc. Mach. Learn. Res., A. A. Billard Dragan, J. Peters, and J. Morimoto, Eds. PMLR, 29-31, vol. 87, Oct. 2018, pp. 605–616. [26] B. C. Matei et al., “Image to LiDar matching for geotagging in urban environments,” in Proc. IEEE Workshop Appl. Comput. Vis., Jan. 2013, pp. 413–420. [27] T. Senlet, T. El-Gaaly, and A. Elgammal, “Hierarchical semantic hashing: Visual localization from buildings on maps,” in Proc. 22nd Int. Conf. Pattern Recognit., Aug. 2014, pp. 2990–2995. [28] Y. Tian, C. Chen, and M. Shah, “Cross-view image matching for geo- localization in urban environments,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jul. 2017, pp. 3608–3616. [29] F. Yan, O. Vysotska, and C. Stachniss, “Global localization on open- streetmap using 4-bit semantic descriptors,” in Proc. Eur. Conf. Mobile Robots, 2019, pp. 1–7. [30] E. Stenborg, C. Toft, and L. Hammarstrand, “Long-term visual localization using semantically segmented images,” in Proc. IEEE Int. Conf. Robot. Automat., 2018, pp. 6484–6490. [31] Y. Liu, Y. Petillot, D. Lane, and S. Wang, “Global localization with object- level semantics and topology,” in Proc. Int. Conf. Robot. Automat., 2019, pp. 4909–4915. [32] A. Cowley, I. D. Miller, and C. J. Taylor, “UPSLAM: Union of panoramas SLAM,” in Proc. Int. Conf. Robot. Automat., 2021. [33] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 3431–3440. [34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770–778. [35] M. Cordts et al., “The cityscapes dataset for semantic urban scene un- derstanding,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 3213–3223. [36] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? The KITTI vision benchmark suite,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 3354–3361. [37] S. S. Shivakumar, T. Nguyen, I. D. Miller, S. W. Chen, V. Kumar, and C. J. Taylor, “Dfusenet: Deep fusion of RGB and sparse depth information for image guided dense depth completion,” in Proc. IEEE Intell. Transp. Syst. Conf., 2019, pp. 13–20.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;any-way-you-look-at-it-semantic-crossview-localization-and-mapping-with-lidar&quot;&gt;Any Way You Look at It: Semantic Crossview Localizati
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>Adjoints and Covariances（伴随与协方差）</title>
    <link href="http://yoursite.com/2021/05/09/SE3%E4%BC%B4%E9%9A%8F/"/>
    <id>http://yoursite.com/2021/05/09/SE3%E4%BC%B4%E9%9A%8F/</id>
    <published>2021-05-09T14:55:30.000Z</published>
    <updated>2021-05-27T02:27:22.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="adjoints">Adjoints</h1><p>我们将介绍李群伴随的概念，这将帮助我们将右边的增量或矫正值与左边的增量或校正值联系起来。这种性质使我们能够用代数方法处理李群定义的不确定性，并得到不同协方差变换的表达式。我们将重点讨论3D构造，即 SE (3)，因为它的广泛适用性，但类似的定义应该适用于其他李群，因为他们主要依赖于伴随的定义。</p><p><a href="http://ncfrn.cim.mcgill.ca/members/pubs/barfoot_tro14.pdf" target="_blank" rel="noopener">Barfoot 和 Furgale (2014年)</a>和 <a href="https://arxiv.org/abs/1906.07795" target="_blank" rel="noopener">Mangelson 等人(2020年)</a>的文献中已经出现了大多数这样的表达式，但由于它们遵循左手惯例，所以不能直接用于 GTSAM。我们为 Mangelson 等人之后的协方差变换提供了结果表达式，但是我们建议参考他们的工作来理解这个过程的细节。</p><p>让我们考虑一个例子，我们在一个姿态<span class="math inline">\(\mathbf{T}_{WB_i}\)</span>加入小增量<span class="math inline">\(_{B_i}\mathbf{\xi}\)</span>:</p><p><span class="math display">\[\begin{aligned}\mathbf{T}_{W_i B_{i}} \text{Exp}( _{B_i}\mathbf{\xi})\end{aligned}\]</span></p><p><img src="http://s1.nsloop.com:59080/images/2021/05/26/20210526224034.png"></p><blockquote><p>以上遵循了right-hand定则，与GTSAM一致</p></blockquote><p>然而，一些应用则使用left-hand的形式:</p><span class="math display">\[\begin{aligned}\mathbf{T}_{W_{i+1} B} = \text{Exp}( _{W_i}\mathbf{\xi}) \mathbf{T}_{W_i B_i}\end{aligned}\]</span><p>那么，对应的就是参考坐标系的增量变化：</p><p><img src="http://s1.nsloop.com:59080/images/2021/05/26/20210526224431.png"></p><p>实际上，两种表达的意义都是一样的：</p><p><img src="http://s1.nsloop.com:59080/images/2021/05/26/20210526225000.png"></p><p>即有如下等式(暂时忽略时间索引)：</p><p><span class="math display">\[\begin{aligned}\text{Exp}( _{W}\mathbf{\xi}) \mathbf{T}_{WB} = \mathbf{T}_{WB} \text{Exp}( _{B}\mathbf{\xi})\end{aligned}\]</span></p><p>进一步的，就可以根据body系的增量来求出world系的增量：</p><p><span class="math display">\[\begin{aligned}\text{Exp}( _{W}\mathbf{\xi}) = \mathbf{T}_{WB} \text{Exp}( _{B}\mathbf{\xi}) \mathbf{T}_{WB}^{-1}\end{aligned}\]</span></p><p><img src="http://s1.nsloop.com:59080/images/2021/05/26/20210526230129.png"></p><p>对于我们的目的，使用一个等价的替代表达式是有用的，它直接应用于切空间的元素(<a href="https://arxiv.org/abs/1812.01537" target="_blank" rel="noopener">solà</a>等人给出了一个更完整的推导，由于一些属性我们在这里省略了) :</p><p><span class="math display">\[\begin{aligned}\text{Exp}( _{W}\mathbf{\xi}) \mathbf{T}_{WB_i} = \mathbf{T}_{WB_i} \text{Exp}( \text{Ad}_{T_{WB_i}^{-1}}  {_{W}}\mathbf{\xi})\end{aligned}\]</span></p><p>其中，<span class="math inline">\(\text{Ad}_{T_{WB_i}^{-1}}\)</span>称为<span class="math inline">\(T_{WB_i}^{-1}\)</span>的伴随，伴随直接作用于切线空间的元素上，改变它们的参考坐标系，即：</p><p><span class="math display">\[{_{B}}\mathbf{\xi} = \text{Ad}_{T_{WB_i}^{-1}} {_{W}}\mathbf{\xi}\]</span></p><p>我们也可以把这解释为一种方法，将左边(在世界坐标系中)施加的增量一致地移动到右边(body坐标系) ，这对于保持右边惯例对于回溯和概率分布的一致性特别有用。</p><blockquote><p>这是我们用来定义一些协方差转换的主要属性，并且它已经在<code>GTSAM</code>的<code>Pose3</code>中实现为<code>AdjointMap</code>。</p></blockquote><h1 id="distribution-of-the-inverse">Distribution of the inverse</h1><p>考虑这样一种情况: 我们有一个因子图的解，其协方差定义在body坐标系中。我们感兴趣的是获得一个表示world坐标系的协方差表达式.</p><table><thead><tr class="header"><th style="text-align: center;"><img src="http://s1.nsloop.com:59080/images/2021/05/27/20210527100129.png"></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><em>给定body坐标系的协方差<span class="math inline">\(B_i\)</span></em>(左图)，然而我们感兴趣的是右图的世界坐标系的协方差<span class="math inline">\(W\)</span></td></tr></tbody></table><p>假设具有正态分布的位姿表达如下：</p><span class="math display">\[\begin{aligned}\mathbf{\tilde{T}}_{WB} = \mathbf{T}_{WB} \text{Exp}( _{B}\mathbf{\eta})\end{aligned}\]</span><p>其中，<span class="math inline">\(_{B}\mathbf{\eta}\)</span>是协方差为<span class="math inline">\(\Sigma_{B}\)</span>的零均值高斯分布噪声，因此，逆位姿的分布可以通过对位姿表达式求逆：</p><p><span class="math display">\[\begin{aligned}(\mathbf{\tilde{T}}_{WB})^{-1} &amp; = (\mathbf{T}_{WB} \text{Exp}( _{B}\mathbf{\eta}) )^{-1}\\&amp; = (\text{Exp}( _{B}\mathbf{\eta}) )^{-1}\ \mathbf{T}_{WB}^{-1}\\&amp; = \text{Exp}(- _{B}\mathbf{\eta}) \ \mathbf{T}_{WB}^{-1}\end{aligned}\]</span></p><p>然而，在求逆后，噪声项被定义在左边，并且仍然是body坐标系下的表达，接下来使用伴随来将这一项移到右边:</p><p>1.根据前面推导的:</p><p><span class="math display">\[\begin{aligned}\text{Exp}( _{W}\mathbf{\xi}) \mathbf{T}_{WB_i} = \mathbf{T}_{WB_i} \text{Exp}( \text{Ad}_{T_{WB_i}^{-1}}  {_{W}}\mathbf{\xi})\end{aligned}\]</span></p><p>2.应用于位姿的逆表达（等号右侧），则有：</p><p><span class="math display">\[\begin{aligned}\text{Exp}( -{_{B}}\mathbf{\eta}) \mathbf{T}_{WB}^{-1} = \mathbf{T}_{WB}^{-1} \text{Exp}( -\text{Ad}_{T_{WB}} {_{B}}\mathbf{\eta})\end{aligned}\]</span></p><p>3.因此，有：</p><p><span class="math display">\[\begin{aligned}(\mathbf{\tilde{T}}_{WB})^{-1} = \ \mathbf{T}_{WB}^{-1}\ \text{Exp}(- \text{Ad}_{\mathbf{T}_{WB}} {_{B}}\mathbf{\eta})\end{aligned}\]</span></p><p>最终，得到了符合right-hand的分布，其中定义了world坐标系的协方差，如下：</p><p><span class="math display">\[\begin{aligned}\Sigma_{W} = \text{Ad}_{\mathbf{T}_{WB}} \Sigma_B \text{Ad}_{\mathbf{T}_{WB}}^{T}\end{aligned}\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;adjoints&quot;&gt;Adjoints&lt;/h1&gt;
&lt;p&gt;我们将介绍李群伴随的概念，这将帮助我们将右边的增量或矫正值与左边的增量或校正值联系起来。这种性质使我们能够用代数方法处理李群定义的不确定性，并得到不同协方差变换的表达式。我们将重点讨论3D构造，即 SE (3)
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Incremental-Segment-Based Localization in 3-D Point Clouds</title>
    <link href="http://yoursite.com/2021/05/09/Incremental_Segmental_localization/"/>
    <id>http://yoursite.com/2021/05/09/Incremental_Segmental_localization/</id>
    <published>2021-05-09T14:55:30.000Z</published>
    <updated>2021-05-09T15:02:58.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="incremental-segment-based-localization-in-3-d-point-clouds">Incremental-Segment-Based Localization in 3-D Point Clouds</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;incremental-segment-based-localization-in-3-d-point-clouds&quot;&gt;Incremental-Segment-Based Localization in 3-D Point Clouds&lt;/h1&gt;

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>LiDAR-Camera 标定-5</title>
    <link href="http://yoursite.com/2021/04/27/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_5/"/>
    <id>http://yoursite.com/2021/04/27/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_5/</id>
    <published>2021-04-26T20:55:30.000Z</published>
    <updated>2021-06-03T16:14:32.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="d激光雷达与全向相机外参标定">3D激光雷达与全向相机外参标定</h1><p><img src="http://s1.nsloop.com:59080/images/2021/04/26/20210426223058.png"></p><h1 id="摘要">摘要</h1><p>我们提出了一种使用全向摄像头系统对3D激光扫描仪进行外部校准的方法，该程序要求至少从3个视角同时从激光扫描仪和相机系统观察平面棋盘格图案，棋盘格的平面法向量和位于表面的3D点约束了激光扫描仪和全向相机系统的相对姿态，这些约束可用于形成外参标定的非线性优化问题，用于求解外参以及对应的协方差。</p><h1 id="介绍">介绍</h1><p>Scaramuzza et al. (2007).首先提出3d激光雷达与全向相机的外参标定问题，他们提出了使用手动选择关联点的方法完成标定。</p><p>本文提出的方法<strong>不需要手动选取对应点</strong>的3d激光雷达与全向相机的外参标定方法。</p><h1 id="方法">方法</h1><p>在我们的校准程序中，我们使用安装在平面表面上的棋盘图案，我们将从现在开始将其称为目标平面，外部校准的近似设置以及用于实验的感知传感器（Velodyne HDL-64e 3D激光扫描仪和Pointgrey Ladybug3全向相机在图1中示出。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/26/20210426225004.png"></p><h2 id="激光雷达内参标定">激光雷达内参标定</h2><p>有关传感器的更多技术细节，参见McBride（2008）。 计算出的范围测量<span class="math inline">\(D_l\)</span>由于TOF计算中的误差而包含一些偏差，因此具有来自实际测量的偏置校正<span class="math inline">\(\delta D\)</span>，因此制造商通常会为每条线束提供一个标定值<span class="math inline">\(\delta D\)</span>来进行补偿，通常使用图2的方法：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/26/20210426225828.png"></p><p>激光扫描仪安装在壁前的支撑件上，并且通过考虑墙壁和激光扫描仪之间的手动测量距离来计算范围测量中的偏移量：</p><p><span class="math display">\[    \delta D = D_m -D_l\]</span></p><ul><li><span class="math inline">\(D_m\)</span>是墙与雷达之间的距离手工测量值</li><li><span class="math inline">\(D_l\)</span>是tof测量值</li></ul><p>我们提出了一种鲁棒的方法来在范围测量中自动计算这种最佳偏移<span class="math inline">\(\delta D\)</span>。</p><p>与制造商采用的校准方法相反，该方法仅要求用户将激光雷达放置于墙或者平面的前面，然后记录测量数据，如图3所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/27/20210427091346.png"></p><p>在墙壁或平面表面前方的传感器平台的不同位置记录激光测量。 现在，如果我们使用（1）中计算的<span class="math inline">\(\delta D\)</span>校正，并考虑躺在墙壁上的点，它们应该都是共面的。</p><p>但是由于<span class="math inline">\(\delta D\)</span>并非真值，因此这些点的重投影误差是显著的。因此可以通过最小化重投影误差来求解。</p><p>我们使用Ransac（Fischler和Bolles（1981））来估算在墙壁上的所有点的最佳拟合平面的等式，首先生成一个包含目标平面以及潜在激光点的边界框。然后，这些潜在激光点<span class="math inline">\(\{\tilde{Q}_l^i ; i=1,2,\cdots,N\}\)</span>用于<code>RANSAC</code>进行平面拟合并返回内点。<code>RANSAC</code>步骤如下：</p><ol type="1"><li>随机选取3个点from<span class="math inline">\(\{\tilde{Q}_l^i ; i=1,2,\cdots,N\}\)</span></li><li>求解3个点的平面方程</li><li>遍历其他点，寻找内点</li><li>重复，直到找到最佳平面方程</li></ol><p>对<code>RANSAC</code>的最终结果进行参数化，<span class="math inline">\(\mathbf{N}=[n_x,n_y,n_z]^{\mathbf{T} }\)</span>，其中<span class="math inline">\(\|\mathbf{N}\|\)</span>表示平面到坐标系原点的距离。</p><p>因此，平面中的点<span class="math inline">\(\tilde{P}=[X,Y,Z]^{T}\)</span>在平面法向量<span class="math inline">\(\mathbf{N}\)</span>的投影等价于平面距离<span class="math inline">\(\|\mathbf{N}\|\)</span>，即：</p><p><span class="math display">\[    \mathbf{P}\cdot\mathbf{N} = \|\mathbf{N}\|^{2}\]</span></p><p>记<span class="math inline">\(D\)</span>为点的距离，<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\omega\)</span>分别记为俯仰角和方位角，则有：</p><p><span class="math display">\[\begin{aligned} X &amp;=D \cos \theta \sin \omega, \\ Y &amp;=D \cos \theta \cos \omega \\ Z &amp;=D \sin \theta \end{aligned}\]</span></p><p>所以，使用平面法向量表示的点距离可以表示为（联合上面所有式子可得）：</p><p><span class="math display">\[D=\frac{\|\mathbf{N}\|}{n_{x} \cos \theta \sin \omega+n_{y} \cos \theta \cos \omega+n_{z} \sin \theta}\]</span></p><p>所以，我们现在获得了从<code>RANSAC</code>方法获得的点距离，以及激光雷达本身直接返回的点距离，可以构造如下非线性最小二乘：</p><p><span class="math display">\[\delta D_{i}^{\prime}=\underset{\delta D_{i}^{\prime} }{\operatorname{argmin} } \sum_{i=1}^{64} \sum_{j=1}^{n}\left\|D_{i j}-\left(D_{l_{i j} }+\delta D_{i}^{\prime}\right)\right\|\]</span></p><ul><li><span class="math inline">\(D_{i j}\)</span>是根据<code>RANSAC</code>计算的平面方程后得到的点距离</li><li><span class="math inline">\(D_{l_{i j} }\)</span>是雷达直接返回的测距值</li></ul><p>经过补偿之后的结果如图4所示：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/27/20210427093756.png"></p><h2 id="全向相机">全向相机</h2><p><code>PointGrey Ladybug 3（LB3）</code>是高分辨率全向摄像机系统，它有六个200万像素摄像头，其中五个CCD位于水平环中，另外一个位于垂直方向，使系统能够从超过80％的球形范围内收集视频。</p><p>摄像机由制造商预校准，因此单个相机的内在参数是已知的，同样的，以公共坐标系为参考基准（称为<code>camera head</code>），所有摄像头相对于参考坐标系的刚体变换也是已知的。</p><p>因此，我们需要估计<code>camera head</code>的姿势（相对于一些本地参考帧），以便我们可以代表相机头帧中的任何3D点，然后到任何相机的坐标系。</p><h2 id="激光雷达与全向相机外参标定">激光雷达与全向相机外参标定</h2><p>外参标定方法与Zhang(2004)的方法相似，要求系统在不同的位姿观察平面pattern（如棋盘格），并根据激光雷达和相机同时观测的数据建立约束。</p><p>目标平面的法向量以及平面上的激光点之间存在关系，可用于约束相机和激光雷达的相对位姿关系。我们已知目标平面的方程，为了方便起见，以该平面构建坐标系，令</p><p><span class="math display">\[    Z=0\]</span></p><p>令：</p><ul><li><span class="math inline">\(\tilde{P}_{w}\)</span>为在世界参考坐标系中的点(此处是attach到目标平面的坐标系)</li><li><span class="math inline">\({ }_{w}^{c_{i} } R\)</span>是从世界坐标系<span class="math inline">\(w\)</span>到第i帧相机坐标系<span class="math inline">\(c_i\)</span>的旋转变换</li><li><span class="math inline">\({}^\mathrm{c_i}\mathrm{t}_{\mathrm{c}_{\mathrm{i} } \mathrm{w} }\)</span>是平移量</li></ul><p>因此，将一个世界参考坐标系中的点转换到第i帧相机参考坐标系可表示为：</p><p><span class="math display">\[\tilde{P}_{c_{i} }={ }_{w}^{c_{i} } R \tilde{P}_{w}+{ }^{\mathbf{c}_{\mathbf{i} } } \mathbf{t}_{\mathbf{c}_{\mathbf{i} \mathbf{w} } }\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\tilde{P}_{c_{i} }\)</span>是世界坐标系投影到第i帧相机坐标系的点</li></ul><p>由于已知每个相机与<code>camera head</code>的相对位姿<span class="math inline">\({ }_{c_{i} }^{h} R,{ }^{\mathrm{h} } \mathbf{t}_{\mathbf{h c}_{\mathbf{i} } }\)</span>，因此可以将第i个相机坐标系中的点转换到<code>camera head</code>坐标系中：</p><p><span class="math display">\[\tilde{P}_{h}={ }_{c_{i} }^{h} R \tilde{P}_{c_{i} }+{ }^{\mathbf{h} } \mathbf{t}_{\mathbf{h c}_{\mathbf{i} } }\]</span></p><p>因此，如果已知<span class="math inline">\({ }_{w}^{c_{i} } R\)</span>,<span class="math inline">\({}^\mathrm{c_i}\mathrm{t}_{\mathrm{c}_{\mathrm{i} } \mathrm{w} }\)</span>，就可以将世界坐标系中目标平面上的点<span class="math inline">\(\tilde{P}_w\)</span>转换到<code>camera head</code>坐标系中。</p><p>论文采用了<code>(Zhang (1998))</code>的方法来获取相对目标平面（也就是棋盘格坐标系作为世界坐标系）的位姿变换。</p><p>特别的，对于<code>pin-hole</code>相机模型，3d点投影关系如下：</p><p><span class="math display">\[\tilde{p} = K_i T_{w}^{ci} \tilde{P}_{w}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(K_i\)</span>为3x4矩阵，是第i个相机的内参矩阵</li><li><span class="math inline">\(\tilde{P}_{w}\)</span>为世界参考坐标系（棋盘格坐标系）下的点的齐次表达形式<span class="math inline">\(\tilde{P}_{w}=\left[\begin{array}{llll}X &amp; Y &amp; Z &amp; 1\end{array}\right]^{\top}\)</span></li><li><span class="math inline">\(\tilde{p}\)</span>为投影得到的图像点<span class="math inline">\(\tilde{p}=[u , v , 1]^{\top}\)</span></li><li><span class="math inline">\(T_{w}^{ci}\)</span>为外参，是世界坐标系到相机坐标系的变换</li></ul><p>假设图像点受独立同分布的噪声影响，外参<span class="math inline">\(T_{w}^{ci}\)</span>的最大似然估计可以使用如下最小化重投影误差来表示：</p><p><span class="math display">\[\underset{ { }_{w}{ }_{i} R,{ }^{\mathrm{c} } \mathbf{i} \mathbf{t}_{\mathbf{c}_{\mathbf{w} } \mathbf{w} } }{\operatorname{argmin} } \sum_{k=1}^{n} \sum_{j=1}^{m}\left\|p_{\hat{k} j}-K_{i}\left[{ }_{w}^{c_{i} } R{ }^{\mathbf{c}_{\mathbf{i} } } \mathbf{t}_{\mathbf{c}_{\mathbf{i} \mathbf{w} } }\right] \tilde{P}_{j}\right\|\]</span></p><p>其中，</p><ul><li><span class="math inline">\(n\)</span>表示n张图片</li><li><span class="math inline">\(m\)</span>表示每张图片对应的m个点</li><li><span class="math inline">\({ }_{w}^{c_{i} } R=[\boldsymbol{r}_1,\boldsymbol{r}_2,\boldsymbol{r}_3]\)</span></li><li><span class="math inline">\({}^\mathrm{c_i}\mathrm{t}_{\mathrm{c}_{\mathrm{i} } \mathrm{w} }\)</span>使用3维欧氏向量表示</li></ul><p>因此，根据:</p><p><span class="math display">\[\begin{aligned}    Rp+t &amp;= P_{new} \\    p &amp;= R^{-1}(P_{new}-t)\end{aligned}\]</span></p><p>有：在第i个相机坐标系中的目标平面方程可以写成（利用平面<span class="math inline">\(Z=0\)</span>的条件）：</p><p><span class="math display">\[\mathbf{r}_{3} \cdot\left(\mathbf{p}+{ }^{\mathbf{c}_{\mathbf{i} } } \mathbf{t}_{\mathbf{c}_{\mathbf{i} \mathbf{w} } }\right)=0\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\boldsymbol{r}_3\)</span>是旋转矩阵<span class="math inline">\({ }_{w}^{c_{i} } R\)</span>第三列</li><li>旋转矩阵<span class="math inline">\({ }_{w}^{c_{i} } R\)</span>第三列与点做<code>点乘</code>相当于旋转矩阵的转置后的第3行与点做矩阵乘法，得到转换后的点的Z轴分量</li><li>同时，<span class="math inline">\(\boldsymbol{r}_3\)</span>也是世界坐标系（棋盘格坐标系）的法向量方向</li><li><span class="math inline">\(\boldsymbol{p}\)</span>是相机坐标系中，平面上的点</li></ul><p>目标平面的法向量在第i个相机坐标系表示如下：</p><p><span class="math display">\[\mathbf{N}_{\mathbf{c}_{\mathbf{i} } }=\left(\mathbf{r}_{\mathbf{3} } \cdot{ }^{\mathbf{c}_{\mathbf{i} } } \mathbf{t}_{\mathbf{c}_{\mathbf{i} \mathbf{w} } }\right) \mathbf{r}_{\mathbf{3} }\]</span></p><ul><li>由于<span class="math inline">\(\boldsymbol{r}_3\)</span>也是世界坐标系（棋盘格坐标系）的法向量方向，所以<span class="math inline">\(\|\mathbf{N}_{c_i}\|= \mathbf{r}_3 \cdot { }^{\mathbf{c}_{\mathbf{i} } } \mathbf{t}_{\mathbf{c}_{\mathbf{i} \mathbf{w} } }\)</span> 表示第i个相机坐标系到目标平面的距离 (注意是点乘)</li></ul><p>又因为第i个相机坐标系相对于<code>camera head</code>的位姿变换已知，因此可以计算以<code>camera head</code>坐标系为参考的目标平面法向量<span class="math inline">\(\mathbf{N}_h\)</span>：</p><p><strong>（注意，这不是简单的向量做旋转，因为距离也会改变，即<span class="math inline">\(\| N_h\|\)</span>是目标平面到<code>camera head</code>坐标系原点的距离），因此还要加上刚体变换的平移部分在法向量方向上的投影</strong></p><p><span class="math display">\[\mathbf{N}_{\mathbf{h} }=\frac{{ }^{h} R \mathbf{N}_{\mathbf{c}_{\mathbf{i} } } }{\left\|\mathbf{N}_{\mathbf{c}_{\mathbf{i} } }\right\|}\left(\left\|\mathbf{N}_{\mathbf{c}_{\mathbf{i} } }\right\|+\mathbf{N}_{\mathbf{c}_{\mathbf{i} } } \cdot \mathbf{h}_{\mathbf{t} \mathbf{h c}_{\mathbf{i} } }\right)\]</span></p><ul><li>一旦我们已知了目标平面法向量在<code>camera head</code>坐标系的表示，我们需要找到激光雷达坐标系中在目标平面中的3d点</li><li>我们使用上述<code>RANSAC</code>方法来提取这些3d点</li><li>上述两种信息为外参标定提供了约束条件</li></ul><p>令<span class="math inline">\(\{\tilde{P}_l^i ; i=1 , 2, \cdots ,n\}\)</span>为提取的目标平面中的激光雷达点，利用待估计的外参，可以转换到<code>camera head</code>坐标系中：</p><p><span class="math display">\[\tilde{P}_{h}^{i}={ }_{l}^{h} R \tilde{P}_{l}^{i}+{ }^{\mathbf{h} } \mathbf{t}_{\mathbf{h l} }\]</span></p><p>现在，如果将一束光线从<code>camera head</code>坐标系开始投射到目标平面上的一点，那么这个射线在平面法向量上的投影就等于从<code>camera head</code>坐标系到目标平面的距离，因此，从m个不同的视角获取数据，可以构造如下目标函数：</p><p><span class="math display">\[F=\sum_{i=1}^{m} \sum_{j=1}^{n}\left(\frac{\mathbf{N}_{\mathbf{h} }^{\mathbf{i} } }{\left\|\mathbf{N}_{\mathbf{h} }^{\mathrm{i} }\right\|} \cdot\left({ }_{l}^{h} R \mathbf{P}_{l}^{j}+{ }^{\mathbf{h} } \mathbf{t}_{\mathbf{h l} }\right)-\left\|\mathbf{N}_{\mathbf{h} }^{\mathbf{i} }\right\|\right)^{2}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{N}_{\mathbf{h} }^{\mathbf{i} }\)</span>是目标平面在<code>camera head</code>坐标系中的第i个位姿对应的法向量</li><li>第一项表示激光雷达点在<code>camera head</code>坐标系构成的向量，在法向量方向的投影</li><li>第二项表示由相机信息获取的<code>camera head</code>坐标系与目标平面的距离</li></ul><h2 id="外参标定需要的最少视角个数">外参标定需要的最少视角个数</h2><p>需要至少三个目标平面的非共面观点来完全限制优化问题（17）以估计校准参数，</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/28/20210428091916.png"></p><p>只有一个平面的情况：如图5(a)所示：</p><ul><li>当保持姿态不变，沿目标平面萍乡的方向进行平移时，目标函数的值不变</li><li>以平面法向量为轴进行旋转时，目标函数的值也不变</li></ul><p>相似的，对于只有两个视角的情况：如图5(b)所示：</p><ul><li>传感器沿着平面交叉线平移并不改变目标函数值，从而在该方向上产生大的不确定性</li></ul><h2 id="估计参数的协方差">估计参数的协方差</h2><p>通过最小化目标函数得到的参数估计会由于传感器测量的不确定性导致具有一定的误差，如激光雷达测距误差约为0.02m。知道这种不确定性是非常重要的，以便在任何视觉或SLAM算法中使用此处计算的参数。</p><p>Haralick（1998）已经描述了通过任何标量非线性优化函数传播测量协方差的方法，唯一的假设是标量函数是非负的，具有有限的第一和二阶偏微分，对于理想数据即其值为零，并且输入中的随机扰动足够小，以便可以近似输出一阶泰勒系列扩张。</p><p>前文提出的目标函数满足这些假设，因此可以用这个方法来估计参数的协方差。</p><p>假设<code>camera head</code>坐标系相对于激光雷达坐标系可以用参数来表示：</p><p><span class="math display">\[\Theta=\left[{ }^{1} \mathbf{t}_{\mathbf{l h} }, \mathbf{\Phi}_{\mathbf{l h} }\right]^{\top}\]</span></p><p>其中，</p><ul><li><span class="math inline">\({ }^{1} \mathbf{t}_{\mathbf{l h} }=\left[t_{x}, t_{y}, t_{z}\right]^{\top}\)</span>表示从h坐标系到l坐标系的平移变换（在l坐标系的表示）</li><li><span class="math inline">\(\boldsymbol{\Phi}_{\mathrm{lh} }=\left[\theta_{x}, \theta_{y}, \theta_{z}\right]^{\top}\)</span>表示从h坐标系到l坐标系旋转</li></ul><p>关于参数<span class="math inline">\(\Theta\)</span>的协方差如下：</p><p><span class="math display">\[\Sigma_{\Theta}=\left[\frac{\partial^{2} F}{\partial \Theta^{2} }(X, \Theta)\right]^{-1} \frac{\partial^{2} F^{T} }{\partial X \partial \Theta}(X, \Theta) \Sigma_{X} \frac{\partial^{2} F}{\partial X \partial \Theta}(X, \Theta)\left[\frac{\partial^{2} F}{\partial \Theta^{2} }(X, \Theta)\right]^{-1}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(X=\left[\mathbf{N}_{\mathbf{h} }^{\mathbf{1} }, \tilde{P}_{l}^{1}, \tilde{P}_{l}^{2} \ldots \mathbf{N}_{\mathbf{h} }^{\mathbf{i} }, \tilde{P}_{l}^{1}, \ldots\right]^{T}\)</span>表示观测信息（包括平面法向量和目标平面中的激光点）</li></ul><h1 id="实验">实验</h1><h2 id="仿真实验略">仿真实验（略）</h2><h2 id="真实环境实验">真实环境实验</h2><p>所提出的外在校准方法已经在由安装有3D激光传感器和全向相机系统的车辆收集的实际数据上进行了测试，如图8所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/28/20210428095042.png"></p><p>我们有两套结果验证算法的准确性，在第一种情况下，我们考虑了类似于校准设置的设置：在车库内进行校准，棋盘图案安装在所有可用的平面表面上（包括侧壁和底层），如图9所示，由不同颜色表示的来自不同平面的点已经投影到相应的图像上。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/28/20210428095326.png"></p><p>在第二种情况下，我们将车辆从车库外面带走，并从福特校园周围的行驶中的车辆中收集了一些数据，在全向相机系统的5个摄像机上的5个摄像机上的360度视场的投影的结果如图10所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/28/20210428095455.png"></p><h1 id="代码库">代码库</h1><p>https://github.com/SubMishMar/cam_lidar_calib</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;d激光雷达与全向相机外参标定&quot;&gt;3D激光雷达与全向相机外参标定&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com:59080/images/2021/04/26/20210426223058.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;摘要&quot;&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>UTC Time and GPS Time Conversion</title>
    <link href="http://yoursite.com/2021/04/21/utc%E6%97%B6%E9%97%B4%E4%B8%8Egps%E6%97%B6%E9%97%B4/"/>
    <id>http://yoursite.com/2021/04/21/utc%E6%97%B6%E9%97%B4%E4%B8%8Egps%E6%97%B6%E9%97%B4/</id>
    <published>2021-04-21T06:55:30.000Z</published>
    <updated>2021-04-21T07:50:30.793Z</updated>
    
    <content type="html"><![CDATA[<h1 id="时钟系统的前世今生">时钟系统的前世今生</h1><blockquote><p>最近在完善惯导ros驱动，发现GPGGA语句和GPFPD语句输出的时间不一致，一个是utc时间，另一个是gps时间。为了实现时钟同步，就要完成二者之间的转换。</p></blockquote><h2 id="格林威治标准时间greenwich-mean-timegmt">格林威治标准时间（Greenwich Mean Time，GMT）</h2><p><code>格林尼治平均时间（Greenwich Mean Time，GMT）</code>是指位于英国伦敦郊区的皇家格林尼治天文台当地的平太阳时，格林尼治标准时间的正午是指当平太阳横穿格林尼治子午线时（也就是在格林尼治上空最高点时）的时间。由于地球每天的自转是有些不规则的，而且正在缓慢减速，因此格林尼治平时基于天文观测本身的缺陷，已经被原子钟报时的协调世界时（UTC）所取代。</p><blockquote><p>自1924年2月5日开始，格林尼治天文台负责每隔一小时向全世界发放调时信息。</p></blockquote><h2 id="世界时universal-time-ut">世界时（Universal Time, UT）</h2><p>后来，由于1925年以前人们在天文观测中，常常把每天的起始（0时）定为正午，而不是通常民用的午夜，给格林尼治平时的意义造成含糊，人们使用<code>世界时（Universal Time, UT）</code>一词来明确表示每天从午夜开始的格林尼治平时。</p><p>世界时是以地球自转为基准得到的时间尺度，其精度受到地球自转不均匀变化和极移的影响，为了解决这种影响，1955年国际天文联合会定义了UT0、UT1和UT2三个系统：</p><ul><li>UT0系统是由一个天文台的天文观测直接测定的世界时，没有考虑极移造成的天文台地理坐标变化。</li><li>UT1系统是在UT0的基础上加入了极移改正 Δλ，修正地轴摆动的影响。UT1是目前使用的世界时标准。被作为目前世界民用时间标准UTC在增减闰秒时的参照标准。</li><li>UT2系统是UT1的平滑处理版本，在UT1基础上加入了地球自转速率的季节性改正 ΔT。</li></ul><p>目前使用的世界时测算标准又称UT1。在UT1之前人们曾使用过UT0，但由于UT0没有考虑极移导致的天文台地理坐标变动的问题，因此测出的世界时不准确，现在已经不再被使用。</p><p>在UT1之后，由于人们发现，因为<strong>地球自转本身不均匀的问题</strong>，UT1定义的时间的流逝仍然不均匀，于是人们又发展了一些对UT1进行平滑处理后的时间标准，包括UT1R和UT2，但它们都未能彻底解决定义的时间的流逝不均匀的问题，这些<strong>时间标准现在都不再被使用</strong>。</p><h2 id="原子时international-atomic-time-tai">原子时（International Atomic Time, TAI）</h2><p>为了彻底解决定义的时间的流逝不均匀的问题，开始使用原子钟定义时间。人们首先用全世界的原子钟共同为地球确立了一个均匀流动的时间，称为<code>国际原子时（International Atomic Time, TAI）</code>。</p><blockquote><p>1967年第13届国际计量大会上通过一项决议，定义一秒为铯-133原子基态两个超精细能级间跃迁辐射振荡9,192,631,770周所持续的时间。[2][3]其起点为世界时1958年的开始。</p></blockquote><p>原子时起点定在1958年1月1日0时0分0秒（UT），即规定在这一瞬间原子时时刻与世界时刻重合。但事后发现，在该瞬间原子时与世界时的时刻之差为0.0039秒。这一差值就作为历史事实而保留下来。在确定原子时起点之后，由于<code>地球自转速度</code>的问题，使得原子时钟不能与世界时间保持协调。</p><h2 id="协调世界时coordinated-universal-time-utc">协调世界时（Coordinated Universal Time, UTC）</h2><p>为了使定义的时间与地球自转相配合，人们通过在TAI的基础上不定期<code>增减闰秒</code>的方式，使定义的时间与世界时（UT1）保持差异在0.9秒以内，这样定义的时间就是<code>协调世界时（Coordinated Universal Time, UTC）</code>。</p><blockquote><p>协调世界时是最接近格林威治标准时间（GMT）的几个替代时间系统之一。对于大多数用途来说，UTC时间被认为能与GMT时间互换，但<strong>GMT时间已不再被科学界所确定</strong>。</p></blockquote><p>UTC基于国际原子时，并通过不规则的加入闰秒来抵消地球自转变慢的影响。闰秒在必要的时候会被插入到UTC中，以保证协调世界时（UTC）与世界时（UT1）相差不超过0.9秒。</p><blockquote><p>这就是所谓的跳秒，由于需要适应地球自转变化，需要在不定时进行跳秒，截止2019年2月，已经18次跳秒。正因为跳秒的存在，才会导致后面介绍的GPS时与UTC时不一致。</p></blockquote><h2 id="gps时">GPS时</h2><p>GPS时是用于卫星定位系统时间，由于卫星系统是连续运行的，其要求时间系统也是连续的，因此采用原子钟的方法。GPS时间系统就是采用基于美国海军观测实验室维持的原子时。</p><p>GPS时在1980年1月6日0点0分与世界协调时(UTC)一致，此后就只按原子时来累计，不受外界影响，也不会产生跳秒。因此与UTC时间的差为秒的整数倍，即:</p><p><span class="math display">\[    Time_{GPS} = Time_{UTC}+n\]</span></p><p>特别的，GPS时间的计时方法采用星期数和秒周数来表示，其中周数作为C/A和P码中的十位字段发送，所以<span class="math inline">\(2^{10}=1024\)</span>周(19.6年)后会再次归零。</p><p>为了解决这个问题，现代化的GPS导航消息采用了13位的字段，每隔8192周(157年)才归零。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;时钟系统的前世今生&quot;&gt;时钟系统的前世今生&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;最近在完善惯导ros驱动，发现GPGGA语句和GPFPD语句输出的时间不一致，一个是utc时间，另一个是gps时间。为了实现时钟同步，就要完成二者之间的转换。&lt;/p&gt;
&lt;/bloc
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>LiDAR-Camera 标定-4</title>
    <link href="http://yoursite.com/2021/04/14/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_4/"/>
    <id>http://yoursite.com/2021/04/14/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_4/</id>
    <published>2021-04-13T20:55:30.000Z</published>
    <updated>2021-06-03T16:14:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spatiotemporal-calibration-of-camera-and-3d-laser-scanner">Spatiotemporal Calibration of Camera and 3D Laser Scanner</h1><p><img src="http://s1.nsloop.com:59080/images/2021/04/14/20210414165501.png"></p><h1 id="摘要">摘要</h1><p>我们提出了一种用于相机和3D激光扫描仪的开源时空校准框架。我们的解决方案基于常用的棋盘标记，需要在操作前进行一分钟校准，以提供准确和可重复的结果。该框架基于点对平面约束的批量优化，并且可以通过新颖的最小化来实现时间偏移校准，李代数中平面方程的连续表示以及B样条的使用。在仿真中评估了框架的属性，同时使用Velodyne VLP-16和SICK MRS6124 3D激光扫描仪通过两种不同的感官设置验证了性能。</p><h1 id="介绍">介绍</h1><p>提出的标定方法基于棋盘格的运动，以及一系列的来自相机和激光雷达的观测。然后执行批量优化来估计6dof刚体变换矩阵以及时间偏差。</p><p>该优化基于独立的激光雷达指向相机的平面约束，该平面约束由连续时间平面表示扩展，使得时间偏差可以得到优化。本文贡献在：</p><ul><li>使用通用的棋盘格标记来估计lidar、camera的外参和时间偏移</li><li>新颖的李代数形式的连续时间最小化平面表示，使用B样条</li><li>对初值不敏感，可以收敛到预期结果</li></ul><h1 id="方法">方法</h1><p>提出的框架专用于刚体连接的具有全局快门相机和3D Lidar，并且假设相机内参已知且已经校正了。此外，假设传感器之间的时间偏移未知，且较小近似于常值。</p><p>标定过程需要包含标定板与传感器之间的相对运动，因此应该被激光雷达和相机连续观测得到。录制数据之后，按照下图的过程进行处理：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/14/20210414171512.png"></p><p>其中，对于相机，则使用opencv传统检测算法进行检测；对于激光雷达，首先将点投影成range-image，然后通过手动标记或半自动跟踪的方法来提取标定板对应的平面点。</p><p>如果不考虑时间偏移，标定可以描述为基于待求解位姿的点-面优化问题：</p><p><span class="math display">\[T^{*} = \arg \min \sum_{i} \sum_{j} \pi(t_i)^{T}Tp_{i}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(T^{*}\)</span>是期望得到的从激光雷达坐标系到相机坐标系的变换矩阵</li><li><span class="math inline">\(p_i\)</span>表示标定板上第i个3D点的齐次坐标表示</li><li><span class="math inline">\(\pi(t_i)\)</span>表示由相机在<span class="math inline">\((t_i+\Delta t)\)</span>时刻估计得到的标定板平面等式。</li></ul><p>提出的方法还注意到时间的校准，即考虑到点云中的每个点都对应一个时间戳，则目标函数变为：</p><p><span class="math display">\[T^{*},\Delta t^{*} = \arg \min \sum_{i} \sum_{j} \pi(t_i+\Delta t)^{T}Tp_{i}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\Delta t^{*}\)</span>是待估计的时间差</li><li>要形成上式，需要知道每个3d激光点对应的时间戳以及对应任意时间的平面表示</li><li>本文使用LM以及g2o来求解</li></ul><h2 id="d激光雷达时间偏移">3D激光雷达时间偏移</h2><p>对于机械旋转式激光雷达，都有一定的扫描周期，根据扫描起始和扫描结束，可以推断出对应点的时间：</p><h3 id="vlp16">vlp16</h3><p><span class="math display">\[t_{i}=t_{\text {cloud }}+\frac{\phi_{i}-\phi_{s}}{f\left(\phi_{e}-\phi_{s}\right)}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(t_{cloud}\)</span>表示扫描起始时间</li></ul><h3 id="sick-mrs6124">sick-mrs6124</h3><p><img src="http://s1.nsloop.com:59080/images/2021/04/15/20210415101745.png"></p><h2 id="连续时间平面表示">连续时间平面表示</h2><p>标定板在每一帧图像中都能检测到，由于相机内参、以及标定板物理参数已知，所以可以确定标定板坐标系到相机坐标系的变换。</p><p>因此，标定板的平面可以使用4维向量<span class="math inline">\((\bm{n},d)\)</span>来表示，其中<span class="math inline">\(\bm{n}\)</span>是平面法向量，<span class="math inline">\(d\)</span>是到坐标系原点的距离。对于所有的图像，可以获取到离散时间下的一组平面方程集合。</p><p>为了确定各个时间戳的平面方程，需要对这些方程进行插值，因为4维的平面表示形式并非最小的表示方式，因此可能还需要做归一化处理。为了避免这个问题，提出一种使用类似于SO(3)以及对应的李代数so(3)的思想来表示平面法向量</p><p>该思想只需要使用两个参数就可以表示归一化法向量，因此，提出的方法几乎与[21]提出的球面插值法相同。</p><p>因此，平面的超参数化是因为法向量<span class="math inline">\(\bm{n}\)</span>，如果使用两个成分来表示，那么平面可以表示为：<span class="math inline">\((\bm{n},d) \rightarrow (\omega_x,\omega_y,d)_{3\times 1}\)</span>:</p><p><span class="math display">\[\begin{aligned}\theta = acos(\bm{n}[2]) \\\omega_{x}=-\bm{n}[1]*\frac{\theta}{\sin \theta} \\\omega_{y}=\bm{n}[0]*\frac{\theta}{\sin \theta}\end{aligned}\]</span></p><blockquote><p>这个思想来源于 [20]A. Bartoli, “On the non-linear optimization of projective motion using minimal parameters“ European Conference on Computer Vision (ECCV), Copenhagen, 2002, 340–354. <img src="http://s1.nsloop.com:59080/images/2021/04/15/20210415161745.png"></p></blockquote><p>就是说，在3维欧氏空间中，以点(0,0,1)作为原点，以平行于x轴、y轴的方向作为坐标轴，展开一个超平面，那么3维欧氏空间中的矢量可以通过对数变换投影到该超平面上的一个点。</p><p><strong>二维情况</strong></p><p><img src="http://s1.nsloop.com:59080/images/2021/04/15/20210415163816.png"></p><p><strong>三维情况</strong></p><p><img src="http://s1.nsloop.com:59080/images/2021/04/15/20210415204817.png"></p><p>因此，选取3维欧氏空间中的单位向量<span class="math inline">\(q=[0,0,1]^{T}\)</span>，然后根据<span class="math inline">\(\cos \theta = q^{T} n\)</span>，那么可以得到<span class="math inline">\(\theta = \arccos (q^{T}n)=\arccos(\bm{n}[2])\)</span>，表示两个向量之间的夹角，特别的又因为球面半径是单位向量，因此，<span class="math inline">\(\theta\)</span>也表示两个向量之间的球面距离。</p><p>特别的，根据下式可构成超平面上的点<span class="math inline">\(p=(\omega_x,\omega_y)\)</span>：</p><p><span class="math display">\[\begin{aligned}\theta = \arccos(\bm{n}[2]) \\\omega_{x}=-\bm{n}[1]*\frac{\theta}{\sin \theta} \\\omega_{y}=\bm{n}[0]*\frac{\theta}{\sin \theta}\end{aligned}\]</span></p><p>其中，</p><p><span class="math display">\[\begin{aligned}    \sqrt{\omega_x^2 + \omega_y^2}     &amp;=     \sqrt{        \frac{\arccos^{2} (n[2])}{\sin^{2}( \arccos(n[2])) }        (n[0]^{2}+n[1]^{2})    }    \\    &amp;=    \sqrt{        \frac{\arccos^{2} (n[2])}{1- \cos^{2}(\arccos(n[2])) }        (1-n[2]^{2})    }    \\    &amp;= \arccos (n[2]) = \theta\end{aligned}\]</span></p><p>可以发现，在超平面上，点q和点p的距离等于球面上向量q和另一个向量之间的球面距离。</p><p>因此，实现了使用两个参数来表示欧氏空间中的向量，需要注意的是，<span class="math inline">\(\theta==0\)</span>时，取<span class="math inline">\(\frac{\theta}{\sin \theta}=1\)</span>，特别的，仅适用于<span class="math inline">\(\theta &lt; \pi\)</span>的情况下。</p><h2 id="平面方程插值">平面方程插值</h2><p>使用最小化的平面表示使得可以进行插值，然后返回到所需时间戳对应的4维的平面方程表示形式。</p><p>对于此任务，我们尝试了参数的线性插值，由于缺乏连续的微分导致优化陷入局部最小值。因此，提出了使用三次样条插值，确保最小平面表示具有一阶和二阶的连续微分。如下：</p><p><span class="math display">\[\mathbf{s}(t)=\mathbf{s}_{0} B_{0}(t)+\sum_{i=1}^{n}\left(\mathbf{s}_{i}-\mathbf{s}_{i-1}\right) B_{i}(t)\]</span></p><p>其中，</p><ul><li><span class="math inline">\(s(t)\)</span>是在时间t插值得到的结果</li><li><span class="math inline">\(s_i\)</span>是根据测量得到的第i个控制点的值</li><li><span class="math inline">\(B_i(t)\)</span>是cumulative basis function的第i个成分</li><li><span class="math inline">\(n\)</span>是B样条的阶数</li></ul><p>对于李代数，则有插值方程如下：</p><p><span class="math display">\[\mathbf{r}(t)=\log \left\{\exp \left(\mathbf{r}_{0} B_{0}(t)\right) \prod_{i=1}^{n} \exp \left(\boldsymbol{\Omega}_{i} B_{i}(t)\right)\right\}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(r(t)\)</span>表示t时刻的李代数插值结果</li><li><span class="math inline">\(r_i\)</span>表示李代数表示的第i个控制点</li><li><span class="math inline">\(\Omega_i=\log(\exp(r_{i-1})^{T}\exp(r_i))\)</span>表示两个李代数之间的差值</li></ul><blockquote><p>来源于文献[23]：A. Patron-Perez, S. Lovegrove, and G. Sibley, “A spline-based trajec- tory representation for sensor fusion and rolling shutter cameras“, in International Journal of Computer Vision, vol. 113, no. 3, 208–219, 2015.</p></blockquote><p>使用4阶B样条插值，cumulative basis function 如下：</p><p><span class="math display">\[\mathbf{B}(u)=\frac{1}{6}\left[    \begin{array}{cccc}    6 &amp; 0 &amp; 0 &amp; 0 \\     5 &amp; 3 &amp; -3 &amp; 1 \\     1 &amp; 3 &amp; 3 &amp; -2 \\     0 &amp; 0 &amp; 0 &amp; 1\end{array}    \right]    \left[\begin{array}{c}    1 \\ u \\ u^{2} \\ u^{3}    \end{array}    \right]\]</span></p><p>其中，</p><ul><li><span class="math inline">\(u\)</span>是从t2到t3之间的归一化时间(0~1)，在实践中，我们还检查了检测到的棋盘的时间戳（T1，T2，T3，T4）是否均匀地分布在时间内，如果不满足这种情况，则不会执行插值</li></ul><p>重要的是，可以使用[23]的已知方程来导出B样条曲线的jacobians，特别的，本系统使用了[24]所述的更有效的雅可比表示。</p><blockquote><p>[24]C. Sommer, V. Usenko, D. Schubert, N. Demmel, and D. Cremers, “Efficient derivative computation for cumulative B-splines on Lie groups“, arXiv preprint, 1911.08860v1, 2019.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;spatiotemporal-calibration-of-camera-and-3d-laser-scanner&quot;&gt;Spatiotemporal Calibration of Camera and 3D Laser Scanner&lt;/h1&gt;
&lt;p&gt;&lt;img sr
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
