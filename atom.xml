<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>EpsilonJohn&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-07-24T14:07:21.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>EpsilonJohn</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MSCKF论文阅读</title>
    <link href="http://yoursite.com/2021/07/20/MSCKF%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/07/20/MSCKF%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-07-20T14:03:53.000Z</published>
    <updated>2021-07-24T14:07:21.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="a-multi-state-constraint-kalman-filter-for-vision-aided-inertial-navigation">A Multi-State Constraint Kalman Filter for Vision-aided Inertial Navigation</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720220945.png"></p><h1 id="摘要">摘要</h1><p>本文介绍了一个基于扩展卡尔曼滤波器的算法，用于实时视觉辅助的惯性导航算法。本项工作的主要贡献是观测模型的导出，其能够表达从多个相机pose观察到静态特征所构成的几何约束。该测量模型不需要在EKF的状态向量中包含3D特征点的位置。</p><p>提出的视觉辅助惯性导航算法的计算复杂性只与特征点数量线性相关，并且能够在大型现实环境中进行高精度的姿态估计。</p><h1 id="系统描述">系统描述</h1><p>目标是估计与IMU固定的坐标系相对于全局参考坐标系的3D位姿。</p><p>为了简化地球自转对IMU测量的影响，本文选取全局坐标系为ECEF坐标系，整体算法流程如<strong>算法1</strong>所示.</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/24/20210724152005.png"></p><p>IMU测量被用于进行实时处理，用于传播EKF状态和协方差（Section III-B）。另一方面，每一帧图像到达时，相机的位姿估计被添加到状态向量中（Section III-C）。状态增广是为了处理特征观测所必须的，因为每个跟踪的特征点的观测用于在所有相机位姿中施加约束，从而用于对观测到该特征点的相机位姿进行约束。</p><p>因此，在任何时候，EKF状态向量都包含：</p><ul><li>IMU状态</li><li>过去Nmax帧相机位姿</li></ul><h2 id="ekf状态向量的结构">EKF状态向量的结构</h2><p>IMU状态：</p><p><span class="math display">\[\mathbf{X}_{\mathrm{IMU} }=\left[\begin{array}{lllll}{ }_{G}^{I} \bar{q}^{T} &amp; \mathbf{b}_{g}{ }^{T} &amp; { }^{G} \mathbf{v}_{I}^{T} &amp; \mathbf{b}_{a}{ }^{T} &amp; { }^{G} \mathbf{p}_{I}^{T}\end{array}\right]^{T}\]</span></p><p>其中，</p><ul><li>单位四元数[19] <span class="math inline">\({ }_{G}^{I} \bar{q}\)</span>用于描述从全局坐标系<span class="math inline">\(G\)</span>到IMU坐标系<span class="math inline">\(I\)</span>的旋转</li><li><span class="math inline">\({ }^{G} \mathbf{p}_{I}\)</span>和<span class="math inline">\({ }^{G} \mathbf{v}_{I}\)</span>分别描述了IMU位置和速度，相对于全局坐标系<span class="math inline">\(G\)</span></li><li><span class="math inline">\(b_g\)</span>和<span class="math inline">\(b_a\)</span>是 3x1的向量，描述IMU的bias，IMU Biases被建模为受高斯噪声<span class="math inline">\(n_{wg},n_{wa}\)</span>驱动的随机游走过程。</li></ul><p>因此，IMU的误差状态模型被定义为如下：</p><p><span class="math display">\[\widetilde{\mathbf{X} }_{\mathrm{IMU} }=\left[\begin{array}{lllll}\boldsymbol{\delta} \boldsymbol{\theta}_{I}^{T} &amp; \widetilde{\mathbf{b} }_{g}^{T} &amp; { }^{G} \widetilde{\mathbf{v} }_{I}^{T} &amp; \widetilde{\mathbf{b} }_{a}^{T} &amp; { }^{G} \widetilde{\mathbf{p} }_{I}^{T}\end{array}\right]^{T}\]</span></p><p>对于位置、速度、biases，使用了标准的误差add定义，如位置误差<span class="math inline">\(\widetilde{x}=x-\hat{x}\)</span>，然而，对于四元数，则使用另外的准则。</p><p>实际上，如果<span class="math inline">\(\hat{\bar{q} }\)</span>是四元数<span class="math inline">\(\bar{q}\)</span>的估计值，那么，旋转误差可以描述为四元数误差<span class="math inline">\(\delta \bar{q}\)</span>，其中<span class="math inline">\(\bar{q}=\delta \bar{q} \otimes \hat{\bar{q} }\)</span> (这个要区分误差是定义在哪里，这里跟ESKF一样，定义在局部的，只不过这里的q是从全局坐标系到机体坐标系)</p><p>此处，误差四元数如下：</p><p><span class="math display">\[\delta \bar{q} \simeq\left[\begin{array}{ll}\frac{1}{2} \boldsymbol{\delta} \boldsymbol{\theta}^{T} &amp; 1\end{array}\right]^{T}\]</span></p><p>直观的，误差四元数<span class="math inline">\(\delta \bar{q}\)</span>描述了一个估计姿态和真实姿态之间的小角度旋转。因为姿态包含了3个自由度，使用<span class="math inline">\(\delta \theta\)</span>来描述姿态误差才是最小的表示。</p><p>假设在时间步骤k的EKF状态向量中包含N个相机姿势，该向量具有以下形式：</p><p><span class="math display">\[\hat{\mathbf{X} }_{k}=\left[\begin{array}{llllll}\hat{\mathbf{X} }_{\mathrm{IMU}_{k} }^{T} &amp; { }_{G}^{C_{1} } \hat{\bar{q} }^{T} &amp; { }^{G} \hat{\mathbf{p} }_{C_{1} }^{T} &amp; \ldots &amp; { }_{G}^{C_{N} } \hat{q}^{T} &amp; { }^{G} \hat{\mathbf{p} }_{C_{N} }^{T}\end{array}\right]^{T}\]</span></p><p>其中，<span class="math inline">\({}_G^{G_i} \hat{\bar{q} }, {}^G {\hat{p}_{C_i}, i=1\dots N}\)</span>是N个相机的姿态和位置。</p><p>因此，EKF的误差状态向量定义如下：</p><p><span class="math display">\[\widetilde{\mathbf{X} }_{k}=\left[\begin{array}{llllll}\widetilde{\mathbf{X} }_{\mathrm{IMU}_{k} }^{T} &amp; \boldsymbol{\delta} \boldsymbol{\theta}_{C_{1} }^{T} &amp; { }^{G} \widetilde{\mathbf{p} }_{C_{1} }^{T} &amp; \ldots &amp; \boldsymbol{\delta} \boldsymbol{\theta}_{C_{N} }^{T} &amp; { }^{T} \widetilde{\mathbf{p} }_{C_{N} }^{T}\end{array}\right]^{T}\]</span></p><h2 id="传播">传播</h2><p>滤波器传播的等式通过连续时间模型的离散化形式导出，定义如下：</p><h3 id="连续时间系统模型">连续时间系统模型</h3><p>IMU状态微分方程描述：</p><p><span class="math display">\[{ }_{G}^{I} \dot{\bar{q} }(t)=\frac{1}{2} \boldsymbol{\Omega}(\boldsymbol{\omega}(t))_{G}^{I} \bar{q}(t)\]</span></p><p><span class="math display">\[\dot{\mathbf{b} }_{g}(t)=\mathbf{n}_{w g}(t)\]</span></p><p><span class="math display">\[{ }^{G} \dot{\mathbf{v} }_{I}(t)={ }^{G} \mathbf{a}(t)\]</span></p><p><span class="math display">\[\dot{\mathbf{b} }_{a}(t)=\mathbf{n}_{w a}(t)\]</span></p><p><span class="math display">\[{ }^{G} \mathbf{p}_{I}(t)={ }^{G} \mathbf{v}_{I}(t)\]</span></p><p>其中，<span class="math inline">\({ }^{G} \mathbf{a}\)</span>表示机体加速度在全局坐标系的表示，<span class="math inline">\(\boldsymbol{\omega}=\left[\begin{array}{lll}\omega_{x} &amp; \omega_{y} &amp; \omega_{z}\end{array}\right]^{T}\)</span>表示IMU坐标系的旋转角速度。</p><p>另外的计算符号定义如下：</p><p><span class="math display">\[\boldsymbol{\Omega}(\boldsymbol{\omega})=\left[\begin{array}{cc}-\lfloor\boldsymbol{\omega} \times\rfloor &amp; \boldsymbol{\omega} \\-\boldsymbol{\omega}^{T} &amp; 0\end{array}\right], \quad\lfloor\boldsymbol{\omega} \times\rfloor=\left[\begin{array}{ccc}0 &amp; -\omega_{z} &amp; \omega_{y} \\\omega_{z} &amp; 0 &amp; -\omega_{x} \\-\omega_{y} &amp; \omega_{x} &amp; 0\end{array}\right]\]</span></p><p>陀螺仪和加速度计的测量描述如下 [20 ]：</p><p><span class="math display">\[\boldsymbol{\omega}_{m}=\boldsymbol{\omega}+\mathbf{C}\left({ }_{G}^{I} \bar{q}\right) \boldsymbol{\omega}_{G}+\mathbf{b}_{g}+\mathbf{n}_{g}\]</span></p><p><span class="math display">\[\begin{aligned}\mathbf{a}_{m}=&amp; \mathbf{C}\left({ }_{G}^{I} \bar{q}\right)\left({ }^{G} \mathbf{a}-{ }^{G} \mathbf{g}+2\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor^{G} \mathbf{v}_{I}+\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor^{2}{ }^{G} \mathbf{p}_{I}\right) \\&amp;+\mathbf{b}_{a}+\mathbf{n}_{a}\end{aligned}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(C(\cdot)\)</span>表示旋转矩阵</li><li><span class="math inline">\(n_g,n_a\)</span>为零均值高斯白噪声</li><li>值得注意的是，IMU测量结合了星球的旋转，<span class="math inline">\(w_{G}\)</span>的效果</li><li>此外，加速度计测量包括引力加速度,<span class="math inline">\({ }^{G} \mathbf{g}\)</span>, (expressed in the local frame)</li></ul><p>在上面的连续时间状态方程中，应用这些运算符，就可以得到了IMU的状态估计方程：</p><p><span class="math display">\[{}_{G}^{I} \dot{\hat{\bar{q} } }=\frac{1}{2} \boldsymbol{\Omega}(\hat{\boldsymbol{\omega} })_{G}^{I} \hat{\bar{q} }\]</span></p><p><span class="math display">\[\dot{\hat{\mathbf{b} } }_{g}=\mathbf{0}_{3 \times 1}\]</span></p><p><span class="math display">\[{ }^{G} \dot{\hat{\mathbf{v} } }_{I}=\mathbf{C}_{\hat{q} }^{T} \hat{\mathbf{a} }-2\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor^{G} \hat{\mathbf{v} }_{I}-\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor^{2}{ }^{G} \hat{\mathbf{p} }_{I}+{ }^{G} \mathbf{g}\]</span></p><p><span class="math display">\[\dot{\hat{\mathbf{b} } }_{a}=\mathbf{0}_{3 \times 1}\]</span></p><p><span class="math display">\[{ }^{G} \dot{\hat{\mathbf{p} } }_{I}={ }^{G} \hat{\mathbf{v} }_{I}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{C}_{\hat{q} }=C({}_G^{I}\hat{\bar{q} })\)</span></li><li><span class="math inline">\(\hat{a}=a_m-\bar{b}_{a}\)</span></li><li><span class="math inline">\(\hat{\omega}=\omega_{m}-\hat{b}_{g}-C_{\hat{q} }\omega_{G}\)</span></li></ul><p>IMU误差状态的线性化连续时间模型如下表示：</p><p><span class="math display">\[\dot{\widetilde{\mathbf{X} } }_{\mathrm{IMU} }=\mathbf{F} \tilde{\mathbf{X} }_{\mathrm{IMU} }+\mathbf{G} \mathbf{n}_{\mathrm{IMU} }\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{n}_{\mathrm{IMU} }=\left[\begin{array}{llll}\mathbf{n}_{g}^{T} &amp; \mathbf{n}_{w g}^{T} &amp; \mathbf{n}_{a}^{T} &amp; \mathbf{n}_{w a}^{T}\end{array}\right]^{T}\)</span>是系统噪声</li><li>关于<span class="math inline">\(\mathbf{n}_{IMU}\)</span>的协方差矩阵，<span class="math inline">\(\mathbf{Q}_{\mathrm{IMU} }\)</span>，取决于IMU噪声特性，可以再传感器标定期间离线计算。</li></ul><p>最后，F矩阵和G矩阵可以整理如下：</p><p><span class="math display">\[\mathbf{F}=\left[\begin{array}{ccccc}-\lfloor\hat{\boldsymbol{\omega} } \times\rfloor &amp; \mathbf{- I}_{3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} \\-\mathbf{C}_{\hat{q} }^{T}\lfloor\hat{\mathbf{a} } \times\rfloor &amp; \mathbf{0}_{3 \times 3} &amp; -2\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor &amp; -\mathbf{C}_{\hat{q} }^{T} &amp; -\left\lfloor\boldsymbol{\omega}_{G} \times\right\rfloor^{2} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{I}_{3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3}\end{array}\right]\]</span></p><p><span class="math display">\[\mathbf{G}=\left[\begin{array}{cccc}-\mathbf{I}_{3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{I}_{3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; -\mathbf{C}_{\hat{q} }^{T} &amp; \mathbf{0}_{3 \times 3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{I}_{3} \\\mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 3}\end{array}\right]\]</span></p><h3 id="离散时间模型实现">离散时间模型实现</h3><p>由于IMU在周期T内进行采样，得到采样信号<span class="math inline">\(\omega_{m},a_{m}\)</span>，每次接收到新的IMU测量时，IMU状态估计采用5阶龙哥库塔(RK-5)记性积分传播。</p><p>此外，EKF的协方差矩阵必须传播，因此，我们介绍对协方差的分区：</p><p><span class="math display">\[\mathbf{P}_{k \mid k}=\left[\begin{array}{ll}\mathbf{P}_{I I_{k \mid k} } &amp; \mathbf{P}_{I C_{k \mid k} } \\\mathbf{P}_{I C_{k \mid k} }^{T} &amp; \mathbf{P}_{C C_{k \mid k} }\end{array}\right]\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{P}_{I I_{k \mid k} }\)</span>是 15x15的协方差矩阵，关于IMU状态</li><li><span class="math inline">\(\mathbf{P}_{C C_{k \mid k} }\)</span>是6Nx6N的协方差矩阵，关于相机位姿估计的</li><li><span class="math inline">\(\mathbf{P}_{I C_{k \mid k} }\)</span>是IMU状态和相机位姿估计误差的相关性</li></ul><p>通过这样的分块，传播状态的协方差矩阵按如下进行：</p><p><span class="math display">\[\mathbf{P}_{k+1 \mid k}=\left[\begin{array}{cc}\mathbf{P}_{I I_{k+1 \mid k} } &amp; \mathbf{\Phi}\left(t_{k}+T, t_{k}\right) \mathbf{P}_{I C_{k \mid k} } \\\mathbf{P}_{I C_{k \mid k} }^{T} \mathbf{\Phi}\left(t_{k}+T, t_{k}\right)^{T} &amp; \mathbf{P}_{C C_{k \mid k} }\end{array}\right]\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{P}_{I I_{k+1 \mid k} }\)</span>有李雅普诺夫(Lyapunov)等式进行数值积分得到：</li></ul><p><span class="math display">\[\dot{\mathbf{P} }_{I I}=\mathbf{F} \mathbf{P}_{I I}+\mathbf{P}_{I I} \mathbf{F}^{T}+\mathbf{G} \mathbf{Q}_{\mathrm{IMU} } \mathbf{G}^{T}\]</span></p><p>数值积分即以初始值<span class="math inline">\(\mathbf{P}_{I I_{k\mid k} }\)</span>对时间间隔<span class="math inline">\((t_k,t_{k+T})\)</span>进行积分。</p><ul><li>误差转移矩阵<span class="math inline">\(\mathbf{\Phi}\left(t_{k}+T, t_{k}\right)\)</span>由微分方程进行数值积分来近似得到：</li></ul><p><span class="math display">\[\dot{\boldsymbol{\Phi} }\left(t_{k}+\tau, t_{k}\right)=\mathbf{F} \boldsymbol{\Phi}\left(t_{k}+\tau, t_{k}\right), \quad \tau \in[0, T]\]</span></p><p>其中，初始条件为</p><p><span class="math display">\[\mathbf{\Phi}\left(t_{k}, t_{k}\right)=\mathbf{I}_{15\times 15}\]</span></p><h3 id="状态增广">状态增广</h3><p>当接受到新的图像时，首先聪IMU姿态估计来计算相机的姿态估计初值：</p><p><span class="math display">\[_{G}^{C} \hat{\bar{q} }=\underset{I}{C} \bar{q} \otimes_{G}^{I} \hat{\bar{q} }\]</span></p><p><span class="math display">\[{ }^{G} \hat{\mathbf{p} }_{C}={ }^{G} \hat{\mathbf{p} }_{I}+\mathbf{C}_{\hat{q} }^{T}{ }^{I} \mathbf{p}_{C}\]</span></p><p>其中，</p><ul><li><span class="math inline">\({}_I^C \bar{q}\)</span>表示从IMU坐标系到相机坐标系的变换</li><li><span class="math inline">\({ }^{I} \mathbf{p}_{C}\)</span>表示相对于IMU坐标系的原点，相机坐标系的位置。</li></ul><p>由于相机位姿估计附加到状态向量中，因此EKF的协方差矩阵也进行增广:</p><p><span class="math display">\[\mathbf{P}_{k \mid k} \leftarrow\left[\begin{array}{c}\mathbf{I}_{6 N+15} \\\mathbf{J}\end{array}\right] \mathbf{P}_{k \mid k}\left[\begin{array}{c}\mathbf{I}_{6 N+15} \\\mathbf{J}\end{array}\right]^{T}\]</span></p><p>其中，雅克比<span class="math inline">\(J\)</span>根据式(14)进行微分推导 （谁对谁求导？）</p><p>式(14)截图如下： <img src="http://s1.nsloop.com:59080/images/2021/07/24/20210724170432.png"></p><p>最后得到雅克比如下：</p><p><span class="math display">\[\mathbf{J}=\left[\begin{array}{cccc}\mathbf{C}\left({ }_{I}^{C} \bar{q}\right) &amp; \mathbf{0}_{3 \times 9} &amp; \mathbf{0}_{3 \times 3} &amp; \mathbf{0}_{3 \times 6 N} \\\left\lfloor\mathbf{C}_{\hat{q} }^{T I} \mathbf{p}_{C} \times\right\rfloor &amp; \mathbf{0}_{3 \times 9} &amp; \mathbf{I}_{3} &amp; \mathbf{0}_{3 \times 6 N}\end{array}\right]\]</span></p><h3 id="测量模型">测量模型</h3><p>我们现在介绍用于更新状态估计的测量模型，这是本文的主要贡献。</p><p>由于EKF用于状态估计，因此对于构造测量模型，需要定义残差r ， 这取决于误差状态<span class="math inline">\(\widetilde{\mathbf{X} }\)</span>，因此，根据通用形式，有：</p><p><span class="math display">\[\mathbf{r}=\mathbf{H} \widetilde{\mathbf{X} }+ noise\]</span></p><p>在这个表达式中，H是测量雅比亚矩阵。对于EKF框架，应用的对于误差状态的噪声项必须是零均值、不相关的白噪声。</p><p>为了衍生我们的测量模型，我们的动机是通过多个相机的静态特征来实现涉及所有这些姿势的约束。在我们的工作中，相机观测按跟踪的特征进行分组，而不是每个相机姿势记录对应的观测（如[7,13,14]）。</p><p>同一个3D点的所有测量值都用于定于约束方程（在后面的等式24），与测量发生的所有相机姿势所相关联。<strong>这样的方法实现了在状态向量中可以不包含特征点的位置</strong>。</p><p>我们通过考虑单个特征 <span class="math inline">\(f_j\)</span> 被多个相机pose<span class="math inline">\(\left({ }_{G}^{C_{i} } \bar{q},{ }^{G} \mathbf{p}_{C_{i} }\right), i \in \mathcal{S}_{j}\)</span>所构成的集合<span class="math inline">\(M_{j}\)</span>所共同观测到的情况来提出测量模型。 <span class="math inline">\(M_{j}\)</span>中的每个观测都可以使用如下模型来描述：</p><p><span class="math display">\[\mathbf{z}_{i}^{(j)}=\frac{1}{ { }^{C_{i} } Z_{j} }\left[\begin{array}{c}{ }^{C_{i} } X_{j} \\{ }^{C_{i} } Y_{j}\end{array}\right]+\mathbf{n}_{i}^{(j)}, \quad i \in \mathcal{S}_{j}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{n}_{i}^{(j)}\)</span>是2x1的图像噪声向量，具有协方差矩阵<span class="math inline">\(\mathbf{R}_{i}^{(j)}=\sigma_{i m}^{2} \mathbf{I}_{2}\)</span></li><li>特征点的位置<span class="math inline">\({ }^{C_{i} } \mathbf{p}_{f_{j} }\)</span>被表示为在相机坐标系中，如下得到：</li></ul><p><span class="math display">\[{ }^{C_{i} } \mathbf{p}_{f_{j} }=\left[\begin{array}{c}{ }^{C_{i} } X_{j} \\{ }^{C_{i} } Y_{j} \\{ }^{C_{i} } Z_{j}\end{array}\right]=\mathbf{C}\left({ }_{G}^{C_{i} } \bar{q}\right)\left({ }^{G} \mathbf{p}_{f_{j} }-{ }^{G} \mathbf{p}_{C_{i} }\right)\]</span></p><p>其中，</p><ul><li><span class="math inline">\({ }^{G} \mathbf{p}_{f_{j} }\)</span>表示特征点在全局坐标系的3D位置，由于这是未知的，在我们的算法的第一步中，我们使用最小二乘最小化以获得估计值<span class="math inline">\({ }^{G} \hat{\mathbf{p} }_{f_{j} }\)</span>。 这是利用观测值<span class="math inline">\(\mathbf{z}_{i}^{(j)}, i \in \mathcal{S}_{j}\)</span>以及滤波器估计值关于相机姿态来实现的（具体参考附录）</li></ul><p>一旦获得了特征位置的估计，我们计算观测的残差：</p><p><span class="math display">\[\mathbf{r}_{i}^{(j)}=\mathbf{z}_{i}^{(j)}-\hat{\mathbf{z} }_{i}^{(j)}\]</span></p><p>其中，</p><p><span class="math display">\[\hat{\mathbf{z} }_{i}^{(j)}=\frac{1}{ {}^{C_{i} } \hat{Z}_{j} }\left[\begin{array}{c}{ }^{C_{i} } \hat{X}_{j} \\{ }^{C_{i} } \hat{Y}_{j}\end{array}\right]\]</span></p><p><span class="math display">\[\left[\begin{array}{c}{ }^{C_{i} } \hat{X}_{j} \\{ }^{C_{i} } \hat{Y}_{j} \\{ }^{C_{i} } \hat{Z}_{j}\end{array}\right]=\mathbf{C}\left({ }_{G}^{C_{i} } \hat{q}\right)\left({ }^{G} \hat{\mathbf{p} }_{f_{j} }-{ }^{G} \hat{\mathbf{p} }_{C_{i} }\right)\]</span></p><p>对上式( 等式20 )中关于相机位姿和特征点位置进行线性化，得到近似如下：</p><p><span class="math display">\[\mathbf{r}_{i}^{(j)} \simeq \mathbf{H}_{\mathbf{X}_{i} }^{(j)} \widetilde{\mathbf{X} }+\mathbf{H}_{f_{i} }^{(j) G} \widetilde{\mathbf{p} }_{f_{j} }+\mathbf{n}_{i}^{(j)}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{H}_{\mathbf{X}_{i} }^{(j)}\)</span>是观测<span class="math inline">\(\mathbf{z}_{i}^{(j)}\)</span>对状态的雅克比</li><li><span class="math inline">\(\mathbf{H}_{f_{i} }^{(j)}\)</span>是观测<span class="math inline">\(\mathbf{z}_{i}^{(j)}\)</span>对特征点位置的雅克比 ( <span class="math inline">\(\mathbf{z}_{i}^{(j)}\)</span> 难道不是直接从图像获取的特征点吗)</li><li><span class="math inline">\(\widetilde{\mathbf{X} }\)</span>滤波器的误差状态</li><li><span class="math inline">\({ }^{G} \widetilde{\mathbf{p} }_{f_{j} }\)</span> 关于特征点<span class="math inline">\(f_j\)</span>的误差</li><li>上面H矩阵的具体推导可参见[21]</li></ul><p>通过叠加关于特征点<span class="math inline">\(f_j\)</span>的相机位姿集合<span class="math inline">\(M_j\)</span>中所有的残差，可以得到：</p><p><span class="math display">\[\mathbf{r}^{(j)} \simeq \mathbf{H}_{\mathbf{X} }^{(j)} \widetilde{\mathbf{X} }+\mathbf{H}_{f}^{(j) G} \widetilde{\mathbf{p} }_{f_{j} }+\mathbf{n}^{(j)}\]</span></p><p>其中，<span class="math inline">\(\mathbf{r}^{(j)}, \mathbf{H}_{\mathbf{X} }^{(j)}, \mathbf{H}_{f}^{(j)}\)</span>, and <span class="math inline">\(\mathbf{n}^{(j)}\)</span>都是分别包含如下元素<span class="math inline">\(\mathbf{r}_{i}^{(j)}, \mathbf{H}_{\mathbf{X}_{i} }^{(j)}, \mathbf{H}_{f_{i} }^{(j)}\)</span>, and <span class="math inline">\(\mathbf{n}_{i}^{(j)}\)</span>的向量或矩阵。 并且由于不同图像中的特征观测是独立的，因此<span class="math inline">\(\mathbf{n}^{(j)}\)</span>的协方差矩阵为<span class="math inline">\(\mathbf{R}^{(j)}=\sigma_{\mathrm{im} }^{2} \mathbf{I}_{2 M_{j} }\)</span>。</p><p>请注意，由于状态估计值<span class="math inline">\(\mathbf{X}\)</span>，用于计算特征点的位置估计（参考附录），式(22) 即线性化后的残差等式与误差状态<span class="math inline">\(\mathbf{\tilde{X} }\)</span>相关联。因此，残差<span class="math inline">\(\mathbf{r}^{(j)}\)</span>并非如等式(17)的形式(<span class="math inline">\(\mathbf{r}=\mathbf{H} \tilde{\mathbf{X} }+\)</span> noise)，不能直接用于EKF的测量更新步骤。</p><p>为了克服这个问题，我们通过将<span class="math inline">\(\mathbf{r}^{(j)}\)</span>投影到矩阵<span class="math inline">\(\mathbf{H}_{f}^{(j)}\)</span>的左零空间，从而定义了一个新的残差<span class="math inline">\(\mathbf{r}_{o}^{(j)}\)</span>。特别的，如果我们使用酉矩阵来表示<span class="math inline">\(\mathbf{A}\)</span>，其中它的列形成了关于<span class="math inline">\(\mathbf{H}_{f}\)</span>左零空间中的bias （这说的啥意思）</p><p><span class="math display">\[\begin{aligned}\mathbf{r}_{o}^{(j)} &amp;=\mathbf{A}^{T}\left(\mathbf{z}^{(j)}-\hat{\mathbf{z} }^{(j)}\right) \simeq \mathbf{A}^{T} \mathbf{H}_{\mathbf{X} }^{(j)} \widetilde{\mathbf{X} }+\mathbf{A}^{T} \mathbf{n}^{(j)} \\&amp;=\mathbf{H}_{o}^{(j)} \widetilde{\mathbf{X} }^{(j)}+\mathbf{n}_{o}^{(j)}\end{aligned}\]</span></p><p>因为 <span class="math inline">\(2M_j \times 3\)</span>的矩阵<span class="math inline">\(\mathbf{H}_{f}^{(j)}\)</span>是列满秩的，他的左零空间维度为<span class="math inline">\(2 M_{j}-3\)</span>。因此<span class="math inline">\(\mathbf{r}_{o}^{(j)}\)</span>是<span class="math inline">\(\left(2 M_{j}-3\right) \times 1\)</span>的向量。这种残差独立于特征坐标中的误差，因此可以基于它执行EKF更新。</p><p>式<span class="math inline">\(\mathbf{H}_{o}^{(j)} \widetilde{\mathbf{X} }^{(j)}+\mathbf{n}_{o}^{(j)}\)</span>定义了所有观测到特征点<span class="math inline">\(f_j\)</span>。这表达了测量<span class="math inline">\(\mathbf{z}_{i}^{(j)}\)</span>为<span class="math inline">\(M_j\)</span>的状态提供所有的有效信息，因此产生的EKF更新是最优的，除了由于线性化所引起的不准确性。</p><p>应该提到的是，为了计算残差<span class="math inline">\(\mathbf{r}_{O}^{(j)}\)</span>和观测矩阵<span class="math inline">\(\mathbf{H}_{o}^{(j)}\)</span>，酉矩阵<span class="math inline">\(\mathbf{A}\)</span>不需要显式地被评估。相反，残差<span class="math inline">\(\mathbf{r}\)</span>和矩阵<span class="math inline">\(\mathbf{H}_{\mathbf{X} }^{(j)}\)</span>在<span class="math inline">\(\mathbf{H}_{f}^{(j)}\)</span>矩阵左零空间的投影可以通过使用<code>Givens</code>旋转[22]来计算得到，操作复杂度为<span class="math inline">\(O\left(M_{j}^{2}\right)\)</span>。另外，<span class="math inline">\(\mathbf{A}\)</span>是酉矩阵，因此向量<span class="math inline">\(\mathbf{n}_{o}^{(j)}\)</span>的协方差可以如下计算：</p><p><span class="math display">\[E\left\{\mathbf{n}_{o}^{(j)} \mathbf{n}_{o}^{(j) T}\right\}=\sigma_{\mathrm{im} }^{2} \mathbf{A}^{T} \mathbf{A}=\sigma_{\mathrm{im} }^{2} \mathbf{I}_{2 M_{j}-3}\]</span></p><h3 id="ekf更新">EKF更新</h3><p>在前面的部分中，我们呈现了一种测量模型，其表示通过观察来自多个相机姿势的静态特征而施加的几何约束。 我们现在详细介绍了EKF的更新阶段，其中使用从观察多个特征的约束。 EKF更新由以下两个事件之一触发</p><ul><li>当检测不到之前在多个图像跟踪的特征时，则使用第III-D部分中呈现的方法处理此特征的所有测量。 这种情况最常出现，因为特征点有可能在相机视野范围之外。</li><li>每次记录新图像时，当前相机姿势估计将被包含在状态向量中，如果已经达到了设定的最大相机位姿数<span class="math inline">\(N_{max}\)</span>，则必须删除最少一个旧的相机位姿。在丢弃状态之前，使用在相应的时间瞬间发生的所有特征观测，以便利用其局部信息。在我们的算法中，从第二最旧的相机位姿开始，我们选择均匀间隔的<span class="math inline">\(\frac{N_{max} }{3}\)</span>的位姿，在使用这些姿势共有的特征的约束执行 EKF 更新后，这些被丢弃。我们选择始终保持最古老的姿势在状态向量中，因为涉及及时进一步姿势的几何结构通常对应于较大的基线，因此携带更有价值的定位信息，这种方法在实践中表现得非常好。</li></ul><p>考虑到在给定的时间步骤中，必须处理由上述两个标准选择的<span class="math inline">\(L\)</span>个特征点的约束。遵循前一节中描述的过程，我们对每一个特征点计算残差向量<span class="math inline">\(\mathbf{r}_{o}^{(j)}, j=1 \ldots L\)</span>以及相关联的观测矩阵<span class="math inline">\(\mathbf{H}_{o}^{(j)}, j=1 \ldots L\)</span>。通过将所有残差堆叠在一个向量中，我们得到：</p><p><span class="math display">\[\mathbf{r}_{o}=\mathbf{H}_{\mathbf{X} } \widetilde{\mathbf{X} }+\mathbf{n}_{o}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{r}_{o}\)</span>的块元素为<span class="math inline">\(\mathbf{r}_{o}^{(j)}\)</span></li><li><span class="math inline">\(\mathbf{n}_{o}\)</span>的块元素为<span class="math inline">\(\mathbf{n}_{o}^{(j)}, j=1 \ldots L\)</span></li><li><span class="math inline">\(\mathbf{H}_{\mathbf{X} }\)</span>矩阵具有行块元素<span class="math inline">\(\mathbf{H}_{\mathbf{X} }^{(j)}, j=1 \ldots L\)</span></li></ul><p>由于特征测量是统计上的，因此噪声向量<span class="math inline">\(\mathbf{n}_{o}^{(j)}\)</span>是不相关的，因此，其协方差矩阵等价于<span class="math inline">\(\mathbf{R}_{o}=\sigma_{\mathrm{im} }^{2} \mathbf{I}_{d}\)</span>，其中 <span class="math inline">\(d=\sum_{j=1}^{L}\left(2 M_{j}-3\right)\)</span> 是残差<span class="math inline">\(\mathbf{r}_{o}\)</span>的维度</p><p>一个实践中的问题是，<span class="math inline">\(d\)</span>可以是一个相当大的数字，例如，如果10个特征点在10个相机位姿中都被观测到，那么残差的维度是170 {(2x10-3)x10=170}.</p><p>为了降低EKF更新的计算复杂度，我们采用QR分解，对<span class="math inline">\(\mathbf{H}_{\mathbf{X} }\)</span>，特别的，我们记分解为如下形式：</p><p><span class="math display">\[\mathbf{H}_{\mathbf{X} }=\left[\begin{array}{ll}\mathbf{Q}_{1} &amp; \mathbf{Q}_{2}\end{array}\right]\left[\begin{array}{c}\mathbf{T}_{H} \\\mathbf{0}\end{array}\right]\]</span></p><p>其中，</p><ul><li>Q1和Q2是关于矩阵<span class="math inline">\(\mathbf{H}_{\mathbf{X} }\)</span>列形式的range和nullspace。</li><li><span class="math inline">\(\mathbf{T}_{H}\)</span>是上三角矩阵</li></ul><p>根据这个定义，等式(25)(<span class="math inline">\(\mathbf{r}_{o}=\mathbf{H}_{\mathbf{X} } \widetilde{\mathbf{X} }+\mathbf{n}_{o}\)</span>) 可以产生如下形式：</p><p><span class="math display">\[\begin{aligned}\mathbf{r}_{o} &amp;=\left[\begin{array}{ll}\mathbf{Q}_{1} &amp; \mathbf{Q}_{2}\end{array}\right]\left[\begin{array}{c}\mathbf{T}_{H} \\\mathbf{0}\end{array}\right] \tilde{\mathbf{X} }+\mathbf{n}_{o} \Rightarrow \\\left[\begin{array}{c}\mathbf{Q}_{1}^{T} \mathbf{r}_{o} \\\mathbf{Q}_{2}^{T} \mathbf{r}_{o}\end{array}\right] &amp;=\left[\begin{array}{c}\mathbf{T}_{H} \\\mathbf{0}\end{array}\right] \widetilde{\mathbf{X} }+\left[\begin{array}{c}\mathbf{Q}_{1}^{T} \mathbf{n}_{o} \\\mathbf{Q}_{2}^{T} \mathbf{n}_{o}\end{array}\right]\end{aligned}\]</span></p><p>从最后一个等式开始，通过投影<span class="math inline">\(\mathbf{H}_{\mathbf{X} }\)</span>范围的基础向量，我们保留了测量中的所有有用信息。</p><p>残差中的<span class="math inline">\(\mathbf{Q}_{2}^{T} \mathbf{r}_{o}\)</span>只是噪声，并且可以完全丢弃。因此，相比于使用等式(25)中的残差表示，我们使用下面形式的残差来执行EKF更新：</p><p><span class="math display">\[\mathbf{r}_{n}=\mathbf{Q}_{1}^{T} \mathbf{r}_{o}=\mathbf{T}_{H} \widetilde{\mathbf{X} }+\mathbf{n}_{n}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{n}_{n}=\mathbf{Q}_{1}^{T} \mathbf{n}_{o}\)</span> 是噪声向量，其协方差矩阵等价于<span class="math inline">\(\mathbf{R}_{n}=\mathbf{Q}_{1}^{T} \mathbf{R}_{o} \mathbf{Q}_{1}=\sigma_{\operatorname{im} }^{2} \mathbf{I}_{r}\)</span>，且r为Q1的列数</li></ul><p>EKF更新步骤计算卡尔曼增益：</p><p><span class="math display">\[\mathbf{K}=\mathbf{P} \mathbf{T}_{H}^{T}\left(\mathbf{T}_{H} \mathbf{P} \mathbf{T}_{H}^{T}+\mathbf{R}_{n}\right)^{-1}\]</span></p><p>并且，矫正的状态按下式给出：</p><p><span class="math display">\[\Delta \mathbf{X}=\mathbf{K} \mathbf{r}_{n}\]</span></p><p>最后，状态的协方差矩阵如下更新：</p><p><span class="math display">\[\mathbf{P}_{k+1 \mid k+1}=\left(\mathbf{I}_{\xi}-\mathbf{K} \mathbf{T}_{H}\right) \mathbf{P}_{k+1 \mid k}\left(\mathbf{I}_{\xi}-\mathbf{K} \mathbf{T}_{H}\right)^{T}+\mathbf{K} \mathbf{R}_{n} \mathbf{K}^{T}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\xi=6N+15\)</span>是协方差矩阵的维度</li></ul><p>审查EKF更新期间所需的操作的计算复杂性很有意思，残差 <span class="math inline">\(\mathbf{r}_{n}\)</span>以及矩阵<span class="math inline">\(\mathbf{T}_{H}\)</span>可以使用 Givens 旋转计算，操作复杂度是<span class="math inline">\(O\left(r^{2} d\right)\)</span>，而无需显式地计算Q1的形式。</p><p>另一方面，等式(31)包含了<span class="math inline">\(\xi\)</span>维度的方阵的乘法计算，是<span class="math inline">\(O\left(\xi^{3}\right)\)</span>的操作。因此，EKF更新的复杂度是<span class="math inline">\(\max \left(O\left(r^{2} d\right), O\left(\xi^{3}\right)\right)\)</span>。</p><p>另一方面，如果使用的残余向量<span class="math inline">\(\mathbf{r}_{o}\)</span>，而不将其投影在<span class="math inline">\(\mathbf{H}_{\mathbf{X} }\)</span>的range内，计算卡尔曼增益的计算成本是<span class="math inline">\(O\left(d^{3}\right)\)</span>，然而，通常<span class="math inline">\(d \gg \xi, r\)</span>，所以可知，使用残差<span class="math inline">\(\mathbf{r}_{n}\)</span>可以减少计算量。</p><h1 id="实验暂略">实验（暂略）</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/24/20210724205124.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;a-multi-state-constraint-kalman-filter-for-vision-aided-inertial-navigation&quot;&gt;A Multi-State Constraint Kalman Filter for Vision-aided
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>LT-mapper论文阅读</title>
    <link href="http://yoursite.com/2021/07/20/LT-mapper%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/07/20/LT-mapper%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-07-20T01:33:53.000Z</published>
    <updated>2021-07-20T01:56:02.029Z</updated>
    
    <content type="html"><![CDATA[<h1 id="lt-mapper-a-modular-framework-for-lidar-based-lifelong-mapping">LT-mapper: A Modular Framework for LiDAR-based Lifelong Mapping</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720094022.png"></p><h1 id="摘要">摘要</h1><p>本文开发了一个开源的、模块化的、现成的、基于lidar的城市站点lifelong mapping</p><p>这是通过将问题划分为连续的子问题来实现的：</p><p>- multi-session SLAM (MSS) - high/low dynamic change detection - positive/negative change management</p><p>所提出的方法利用MSS，并处理潜在的轨迹误差，因此，change检测不需要良好的初始对齐，我们的change管理方案保留了内存和计算成本的有效性，提供了从大规模点云图中自动分离对象的功能。通过对多个时间间隔(从一天到一年)的广泛实际实验，我们验证了该框架的可靠性和适用性，甚至在永久的年水平变化。</p><h1 id="介绍">介绍</h1><p>环境的变化如图1所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720094103.png"></p><p>为了更好地处理这种变化，lifelong mapping必须通过检测、更新和管理环境变化来解决自治的建图维护[1]</p><p>1）Integration to multi-session SLAM for scalability：一些研究认为，变化检测是比较多个预先构建的地图与时间上遥远和独立的后处理过程。在这项工作中，我们集成了多会话SLAM (MSS)，并将会话与锚节点[2]对齐，以在大型城市环境中执行变化检测，而不是在一个小房间，我们的框架包括一个基于激光雷达的多会话三维同步定位和映射(SLAM)模块，称为LT-SLAM。</p><p>2）Change detection under SLAM error：如果地图完全对齐，那么两个地图之间的变化检测就不重要了，早期的地图变化检测工作[5,3,6,7]依赖于全局对齐地图的强假设，没有错误，避免了处理这种模糊性问题。不幸的是，轨迹误差在现实中不可避免地发生。我们在变更检测期间调和了这种潜在的不对准，并使所提出的方法能够稳健地处理潜在的对准误差。为了处理模糊性，我们提出了一种具有投射可见性的scan-to-map方案，使用多个窗口大小的range-image，称为LT-removert</p><p>3）Compact place management：除了变更检测之外，我们还提出并证明了变更组合的概念，一旦检测到更改，就应该遵循地图维护的决定，以确定包含或排除什么，利用这一特性，我们不仅可以维护现有作品[1,3]等最新的地图，还可以提取具有较高placeness的稳定结构。因此，我们构建了一个可靠的3D地图，具有真正有意义的结构，用于其他任务，如跨模式定位[9]和长期定位[10]。这个最后的模块，称为LT-map.</p><p>总结，提出了一个新颖的基于激光雷达的lifelong mapping，称为LT-mapper. 框架中的每一个模块都可独立运行，基于file-based i/o 协议。与最近(但部分)提供的基于视觉的方法不同的是，3D LiDAR几乎没有实现统一和模块化的终身映射[11,12,13,14,15]。据我们所知，LT-mapper是第一个开放的模块化框架，支持基于lidar的复杂城市站点终身绘图，本文主要贡献：</p><p>- LT-SLAM 集成变化检测MSS，通过anchor node来解决会话恢复，只使用激光雷达在共享帧中缝合多个会话。 - LT-removert 利用时空轴上的remove-then-revert算法，克服了会话间的对齐模糊性。 - LT-map 能有效地生成最新地图(实时地图)和持久地图(元地图)，同时将更改存储为增量地图，通过增量建图，减少内存和储存的成本。</p><h1 id="相关工作">相关工作</h1><h1 id="概述">概述</h1><p>LT-mapper是完全模块化的，并支持上述三个功能。整个pipline有3个模块组成(图3)，顺序运行并且模块独立。不像现有的基于激光雷达的变化检测[21]，装备有昂贵的定位设备，我们的系统只需要一个激光雷达（可选的IMU）。</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720094454.png"></p><p>在真实的户外环境中，暂时不连接的场景之间的准确对齐是难以捉摸的，如图2(a)所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720094903.png"></p><p>在LT-SLAM模块中，我们利用多会话SLAM，联合优化多个会话，并使用基于lidar的全局定位器进行鲁棒的会话间闭环检测，在这个模块中，一个查询度量被配准到现有的中心地图中。</p><p>我们同样需要考虑观测的变化，如图2(b)，一个构建的点云图可能包含噪声，由于周围的运动物体(红点)，即使是精确的里程表。这些不稳定的物体对一个地方的显著性的贡献不如静止点，因此，在LT-removert模块中，这些高动态(HD)点应该在计算会话间差异之前预先删除。</p><p>在对齐查询帧和中心会话并移除高清点后，我们通过应用查询测量和中心地图之间的差分操作来检测变化，如图2(c)。我们称之为低动态变化(LD)，进一步分为新出现点(PD)和消失点(ND)两类。</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/20/20210720095539.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;lt-mapper-a-modular-framework-for-lidar-based-lifelong-mapping&quot;&gt;LT-mapper: A Modular Framework for LiDAR-based Lifelong Mapping&lt;/h1&gt;
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>TEB局部路径规划论文阅读</title>
    <link href="http://yoursite.com/2021/07/19/TEB%E5%B1%80%E9%83%A8%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/07/19/TEB%E5%B1%80%E9%83%A8%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-07-19T09:03:53.000Z</published>
    <updated>2021-07-19T12:51:34.355Z</updated>
    
    <content type="html"><![CDATA[<h1 id="trajectory-modification-considering-dynamic-constraints-of-autonomous-robots">Trajectory modification considering dynamic constraints of autonomous robots</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719174333.png"></p><h1 id="摘要">摘要</h1><p>经典的“松紧带”使由全局规划器生成的路径相对于最短路径长度发生变形，为了避免与障碍物接触。它不直接考虑底层机器人的任何动态约束。本文贡献引入了一种名为“时间弹性带”的新方法，该方法明确地考虑了运动的时间方面的动态约束，如有限的机器人速度和加速度。“时间弹性带”问题用加权多目标优化框架表示。大多数目标是局部的，因为它们依赖于一些邻近的中间配置。这就得到了一个有效的大规模约束最小二乘优化方法存在的稀疏系统矩阵。</p><p>仿真和实际机器人的实验结果表明，该方法具有较好的鲁棒性和计算效率，能够实时生成最优机器人轨迹。“时间弹性带”将由一系列路径点组成的初始路径转换为明确依赖于时间的轨迹，从而实现对机器人的实时控制。由于其模块化的形式，该方法很容易扩展到包含额外的目标和约束。</p><h1 id="介绍">介绍</h1><p>在运动规划的背景下，本文侧重于局部路径修改，假设初始路径已由全局规划器生成[1]。特别是在服务机器人的环境中，由于动态环境可能是动态的，由于其固有的不确定性，修改路径是一种较好的方法。此外，由于局部、不完整的地图和动态障碍，环境模型可能会发生变化，此外，在实时应用中，大规模全局路径的(重新)计算往往是不可行的。这种观测结果导致了局部修改路径的方法，如[2,3]提出的“弹性带”，“松紧带”方法的主要思想是，将原来给定的路径视为受内外力影响的弹性橡皮筋，使其变形，而内外力相互平衡，使路径收缩，同时与障碍物保持一定距离。</p><p>后来这种方法被推广到非完整运动学[4,5,6]、多自由度[7]机器人系统和动态障碍[8]，然而，据我们所知，动态运动约束还没有被认为是对路径变形的一个目标。典型的方法是用样条曲线平滑路径，获得动态可行轨迹。</p><p>我们的方法称为“时间弹性带”，是一种新颖的方法，因为它明确地增加了“弹性带”的时间信息，从而允许考虑机器人的动态约束和直接修改轨迹而不是路径。图1展示了使用时间弹性带架构的机器人系统：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719175444.png"></p><p>通过考虑时间信息，“时间弹性带”也可以用来控制机器人的速度和加速度，这个方法也适用于高维的状态空间，尽管本文只考虑了差分驱动机器人的平面环境移动，有三个全局自由度和两个局部自由度。</p><h1 id="时间弹性带">时间弹性带</h1><p>经典的“弹性带”是用n个机器人的中间姿态序列来描述的，<span class="math inline">\(\mathbf{x}_{i}=\left(x_{i}, y_{i}, \beta_{i}\right)^{T} \in \R^{2} \times S^1\)</span>，下面记为位置(x_i,y_i)和旋转<span class="math inline">\(\beta_i\)</span>，如图2所示:</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719191833.png"></p><p><span class="math display">\[Q=\left\{\mathbf{x}_{i}\right\}_{i=0 \ldots n} \quad n \in \mathbb{N}\]</span></p><p>TEB由两个连续的配置之间的时间间隔来进行时间调整，因此一个序列中包含<span class="math inline">\(n-1\)</span>个<span class="math inline">\(\Delta T_i\)</span>：</p><p><span class="math display">\[\tau=\left\{\Delta T_{i}\right\}_{i=0 \ldots n-1}\]</span></p><p>每个时间差表示机器人从一个配置依次过渡到下一个配置的时间(图2)，因此TEB定义为元祖序列：</p><p><span class="math display">\[B:=(Q, \tau)\]</span></p><p>其关键思想是通过实时加权多目标优化，在配置和时间间隔两个方面进行调整和优化：</p><p><span class="math display">\[\begin{aligned}f(B) &amp;=\sum_{k} \gamma_{k} f_{k}(B) \\B^{*} &amp;=\underset{B}{\operatorname{argmin}} f(B)\end{aligned}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(B*\)</span>表示优化的TEB</li><li><span class="math inline">\(f(B)\)</span>记为目标函数，在本文中由多个加权成分<span class="math inline">\(f_k\)</span>组成，用于面对不同的方面，这是最基本的多目标优化方法，但它已经产生了非常好的结果。</li></ul><p>目标函数的大部分分量相对于B是局部的，因为它们只依赖于几个连续的配置，而不是整个可配置空间带。</p><p>TEB的这种局部性导致了一个稀疏系统矩阵，为其提供了专门的快速有效的大规模数值优化方法[11]。</p><p>TEB的目标函数分为两类：</p><ul><li>约束例如速度、加速度限制等惩罚项</li><li>目标项如最快、最短路径或者远离障碍等(等式8)</li></ul><p>因此，在“时间弹性带”的背景下，这些约束被表述为一个分段连续、可微的代价函数的目标，该函数会惩罚违反约束的行为：</p><p><span class="math display">\[e_{\Gamma}\left(x, x_{r}, \epsilon, S, n\right) \simeq \begin{cases}\left(\frac{x-\left(x_{r}-\epsilon\right)}{S}\right)^{n} &amp; \text { if } x&gt;x_{r}-\epsilon \\ 0 &amp; \text { otherwise }\end{cases}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(x_r\)</span>表示边界</li><li><span class="math inline">\(S,n,\epsilon\)</span> 影响近似的准确性</li><li>特别的，<span class="math inline">\(S\)</span>表示尺度缩放，<span class="math inline">\(n\)</span>表示阶数，<span class="math inline">\(\epsilon\)</span>是近似的一个位移小量</li></ul><p>图3展示了等式6的两个不同的实现。 Approximation 1 （n = 2, S = 0.1, <span class="math inline">\(\epsilon\)</span>= 0.1） ， Approximation 2 （n = 2, S = 0.05 and <span class="math inline">\(\epsilon\)</span> = 0.1） ， 这个例子展示了约束<span class="math inline">\(x_r=0.4\)</span>的一个近似。</p><p>使用多目标优化框架的一个明显优势是目标函数的模块化表达。目前TEB所采用的目标函数如下：</p><h2 id="way-points-and-obstacles">Way points and obstacles</h2><p>TEB同时考虑原始路径的中间路径点的到达和避免静态或者动态的障碍物。这两个目标函数相似，不同之处在于点吸引橡皮筋，而障碍物排斥它。</p><p>目标函数企图最小化TEB和way point的距离<span class="math inline">\(d_{min,j}\)</span>，如图4所示：</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719194704.png"></p><p>对于way point的情况，其距离以最大的目标半径<span class="math inline">\(r_{pmax}\)</span>为界，这些约束由公式6中的惩罚函数实现：</p><p><span class="math display">\[\begin{aligned}f_{\text {path }} &amp;=e_{\Gamma}\left(d_{\min , j}, r_{p_{\max }}, \epsilon, S, n\right) \\f_{o b} &amp;=e_{\Gamma}\left(-d_{\min , j},-r_{o_{\min }}, \epsilon, S, n\right)\end{aligned}\]</span></p><p>由图3可知，必须将Eq. 8中<span class="math inline">\(d_{\min , j}\)</span>和<span class="math inline">\(r_{o_{min}}\)</span>交换符号来实现下界.</p><p>注意，这些目标函数的梯度可以解释为作用在弹性带上的外力</p><h2 id="velocity-and-acceleration">Velocity and acceleration</h2><p>机器人速度和加速度的动力学约束用与几何约束相似的罚函数来描述，图2展示了TEB的结构，线速度和角速度的均值使用两个连续配置<span class="math inline">\(x_i,x_{i+1}\)</span>之间的欧式距离和角距离和时间差<span class="math inline">\(\Delta T_i\)</span>来计算：</p><p><span class="math display">\[\begin{aligned}v_{i} &amp; \simeq \frac{1}{\Delta T_{i}}\left\|\left(\begin{array}{l}x_{i+1}-x_{i} \\y_{i+1}-y_{i}\end{array}\right)\right\| \\\omega_{i} &amp; \simeq \frac{\beta_{i+1}-\beta_{i}}{\Delta T_{i}}\end{aligned}\]</span></p><p>由于配置的临近，欧几里得距离是两个连续姿态之间的圆路径的真实长度的充分近似值。</p><p>加速度涉及两个连续的平均速度，因此考虑三个连续的构型，其中两个对应时间差：</p><p><span class="math display">\[a_{i}=\frac{2\left(v_{i+1}-v_{i}\right)}{\Delta T_{i}+\Delta T_{i+1}}\]</span></p><p>为了清楚起见，用上式两个相关的速度来代替这三个连续的配置，旋转的加速度计算类似。</p><p>考虑一个差分驱动的移动机器人，轮速和位移<span class="math inline">\(v_i\)</span>、旋转速度<span class="math inline">\(w_i\)</span>关于机器人中心点的关系如下：</p><p><span class="math display">\[\begin{aligned}v_{w_{r}, i} &amp;=v_{i}+L \omega_{i} \\v_{w_{l}, i} &amp;=v_{i}-L \omega_{i}\end{aligned}\]</span></p><p>其中，L为机器人轮距的一半</p><p>将公式12和公式13（即上面两个式子）对时间进行微分就得到了相应的车轮加速度。车轮的速度和加速度是有界的，可根据制造商的规格获取。机器人的平移和转动惯量可以以一种明显的方式包括在内，但在这第一个实现中，我们还没有这样做。</p><h2 id="non-holonomic-kinematics非完整约束运动学方程">Non-holonomic kinematics（非完整约束运动学方程）</h2><p>差动驱动的机器人只有两个局部自由度，因此，它们只能在机器人当前航向的方向执行运动，这种运动学约束导致了由圆弧段组成的平滑路径。</p><p>因此，两个相邻的构型需要位于一个常曲率的公共圆弧上，如图5所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719200953.png"></p><p>初始配置<span class="math inline">\(x_i\)</span>和方向<span class="math inline">\(d_{i,i+1}\)</span>之间的角度<span class="math inline">\(\vartheta_{i}\)</span>必须等于配置<span class="math inline">\(x_{i+1}\)</span>和方向<span class="math inline">\(d_{i,i+1}\)</span>之间的夹角，即：</p><p><span class="math display">\[\vartheta_{i}=\vartheta_{i+1}\]</span></p><p>根据二维叉积（<span class="math inline">\(A \times B=|A|·|B|·\sin \alpha\)</span>），有：</p><p><span class="math display">\[\Leftrightarrow\left(\begin{array}{c}\cos \beta_{i} \\\sin \beta_{i} \\0\end{array}\right) \times \mathbf{d}_{i, i+1}=\mathbf{d}_{i, i+1} \times\left(\begin{array}{c}\cos \beta_{i+1} \\\sin \beta_{i+1} \\0\end{array}\right)\]</span></p><p>其中，机器人的绝对旋转为<span class="math inline">\(\beta_i\)</span>，位移方向向量为：</p><p><span class="math display">\[\mathbf{d}_{i, i+1}:=\left(\begin{array}{c}x_{i+1}-x_{i} \\y_{i+1}-y_{i} \\0\end{array}\right)\]</span></p><p>因此，对应的目标函数为：</p><p><span class="math display">\[f_{k}\left(\mathbf{x}_{i}, \mathbf{x}_{i+1}\right)=\left\|\left[\left(\begin{array}{c}\cos \beta_{i} \\\sin \beta_{i} \\0\end{array}\right)+\left(\begin{array}{c}\cos \beta_{i+1} \\\sin \beta_{i+1} \\0\end{array}\right)\right] \times \mathbf{d}_{i, i+1}\right\|^{2}\]</span></p><p>惩罚违反此约束的二次误差。一个潜在的180方向的变化用一个额外的项来处理</p><h2 id="fastest-path">Fastest path</h2><p>以往的“松紧带”方法通过收缩松紧带的内力获得最短路径。由于我们的方法考虑时间信息作为最短路径的目标，我们可以选择用最快路径的目标代替最短路径的目标，或者将这些目标结合起来。</p><p>最快路径的目标很容易通过最小化所有时间差的和的平方来实现：</p><p><span class="math display">\[f_{k}=\left(\sum_{i=1}^{n} \Delta T_{i}\right)^{2}\]</span></p><p>这一目标导致了一种最快的路径，其中中间配置在时间上均匀分离，而不是在空间上</p><h2 id="实现">实现</h2><p>图6展示了实现TEB的控制流程，在初始化阶段，初始路径被增强为初始轨迹，方法是根据动力学和运动学约束添加默认的时间信息。</p><p>在我们的例子中，初始轨迹是由带有纯旋转和平移的分段线性分段组成的，这种以多边形表示的路径通常由概率路线图规划者提供[9]，另外，reed - shepp路径很容易被增强为允许的轨迹[10]。</p><p>在每一次迭代中，算法动态地添加新的结构或删除以前的结构，以调整空间和时间分辨率以适应剩余的轨迹长度或规划水平。</p><p>一个滞后被实施以避免振荡。将优化问题转化为一个超图，用包含在“g20 -框架”中的大规模稀疏系统优化算法求解[11]</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719204210.png"></p><p>所要求的超图是一条边的连接节点数量不受限制的图,因此，一条边可以连接两个以上的节点。</p><p>TEB问题(Eq. 4)可以转化为一个以配置和时间差为节点的超图。它们与表示给定目标函数fk或约束函数的边相连，图7展示了一个两配置一个时间差和一个点状障碍物的超图，</p><p><img src="http://s1.nsloop.com:59080/images/2021/07/19/20210719204524.png"></p><p>速度边界目标函数要求的平均速度与两个配置之间的欧氏距离和所需的时间有关。因此它形成一条连接B的那些状态的边。</p><p>障碍物需要一条与最近的配置相连的边，表示障碍物的节点是固定的(双圆)，因此优化算法无法改变其参数(位置)</p><p>在验证优化后的TEB后，可以通过计算控制变量v和ω来直接命令机器人驱动系统。</p><p>在每一次新的迭代之前，重新初始化阶段将检查新的和变化的way-points，这将会比较有用如果way-points是在分析相机或者激光数据之后才收到的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;trajectory-modification-considering-dynamic-constraints-of-autonomous-robots&quot;&gt;Trajectory modification considering dynamic constraint
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>Hector-SLAM论文阅读</title>
    <link href="http://yoursite.com/2021/07/11/Hector-SLAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/07/11/Hector-SLAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-07-11T06:03:53.000Z</published>
    <updated>2021-07-11T13:56:26.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="a-flexible-and-scalable-slam-system-with-full-3d-motion-estimation">A Flexible and Scalable SLAM System with Full 3D Motion Estimation</h1><p><img src="http://s1.nsloop.com:59080/images/2021/07/11/20210711214658.png"></p><h1 id="摘要">摘要</h1><p>在许多应用场景中，比如城市搜救和搜索（USAR）机器人，需要去获取未知环境的地图。我们提出了一个快速在线学习占用栅格地图、占用较少计算资源的系统。它利用激光雷达系统与基于惯性传感器的3D位姿估计系统进行融合，实现了一种鲁棒的扫描匹配方法。通过地图变化的快速近似和多分辨率栅格地图，在各种有挑战性的环境中实现了可靠的定位与建图。提供了多种数据集以适应嵌入式手持建图系统。我们表明，该系统是足够准确的，在我们考虑的应用场景中，不需要显式闭环检测技术。该软件可作为ROS的开源代码包。</p><h1 id="介绍">介绍</h1><p>学习环境模型并定位自身是一个真正的机器人在真实世界运行的最重要的能力。在本文中，我们提出了一种灵活的、可升级的系统来解决SLAM问题，已成功的运用在了UGV、USV和一个小型的室内导航系统上。该方法消耗的计算资源较少，故可以应用于低成本、低功耗的嵌入式系统。该方法是在ROS上实现的开源软件。它适应ROS上的API和导航stack，并可以在ROS的生态中替代其他SLAM方法。</p><p>本文介绍的系统旨在保证计算力要求低的前提下，实现足够精确的环境感知和自我定位。它可以应用在小尺度的、不必做大的闭环的系统中，并需要使用高更新速率的激光雷达系统。类似的场景包括RoboCup搜救比赛，可能需要在模拟的地震场景中找到受害者，因此需要对车辆在6Dof上进行姿态估计。或者，比地面机器人更灵活的室内飞行器的导航。有关USAR的结果和模型在参考【2】中可以找到。</p><p>我们的方法结合了2D SLAM（基于激光雷达的平面地图）和3D导航（基于IMU）融合了2D的SLAM信息作为辅助（FIG.I）。SLAM过程通常是由激光雷达的数据更新来触发的，而整个3D导航解决方案是需要实时计算的，构成车辆控制系统的一部分。</p><h1 id="相关工作">相关工作</h1><p>近些年已经有大量的研究关于SLAM的问题，例如作为开源软件的gmapping使用的是Rao-Blackwellized粒子过滤器，可以可靠在的典型办公室室内场景使用。然而，这些解决方案的工作最好在平面环境，依赖于现有的，足够精确的航迹以及不利用现代雷达系统提供的高更新率。对于非结构化环境，会导致载体的显著滚转和俯仰运动，或在空中平台上实现这种系统不适用或必须进行显著修改。</p><p>一个SLAM的前端和后端系统之间的区别。在大满贯的前端，用于实时在线估计机器人运动，后端用于优化位姿图，和在使用的前端产生的位姿之间的约束。本文提出的方法可以作为一个SLAM的前端和不提供姿势图优化像[ 4 ]和[ 5 ]提出的解决方案。然而，我们表明，在许多情况下，这种优化并不需要在现实中的一些条件，因为这种方法是足够准确的，可用于机器人来执行他们的任务。</p><p>基于激光扫描匹配的室内导航系统提出了对旋翼无人机使用[ 6 ]、[ 7 ]、[ 8 ]。在这里，采用两阶段的方法，前端快速扫描的位姿估计，和用于在后台或远程计算机上进行的较慢的后端建图步骤。从雷达扫描对准的位姿估计不直接纳入车辆的控制回路，因此他们只在低速行驶。</p><p>在[ 9 ]和[ 10 ]中描述了移动机器人上使用的其他前端系统。在本文的对比，他们没有提供完整的六自由度位姿估计和开源软件。</p><p>使用扫描匹配进行定位的工作始于ICP[ 11 ]，它起源于注册三维点云的一般方法。许多基于ICP的方法的主要缺点是对点对应的高复杂度搜索，这必须在每次迭代中进行。极坐标扫描匹配（PSM）[ 12 ]避免了利用激光扫描的自然极坐标系统来估计它们之间的匹配的对应搜索。扫描进行预处理，可用于极性扫描匹配。实时相关扫描匹配方法[ 13 ]采用穷举抽样方法进行扫描匹配。通过多种优化，这种方法能够实时应用。基于正态分布变换（NDT）[ 14 ]的扫描匹配将扫描对齐到代表前扫描的正态分布混合。</p><p>对于沿海的情况，有研究使用昂贵的多传感器扫描仪[ 15 ]，但据笔者的知识，没有单发射器激光雷达为基础的SLAM方法，可在现实世界中的条件下进行测试。</p><h1 id="系统概述">系统概述</h1><p>相对许多其他基于网格的2D-SLAM来说，本文提供了一种可用的，具有完整的6自由度运动的平台。我们的系统可以预测6自由度的平移和旋转状态。为了实现这一点，我们的系统由两个主要部分组成，导航滤波器融合了来自于惯性传感器和其他可用的传感器到一个可用的3D数据，而2D SLAM则提供平面的位姿信息。这两部分的更新都是单独的，是松耦合系统，他们会定时保持同步。</p><p>我们定义导航坐标系统是一个右手系统，它的原点在平台的起点上，Z轴指向上方，X轴在启动时指向平台的朝向。</p><p>完整的3D状态表示为<span class="math inline">\(\mathbf{x}=\left(\begin{array}{lll}\mathbf{\Omega}^{\mathrm{T}} &amp; \mathbf{p}^{\mathrm{T}} &amp; \mathbf{v}^{\mathrm{T}}\end{array}\right)^{\mathrm{T}}\)</span>，其中，<span class="math inline">\(\mathbf{\Omega}=(\phi, \theta, \psi)^{\mathrm{T}}\)</span>表示欧拉角的roll，pitch和yaw，<span class="math inline">\(\mathbf{p}=\left(p_{x}, p_{y}, p_{z}\right)^{\mathrm{T}}\)</span>和<span class="math inline">\(\mathbf{v}=\left(v_{x}, v_{y}, v_{z}\right)^{\mathrm{T}}\)</span>分别表示在导航坐标系下的位置和速度。</p><p>惯性测量使用<span class="math inline">\(\mathbf{u}=\left(\begin{array}{ll}\omega^{T} &amp; \mathbf{a}^{T}\end{array}\right)^{T}\)</span>来表示，其中<span class="math inline">\(\mathbf{a}=\left(a_{x}, a_{y}, a_{z}\right)^{\mathrm{T}}\)</span>和<span class="math inline">\(\mathbf{w}=\left(w_{x}, w_{y}, w_{z}\right)^{\mathrm{T}}\)</span>分别表示加速度和角速度。任意刚体的运动可以表示为如下非线性微分方程（简化版）：</p><p><span class="math display">\[\begin{aligned}\dot{\mathbf{\Omega}} &amp;=\mathbf{E}_{\Omega} \cdot \boldsymbol{\omega} \\\dot{\mathbf{p}} &amp;=\mathbf{v} \\\dot{\mathbf{v}} &amp;=\mathbf{R}_{\Omega} \cdot \mathbf{a}+\mathbf{g}\end{aligned}\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;a-flexible-and-scalable-slam-system-with-full-3d-motion-estimation&quot;&gt;A Flexible and Scalable SLAM System with Full 3D Motion Estimati
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>T-LOAM论文阅读</title>
    <link href="http://yoursite.com/2021/06/11/T-LOAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/06/11/T-LOAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-06-11T06:03:53.000Z</published>
    <updated>2021-06-14T10:18:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="t-loam-truncated-least-squares-lidar-only-odometry-and-mapping-in-real-time">T-LOAM: Truncated Least Squares LiDAR-Only Odometry and Mapping in Real Time</h1><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611150206.png"></p><h1 id="摘要">摘要</h1><p>提出了一个基于截断最小二乘法的新颖的、计算效率高、鲁棒的纯激光里程计。我们的方法侧重于减轻异常值的影响，允许在退化发生的稀疏，嘈杂或杂乱的情况下允许强大的导航。</p><p>作为预处理，使用了多区域的地面提取，动态曲率体素的聚类方法来完成3D点云的分割和滤除不稳定目标的工作。</p><p>提出一个新颖的特征提取模块，用于区分：边缘特征、球形特征、平面特征、地面特征。</p><p>作为前端，是一个基于分层特征的纯激光雷达里程计，通过截断最小二乘法来直接处理不同的特征以进行精确地运动估计。预处理模型和运动估计精度已经在 KITTI 里程计基准以及各种校园场景中进行了评估。 实验结果证明了所提出的<code>T-LOAM</code>的实时能力和优越的精度优于其他最先进的算法。</p><h2 id="变量约定">变量约定</h2><ul><li><p><code>DCVC</code>聚类: Dynamic curved-voxel clustering</p></li><li><p><code>MRGE</code>地面提取：Dultiregion ground extraction</p></li><li><p><code>TLS</code>截断最小二乘：Truncated least squares</p></li><li><p><span class="math inline">\(t_k\)</span>: 第k帧激光扫描结束的时间</p></li><li><p><span class="math inline">\(L_k\)</span>: 时间<span class="math inline">\(t_k\)</span>对应的LiDAR body坐标系</p></li><li><p><span class="math inline">\(Q_i,S_j\)</span>: <code>MRGE</code>的第i个象限，第j部分</p></li><li><p><span class="math inline">\(\mathbb{F}_{k}^{v}, \mathbb{F}_{k}^{b}\)</span>: 时间<span class="math inline">\(t_k\)</span><code>MRGE</code>的前景和背景</p></li><li><p><span class="math inline">\(\mathbb{F}_{k}^{d}\)</span>： 在时间<span class="math inline">\(t_k\)</span>由<span class="math inline">\(\mathbb{F}_{k}^{v}\)</span>输出的<code>DCVC</code>模型</p></li><li><p><span class="math inline">\(\mathbb{F}_{k}\)</span>: 在时间<span class="math inline">\(t_k\)</span>对应<span class="math inline">\(L_k\)</span>坐标系的所有特征</p></li><li><p><span class="math inline">\(F_{k}^{e}, F_{k}^{s}, F_{k}^{p}, F_{k}^{g}\)</span>：边缘、球形、平面、地面特征</p></li><li><p><span class="math inline">\(\mathbb{M} \mathbb{l}_{k}^{w}\)</span>：时间<span class="math inline">\(t_k\)</span>的世界坐标系中的子图第k特征点</p></li><li><p><span class="math inline">\(x_{k}^{w}\)</span>：时间<span class="math inline">\(t_k\)</span>对应的机器人在全局坐标系的状态</p></li></ul><h1 id="介绍">介绍</h1><p>在本文中，已经实施了一系列改进，以解决当前激光SLAM框架中存在的缺陷。 截断最小二乘（TLS）方法创新的应用于扫描匹配，使<code>T-LOAM</code>对于异常值更加鲁棒，并减轻其对初始值解决方案的依赖性。 原始点云通过精心设计的预处理模块进行了分割，以提取四个独特的功能：边缘特征，球形特征，平面特征和地面地面。 为了实现更加平滑的里程计，通过TLS方法构建了与不同特征相关的三种残差函数。 最后，构建如图1所示全局一致的地图。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613122201.png"></p><p>使用了KITTI Odomerty数据集来与其他state-of-art的方法相比，如<code>F-LOAM</code>,<code>A-LOAM</code>,<code>SuMa</code>等，实验结果表明<code>T-LOAM</code>在大多数序列实验场景中达到更优越的性能。</p><p>本文主要贡献：</p><ul><li>提出一个基于截断最小二乘TLS的高效率鲁棒激光里程计，用于进行导航和构建全局一致性高的点云地图。</li><li>利用MRGE地面提取和DCVC聚类来实现一系列预处理步骤，提高点云分割的精度</li><li>首次使用<code>Open3D</code>进行激光SLAM框架的开发，并开源</li></ul><p>本方法受<code>graduated nonconvexity(GNC)</code>方法的启发，这在计算机视觉字段中执行了各种匹配和优化任务，已经证明有效。根据Convex优化理论基于TLS构建一种新颖的姿态优化函数，以估算6自由度的变换。</p><p>该实现遵循如图2所示，并通过多线程和OpenMP [28]设置并行化以保证算法的整体运行效率，另外还使用了ROS社区的<code>nodelet</code>功能包来实现<code>Zero copy</code>.</p><h1 id="数据集与系统硬件介绍">数据集与系统硬件介绍</h1><p>本文中使用的TX2机器人是远程化的车辆,这是由双无刷电机驱动，由670-WH电池供电。LIDAR的安装高度设置为0.75米，以保证全向扫描。 通过遥控记录实验数据集。 此外，LIDAR采样频率设定为10 Hz。</p><p>采用两台计算机评估所提出的框架的实时能力，包括NVIDIA Jetson TX2(ARM Cortex-A57 CPU)以及2.5-GHz i7-10750H CPU的笔记本电脑。</p><p>所有模块由C ++实现，并将Nodelet包作为Ubuntu 18.04 Linux中的唯一节点集成到ROS [29]中，以完成它们之间的零拷贝传输。 本文中提出的实验仅利用这些系统中的CPU运行。</p><h1 id="预处理模块">预处理模块</h1><h2 id="框架总览">框架总览</h2><p><code>T-LOAM</code>框架如图2所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613122918.png"></p><p>首先，对激光扫描进行关于旋转的畸变矫正，然后，将点云输入到<code>MRGE</code>模块中以获取前景<span class="math inline">\(\mathbb{F}_{k}^{v}\)</span>和背景<span class="math inline">\(\mathbb{F}_{k}^{b}\)</span>。随后，使用<code>DCVC</code>方法来对前景进行分割，并滤除不稳定的类别，得到<span class="math inline">\(\mathbb{F}_{k}^{d}\)</span>。</p><p>此外，通过特征提取模块来分别得到特征点<span class="math inline">\(\mathbb{F}_{k}=\left\{F_{k}^{e}, F_{k}^{s}, F_{k}^{p}, F_{k}^{g}\right\}\)</span>，通过预处理扫描，关联的特征点通过位姿优化模块配准到子图上，得到全局一致性地图。此外，基于连续的激光扫描位姿优化得到的平移，将用于对当前帧扫描特征点云进行平移量的矫正。</p><h2 id="多区域地面提取">多区域地面提取</h2><p>地面点云通常占据了车载激光雷达扫描的大部分，并通常具有较简单的数学模型，有利于处理。</p><p>特别的，这些特征可以直接用于构造位姿优化的约束。但是，仅使用单个平面模型不足以准确表示复杂地形中的分布，因此，采用了<code>MRGE</code>方法来提高分割精度。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613224514.png"></p><p>在算法一中，原始的激光扫描首先根据极坐标分为多个象限（默认是4），如图4(a)所示。其次，对于每一个象限，都将继续分为多个子区域（默认为3），如图4(b)所示，其中，每个子区域的边界如下计算：</p><p><span class="math display">\[\theta_{i}=\theta_{s}+\frac{n}{b} \alpha \cdot k_{i}\]</span></p><p><span class="math display">\[\lambda_{i}=\left\{\begin{array}{ll}h\left(\frac{1}{\tan \left(\theta_{i}\right)}\right), &amp; \text { if } i \geq 1 \\0, &amp; \text { if } i=0\end{array}\right.\]</span></p><p>其中，<span class="math inline">\(\theta_s\)</span>是激光扫描的起始仰角，<span class="math inline">\(b\)</span>是子区域数，<span class="math inline">\(n\)</span>是可能包含地面点的激光线束数总和。特别的，<span class="math inline">\(\alpha\)</span>代表激光雷达的垂直角分辨率，<span class="math inline">\(h\)</span>表示激光雷达的安装高度，<span class="math inline">\(k,i\)</span>分别表示区域的系数和索引。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613175331.png"></p><p>对于自动驾驶车辆来说，地面点的高度坐标分量通常位于最低位置，因此，可以利用这个先验信息来根据高度值对区域点云进行整理。</p><p>种子点在指定的阈值<span class="math inline">\(\tau\)</span>内进行选择，主要用于拟合初始的平面模型。为了提高模型精度，多轴线性回归方法量身定制以计算相关参数，主要方向被加权，以减轻异常值的影响。线性平面模型被量身定制以反映子区域的分布，通过如下：</p><p><span class="math display">\[\begin{aligned}a x+b y+c z+d &amp;=0 \\n^{T} \mathrm{p} &amp;=-d\end{aligned}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(n=\left[\begin{array}{lll}a &amp; b &amp; c\end{array}\right]^{T}\)</span>，表示平面法向量</li><li><span class="math inline">\(\mathrm{p}=\left[\begin{array}{lll}x &amp; y &amp; z\end{array}\right]^{T}\)</span>，表示平面上的点</li></ul><p>特别的，协方差矩阵M用于获取对应的每个子区域中指定的种子点的分散性：</p><p><span class="math display">\[\mathcal{M}=\sum_{i=1}^{|s|}\left(s_{i}-\bar{s}\right)\left(s_{i}-\bar{s}\right)^{T}=\left(\begin{array}{lll}a_{1} &amp; a_{2} &amp; a_{3} \\b_{1} &amp; b_{2} &amp; b_{3} \\c_{1} &amp; c_{2} &amp; c_{3}\end{array}\right)\]</span></p><p>其中，<span class="math inline">\(\bar{s} \in \mathbb{R}^{3}\)</span>记为子区域中的所有点<span class="math inline">\(s_{i} \in S\)</span>的均值。</p><p>下一步，计算出三个主方向：</p><p><span class="math display">\[\begin{array}{l}v_{x}=\left[\begin{array}{l}b_{2} c_{3}-b_{3} b_{3} \\a_{3} b_{3}-a_{2} c_{3} \\a_{2} b_{3}-a_{3} b_{2}\end{array}\right], \quad v_{y}=\left[\begin{array}{l}a_{3} b_{3}-a_{2} c_{3} \\a_{1} c_{3}-a_{3} a_{3} \\a_{2} a_{3}-a_{1} b_{3}\end{array}\right] \\v_{z}=\left[\begin{array}{l}a_{2} b_{3}-a_{3} b_{2} \\a_{2} a_{3}-a_{1} b_{3} \\a_{1} b_{2}-a_{2} a_{2}\end{array}\right]\end{array}\]</span></p><p>为了减轻异常值对法向量估计的影响，每个主方向加权，并应用线性回归来细化正常向量。 权重值是所示的每个主方向的二范数，如下：</p><p><span class="math display">\[\begin{aligned}n &amp;=\sum_{k \in\{x, y, z\}} w_{k} v_{k} \\w_{x} &amp;=v_{x}[0]^{2}, \quad w_{y}=v_{y}[1]^{2}, w_{z}=v_{z}[2]^{2}\end{aligned}\]</span></p><p>其中，<span class="math inline">\([]\)</span>操作符表示取该向量对应的元素</p><p>参数<span class="math inline">\(d\)</span>可以根据平面方程和<span class="math inline">\(p,\bar{s}\)</span>计算获得，即当法向量<span class="math inline">\(n\)</span>获取后，将一个点代入最终的平面模型即可计算得到。</p><h2 id="动态曲率体素聚类">动态曲率体素聚类</h2><p>为了准确高效对点云进行分割，提出了<code>DCVC</code>方法。改进的空间体素类型满足[24]中描述的三个重要方面，以及与点云的空间分布更加对应。</p><ul><li>定义1： 第i，j，k个动态弯曲的voxel表示一个3D空间的体素单元，其根据配置可以如下计算：</li></ul><p><span class="math display">\[\begin{aligned}D C V_{i, j, k}=\left\{P(\rho, \theta, \phi)=\mid \rho_{i}\right.&amp; \leq \rho&lt;\rho_{i}+\Delta \rho_{i} \\\theta_{j} &amp; \leq \theta&lt;\theta_{j}+\Delta \theta_{j} \\\phi_{k} &amp;\left.\leq \phi&lt;\phi_{k}+\Delta \phi_{k}\right\}\end{aligned}\]</span></p><p>其中，每个<span class="math inline">\(P(\rho, \theta, \phi)\)</span>是极坐标系中的极径<span class="math inline">\(\rho\)</span>、方位角（俯仰）<span class="math inline">\(\theta\)</span>，极角（水平）<span class="math inline">\(\phi\)</span>。特别的，<span class="math inline">\(\Delta \rho_i,\Delta \theta_j, \Delta \phi_k\)</span>表示根据点云的稀疏性和距离值调整的每个体素单元的边界上下界之差（即体素的尺寸）。</p><p>另外，每个方向上的体素的增量可以计算如下：</p><p><span class="math display">\[\begin{array}{l}\Delta \rho_{i}=\rho_{s}-\left(a \times s_{i}+b\right) \times \Delta \rho \\\Delta \theta_{j}=s_{j} \times \Delta \theta \\\Delta \phi_{k}=s_{k} \times \Delta \phi\end{array}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\rho_s\)</span>表示起始的体素极径</li><li><span class="math inline">\(s\)</span>表示步长</li><li><span class="math inline">\(\Delta \rho,\Delta \theta, \Delta \phi\)</span>分别对应极径、方位角、极角的增量</li><li><span class="math inline">\(a,b\)</span>是体素系数，其可以根据各种光束的点云和3D LIDAR的密度来确定</li></ul><p>为了更好地理解实际分布，在图5中可视化两个相邻的空间动态体素。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613220812.png"></p><blockquote><p>这个图右图有点问题，图中的z轴应该是y轴才对，因为右图是俯视图</p></blockquote><p>稀疏系数可用于调节极径方向上不同距离的体积，这有利于防止对空间中相邻物体区分失败。</p><p><code>DCVC</code>方法总结成算法2。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/13/20210613232018.png"></p><p>我们首先将笛卡尔坐标系的点云转换到极坐标系中，同时建立弯曲的体素。</p><p>然后，将非空的动态体素构建成哈希表，以提高搜索效率。</p><p>随后，根据哈希表映射关系，搜索当前点所在体素的周围体素，并将它们合并到统一标签中。</p><p>最后，我们返回每个点的类别信息，并提供合成判断，以滤除一组或潜在的动态目标，以获得最终的点云<span class="math inline">\(\mathbb{F}_{k}^{d}\)</span>。以这种方式，当各种对象彼此依然相邻时，显著抑制了不正确的分割分类的发生。</p><h2 id="特征提取">特征提取</h2><p>很明显，<code>LOAM</code>[10]中呈现的算法容易受几何退化场景的影响，这将直接衰减激光雷达测量仪的精度甚至使其失败。</p><p>为了提高纯激光雷达里程计的稳定性，<code>T-LOAM</code>将从<span class="math inline">\(\mathbb{F}_{k}^{d}\)</span>和<span class="math inline">\(\mathbb{F}_{k}^{b}\)</span>提取4个可区分的几何特征，包括边缘特征<span class="math inline">\(F_{k}^{e}\)</span>，球面特征<span class="math inline">\(F_{k}^{s}\)</span>，平面特征<span class="math inline">\(F_{k}^{p}\)</span>和地面特征<span class="math inline">\(F_{k}^{g}\)</span>。</p><p>背景<span class="math inline">\(\mathbb{F}_{k}^{b}\)</span>首先进行降采样，边缘则使用<code>LOAM</code>[10]方法，根据平滑度提取：</p><p><span class="math display">\[c=\frac{1}{|s| \cdot\left\|p_{i}\right\|}\left\|\sum_{m \in s, m \neq i}\left(p_{m}-p_{i}\right)\right\|\]</span></p><p>其中，<span class="math inline">\(s\)</span>表示同一激光束的连续10个点的集合，即包含点<span class="math inline">\(p_i\)</span>左右两侧的5个点</p><p>另外，球面特征<span class="math inline">\(F_{k}^{s}\)</span>和平面特征<span class="math inline">\(F_{k}^{p}\)</span>的垂直部分可以通过PCA算法获取。它用于捕捉局部领域的描述，包括曲率、主方向、次方向、法向量以及相应的特征值。为了实现上述，需要计算的协方差矩阵如下计算：</p><p><span class="math display">\[\mathcal{M}=\frac{1}{k} \sum_{i=1}^{k}\left(p_{i}-\overline{p_{k}}\right)\left(p_{i}-\overline{p_{k}}\right)^{T}\]</span></p><p>其中，<span class="math inline">\(k\)</span>表示总的点数，<span class="math inline">\(\bar{p_k}\)</span>表示点集的均值。</p><p>进一步的，对协方差矩阵M执行了SVD分解，以获取特征值<span class="math inline">\(\lambda\)</span>和特征向量<span class="math inline">\(v\)</span>。<strong>可以通过协方差矩阵的特征向量来区分各种特征，这些特征向量与最显著的数据方差方向相关联</strong>。一些分散点主要根据特征值近似平等的方法来进行过滤。然后，点云的平坦度<code>flatness</code>和球面度<code>sphericity</code>[30]可以计算如下：</p><p><span class="math display">\[\begin{aligned}\gamma &amp;=\frac{\lambda_{3}}{\lambda_{1}+\lambda_{2}+\lambda_{3}} \\\sigma &amp;=\frac{\lambda_{2}-\lambda_{3}}{\lambda_{1}}, \quad \psi=\frac{\lambda_{3}}{\lambda_{1}}\end{aligned}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\gamma\)</span>是曲率</li><li><span class="math inline">\(\sigma\)</span>是平坦度</li><li><span class="math inline">\(\psi\)</span>是球面系数</li><li>需要注意的是，特征值以降序的顺序给出</li></ul><p>如算法3展示，如果平坦度系数<span class="math inline">\(\sigma\)</span>比平面特征阈值<span class="math inline">\(\tau\)</span>更大，然后将点作为平面特征的垂直部分提取。对于非平面部分，如果球面系数<span class="math inline">\(\psi\)</span>比球面特征阈值<span class="math inline">\(\pi\)</span>更大，点将会添加到球面特征中。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/14/20210614181724.png"></p><p>来自<code>HDL-64E</code>激光雷达的不同特征的提取示意图如图6所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/14/20210614181433.png"></p><p>进一步的，主方向和法向量将会在后续残差计算中继续使用，以进一步细化水平和垂直分布的关联特征。</p><h1 id="位姿优化">位姿优化</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;t-loam-truncated-least-squares-lidar-only-odometry-and-mapping-in-real-time&quot;&gt;T-LOAM: Truncated Least Squares LiDAR-Only Odometry and
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>LION论文阅读</title>
    <link href="http://yoursite.com/2021/06/09/LION%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/06/09/LION%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-06-09T02:03:53.000Z</published>
    <updated>2021-06-11T02:53:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="lion-lidar-inertial-observability-aware-navigator-for-vision-denied-environments">LION: Lidar-Inertial Observability-Aware Navigator for Vision-Denied Environments</h1><p><a href="https://youtu.be/Jd-sqBioarI" target="_blank" rel="noopener">Video</a></p><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609104338.png"></p><h1 id="摘要">摘要</h1><p>在GPS拒绝和感知的环境中导航的机器人的状态估计，例如地下隧道，矿区和行星子表面空隙[1]，在机器人中仍然具有挑战性。</p><p><code>LION</code>通过融合来自IMU的高频惯性数据和通过固定滞后滑动窗口的LIDAR低频相对位姿观测来提供高频的里程估计。同时也不需要事先获取LiDAR和IMU外参，进行了实时在线外参标定，此外，还使用了可观测性的度量来评估位姿估计是否是几何病态的。</p><h1 id="介绍">介绍</h1><p>提出了LiDAR-Inertial的可观测性算法，用于在感知性降低的环境中，这是<code>CoSTAR</code>团队在第一届<code>DARPA Subterranean</code>挑战中的方法。</p><p>我们的解决方案依赖于IMU预积分和<code>scan-to-scan</code>的ICP，以及固定滞后的滑动窗口。并且该方法实时在线标定LiDAR和IMU的外参，为了解决潜在的可观测性问题，我们使用了几何可观测性分数[23]，使得<code>LION</code>预测其输出中的观测场景的几何结构。通过该输出的得分，基于监督算法(如HeRO)，我们可以切换到不同的状态估计算法（如轮速编码、视觉惯性等）。该方法保证了状态估计的连续性、可靠性以及重力对齐的特点，为后续级联规划和控制算法提供保障。</p><h1 id="具体方法">具体方法</h1><h2 id="激光惯性里程计">激光惯性里程计</h2><p><code>LION</code>是基于滑动窗口的LIO，分为两个部分，前端由LO和IMU预积分、可观测性检测模块组成，后端是因子图优化，如图2所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609151034.png"></p><p>在下面的叙述中，遵循如下约定：</p><ul><li>里程计世界坐标系<span class="math inline">\(W\)</span></li><li>载体坐标系<span class="math inline">\(B\)</span>，即IMU坐标系</li><li>激光雷达坐标系<span class="math inline">\(L\)</span></li><li>一个点聪坐标系A到坐标系B的表示为4x4的矩阵：<span class="math inline">\(_B\mathbf{T}_A\)</span>，由旋转矩阵<span class="math inline">\(_B\mathbf{R}_A\)</span>和平移向量<span class="math inline">\(_Bt_A\)</span>组成</li></ul><p>LO模块使用<code>GICP</code>[31]来获取两帧激光扫描之间的相对位姿变换<span class="math inline">\(_{L_{k-1}} \mathbf{T}_{L_{k}}\)</span>。为了简化ICP算法的收敛性，对于每一帧激光扫描到达，首先进行重力对齐（使用IMU对其进行旋转部分的坐标表变换作为ICP的初始值）。</p><p>IMU预积分模块利用state-of-art的流形积分理论来将关键帧之间的IMU整合成单个运动约束[22,32]。</p><p>或者，基于scan-to-scan的前端可以使用<code>LOCUS</code>[33]框架替换，它额外地将新的激光扫描与局部地图进行对齐，以执行一个refinement步骤。</p><p>在后端，由前端产生的相对位姿观测与IMU测量结合使用，图3展示了因子图中的状态和因子，其中，在第j个时间步的状态<span class="math inline">\(x_j\)</span>表示如下：</p><p><span class="math display">\[\mathbf{x}_{j}:=\left\{_{W} \mathbf{T}_{B}, _W \mathbf{v},{ }_{B} \mathbf{b}^{a},{ }_{B} \mathbf{b}^{g},{ }_{B} \mathbf{T}_{L}\right\}_{j}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(_W{\mathbf{T}}_{B}\)</span>是IMU$到世界坐标系的变换</li><li><span class="math inline">\(_Wv\)</span>是线速度</li><li><span class="math inline">\(_Bb^a,_Bb^g\)</span>是IMU的加速度计bias和陀螺仪bias</li><li><span class="math inline">\(_BT_L\)</span>是激光雷达到IMU的变换</li></ul><p>遵循[32]，记<span class="math inline">\(\mathcal{K}_{k}:=\{k-m+1,\dots,k\}\)</span>为滑动窗口中的m个时间步，并记<span class="math inline">\(\mathcal{X}_{k}:={\mathbf{x}_j}_{j\in\mathcal{K}_k}\)</span>和<span class="math inline">\(\mathcal{Z}_k\)</span>分别记为滑动窗口中的状态和观测。</p><p>因子图优化旨在求解如下[32]函数：</p><p><span class="math display">\[    \mathcal{X}_k^{*}:= \arg \min_{\mathcal{X}_k}(-\log_c p(\mathcal{X}_k | \mathcal{Z}_k))\]</span></p><p>其中，<span class="math inline">\(p(\mathcal{X}_k | \mathcal{Z}_k)\)</span>是一个后验概率分布。</p><p>我们使用GTSAM[29]作为后端，并使用iSAM2[28]求解。</p><h2 id="可观测性度量">可观测性度量</h2><p>在地下场景，确定场景的几何特性是否有利于求解平移方向的位移是至关重要的。遵循[23,34]，假设旋转是小量的，那么<code>Point-to-Plane</code>ICP的cost的Hessian使用<span class="math inline">\(2A\)</span>来近似，其中，</p><p><span class="math display">\[\begin{aligned}    A:=\sum_{i=1}^{M} H_i^TH_i:=    \begin{bmatrix}    A_{rr} &amp; A_{rt} \\    A_{rt}^{T} &amp; A_{tt}    \end{bmatrix}\end{aligned}\]</span></p><p>且有</p><p><span class="math display">\[    H_i:=[-(p_i \times n_i)^{\mathbf{T}},-n_i^{\mathbf{T}}]\]</span></p><p>其中，<span class="math inline">\(n_i\)</span>是与点<span class="math inline">\(p_i\)</span>对应的平面单位法向量。</p><p>因此，矩阵<span class="math inline">\(A\)</span>的最小特征值即为最小可观测性的方向，平移部分通常是位姿估计具有挑战性的部分，主要是环境中可能存在长廊等场景。</p><p>因此，我们使用条件<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right):=\frac{\left|\lambda_{\max }\left(\boldsymbol{A}_{t t}\right)\right|}{\left|\lambda_{\min }\left(\boldsymbol{A}_{t t}\right)\right|}\)</span>作为可观测性的度量。</p><p>当<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>越大，优化问题中在平移部分的约束越差，当高于阈值时，<code>LION</code>向切换逻辑的<code>HeRO</code>[4]模块发出警告，以便可以使用其他更加准确的里程计来源。</p><h1 id="实验">实验</h1><h2 id="隧道竞赛">2019隧道竞赛</h2><p>我们首先在美国匹兹堡的Niosh实验矿山举行的两种不同赛道中的两次不同赛道中的两次不同赛道的两次不同。</p><p>激光里程计计算频率为10Hz，其中IMU和<code>LION</code>输出可以高达20Hz。<code>LION</code>使用的滑动窗口保持在3s内，并且后端被调整为使用<code>i7 Intel NUC</code>核心的30%。</p><p>对于参考，我们将其性能与：轮速惯性里程计（通过扩展卡尔曼滤波器融合的轮速惯性里程计）、scan-to-scan的里程计（<code>LION</code>的相对姿势输入）进行比较，并使用<code>LAMP</code>[35]作为ground-truth。</p><p>结果总结在表1中。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611102040.png"></p><p>我们可以看到，融合惯性数据与前端的里程计中显着降低了纯激光雷达里程计方法的漂移（scan-to-scan）。这从沿图4所示的z轴的z轴尤其明显，另外，<code>LION</code>可靠的估计了机器人的姿态（如图5所示），实现了小的roll和pitch误差</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611101500.png"></p><blockquote><p>没看出来.</p></blockquote><p>为了展示<code>LION</code>的自动校准能力，我们在模拟中生成了一个数据集，其中LIDAR沿IMU的Y轴相对于IMU转换为0.1米。 在图7中，我们观察到大约20秒后，<code>LION</code>（连续线）估计正确的外参（虚线）</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611101847.png"> &gt; 精度不咋地。</p><h2 id="可观测性模块">可观测性模块</h2><p>我们首先展示<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>如何检测几何上的无约束场景。 要测试这一点并建立直觉，我们首先使用<code>JPL-Corlidor</code>数据集，在JPL的办公室录制，其中主要的挑战是该环境沿长廊方向缺少几何特征。</p><p>此外，我们也使用了<code>Arch-Coal-Mine</code>数据集进行测试，它由一条直隧道和一个交叉路口组成。</p><p>将<code>JPL-Corlidor</code>数据集中的<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>以及矩阵特征值进行可视化，如图8所示：</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611102825.png"></p><blockquote><p>某个方向特征值越小，轴越短，不确定性越小，<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>越大，约束越少。</p></blockquote><p>在走廊的开始和末尾，所有方向都有足够的特征，条件号是<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>≈2。但是，在走廊的中间，条件号达到值<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>&gt; 13，同时特征向量沿着隧道的方向相关的特征值很小</p><p>将运行<code>Arch-Coal-Mine</code>数据集时的<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>以及矩阵特征值进行可视化，如图9所示：</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/11/20210611103243.png"></p><p>使用此<span class="math inline">\(\kappa\left(\boldsymbol{A}_{t t}\right)\)</span>作为可观察性度量标准，<code>HeRO</code>可以决定在没有足够的LIDAR功能时切换到不同于<code>LION</code>（如WIO）的其他里程计源。这种行为如图10所示。对于在办公室的环境中进行的真实实验，其中第一走廊的一部分没有足够的激光雷达特征。</p><p>如果未使用可观察性模块，则走廊中的这种特征缺乏会造成<code>lidar slip</code>，即无法观测到机器人的运动，这产生了9m的误差（机器人回到原点）。</p><p>当使用可观测性模块后，这种特征被检测到，将会切换到WIO模块来暂时代替<code>LION</code>，减少误差，此时误差为1m。</p><h1 id="总结">总结</h1><p><code>Local state estimator</code>: <code>LION</code>的主要目标是提供高速、连续、平滑的输出给下游算法。因此，<code>LION</code>并不构建任何地图，也不执行回环检测，因此其参考坐标系会逐渐偏移。这个偏移会由<code>LAMP</code>[35]中进行补偿。</p><p><code>Loosely-coupled architecture</code>： 与其他先进工作相比[16-18]，<code>LION</code>的前后端是松耦合的（即估计的状态x不包含特征点或激光扫描），目的是与建图模块[35]共享计算资源。此外，这种架构通过在选择前端/后端算法中的模块化时决定，并在单个估计引擎的情况下分配几个估计引擎之间的风险以消除单点故障。</p><p><code>Not feature-based</code>： 不同于[18],<code>LION</code>并没有使用特征点进行匹配，原因有两个，以是特征提取耗费计算资源，这将会降低板载资源的运行性能，另外，<code>LION</code>旨在探索完全未知的环境，在那里可以有人类制造的结构（充满了角点，平幔和线条）或完全非结构化的地形。这种不确定环境的特征提取的使用造成了在环境中的先验，因此引起风险或故障。</p><p><code>Automatic extrinsic calibration</code>：激光雷达与IMU的标定非常重要，特别是旋转方面，由于小误差可以快速地积累并导致大漂移。离线标定的方法通常需要校准目标[36]或特定运动序列[37]。</p><p><code>Supervisory Algorithm</code>：<code>LION</code>被设计成由<code>HeRO</code>[4]调用的多种里程计源之一，并且根据可观测性度量进行自主切换。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;lion-lidar-inertial-observability-aware-navigator-for-vision-denied-environments&quot;&gt;LION: Lidar-Inertial Observability-Aware Navigator
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>UPSLAM论文阅读</title>
    <link href="http://yoursite.com/2021/06/08/UPSLAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/06/08/UPSLAM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-06-08T02:03:53.000Z</published>
    <updated>2021-06-09T03:00:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="upslam-union-of-panoramas-slam">UPSLAM: Union of Panoramas SLAM</h1><p><img src="http://s1.nsloop.com:59080/images/2021/06/08/20210608101046.png"></p><blockquote><p>此文表达，晦涩难懂</p></blockquote><h1 id="摘要">摘要</h1><p>提出一种基于全景深度图的建图系统，全景图像可有效捕获由旋转激光雷达获取的范围测量，在大约数百万立方米的空间地图中记录厘米级别的细节。通过手持传感器收集的数据并同时运行建图程序来证明系统的灵活性，在<code>NVIDIA-Jestion-AGX-Xavier</code>系统中，在线的3D建图更新时间小于10毫秒。</p><h1 id="介绍">介绍</h1><p>SLAM中的挑战：</p><ul><li>运动：运动是否快速？是地面行驶还是空中飞行器？地面是否平坦？</li><li>几何：目标环境是否有足够的特征（平面、角点、边缘）？</li><li>性能：准确性、计算效率、环境规模扩展性</li></ul><p>本文提出一种尽可能多的利用传感器数据的方法，并通过以下实验来验证提出方法的鲁棒性，其中包括：行走、跑步、小型轮式机器人、腿式机器人平台、无人机平台，以及汽车从室内杂乱的环境中到地下隧道再到野外森林再到公路。</p><p>对实验结果进行量化分析，其轨迹误差仅为轨迹长度的0.05%，其在15W功率的平台上运行，轨迹长度为4.4公里。</p><p>在这项工作中，环境被代表为一系列全景深度图，这个选择允许我们使用二维数组的来表示我们的局部地图，我们的实施能够有效利用这种表示所提供的常规内存访问模式和并行性，结果表明，即使在嵌入式GPU平台上也运行很快。</p><p><strong>另外一个重要的结论是</strong>，我们能够利用所有可用的激光测量数据，并且避免了早期关于特征的决定，或者说，利用所有数据提供了额外的鲁棒性，使得提出的方法可以适用于各种环境。</p><h1 id="具体方法">具体方法</h1><p><code>UPSLAM</code>的地图是一个<code>graph</code>，其中，该图以平面法向量估计增强的全景深度图作为节点，并以这些深度图关键帧之间的相对位姿关系作为边。</p><p>关键帧可能具有不同的分辨率和扩展区，而不是原始传感器数据，我们通常使用比传感器捕获更宽的关键帧垂直视野进行操作。随着每个帧的LIDAR扫描更新，系统会跟踪当前的关键帧。这里描述的实验使用<code>Ouster os1-64</code>进行，33<span class="math inline">\(^\circ\)</span>垂直视场角，10Hz。另外，由IMU提供100Hz的加速度、角速度信息。</p><h2 id="数据输入">数据输入</h2><p>在快速运动期间，LIDAR扫描的100ms周期可包括显着的传感器运动，因此首先在CPU上进行运动畸变矫正。</p><p>接下来的所有步骤都在GPU上进行。</p><p>经过矫正后的点将投影到一个全景图像中，图像结构有助于计算一个大致的平面法向量估计，即利用某个点与其他两个最近点的叉乘来计算得到。这些表面法向量估计再次使用图像来表示，但是噪声非常大，可以使用如<code>à trous wavelet filter</code>滤波器等边缘敏感的平滑算法进行有效地平滑[12]。</p><h2 id="运动估计">运动估计</h2><p>对于每一帧新来的深度图和平面法向量估计，都使用基于点-面距离误差衡量的ICP进行配准[13,14]，如下式所示：</p><p><span class="math display">\[\mathbf{E}(\mathrm{T})=\sum_{\left(s_{1}, s_{2}\right) \in C \wedge \Omega_{d, \theta}\left(s_{1}, s_{2}\right)}\left|\left(\mathrm{T} s_{2}-s_{1}\right) \cdot N\left(s_{1}\right)\right|\]</span></p><p>其中，我们定义一个位姿变换<span class="math inline">\(T\)</span>，旨在最小化投影点对<span class="math inline">\(C\)</span>之间的距离。另外，还使用了一个相似性滤波器，用于拒绝距离较大、平面法向量夹角相差较大的关联点对。这个方法很容易在GPU上实现[15]。</p><p>扫描的表面正常估计可能略微通过<code>à trous wavelet filter</code>滤器平滑，但这种平滑有助于优化收敛，特别的，每次扫描都会更新关键帧表面法线估计，从而恢复因平滑而丢失的一些细节。</p><p>在建图系统的鲁棒性方面，一个重要因素是有多少个点参与到优化中。图2显示了典型实验的有效点的数量，其中有效点的定义为：从激光雷达有效测距范围内的点且点的强度值大于给定阈值。在图中所示的示例中，传感器在隧道中的墙壁附近开始，但是一旦出现进入开放环境时，在10Hz扫描速率下达到了40,000~50,000个点。</p><h2 id="关键帧更新">关键帧更新</h2><p>优化的变换矩阵为最后的投影关联集合提供了基础，其中，每个关键帧像素都被投影到扫描的深度图上。使用相似性过滤器来确定新的扫描数据是否应该对关键帧像素的进行平均。</p><p>在更新关键帧时使用加权平均值：每个关键帧像素跟踪已结合到其中的样本数量，当大于10时，将被用于确定加权权重。</p><p>移动物体可以在单个关键帧中应对平滑和响应性的平衡，但考虑多个关键帧重建3D占用率的潜在多假设模型，这是一种从地图中过滤移动对象的有效工具。当切换关键帧时，执行关键帧之间的相互一致性检查，以减少移动对象的影响。</p><p>更新后，将决定是否应该继续使用当前关键帧。如果配准的得分定义为ICP配准到深度图上的内点比例低于阈值，则会寻找新的关键帧。并且，会根据pose graph来对最近的节点进行闭环检测。</p><p>由于在显着漂移的影响下可能发生闭环配准，因此使用低分辨率的投影配准的方法对数百个网格搜索，得到候选闭环。然后使用full-ICP来进行配准，如果配准的质量高于创建一个新的关键帧的阈值，那么将会增加一条边，并且该闭环候选作为新的关键帧。除此以外，如果内点比例大于75%，将会使用full-ICP的配准结果对pose graph进行更新（反向传播更新），我们将这些分为strong和weak的闭环：这两个闭环都提高了地图的全局准确性，但是强闭合还会减少关键帧的数量。</p><p>Pose graph使用了GTSAM，CPU上运行，另外，使用了基于IMU的互补滤波器，用于估计重力向量并对每一个关键帧添加约束到GTSAM，这种观测信息将有助于减少pitch方向的误差。</p><h1 id="自带数据集实验">自带数据集实验</h1><h2 id="校园室内">校园——室内</h2><p>mapping的第一次测试是在实验室空间散步145米，以1-2米/秒的速度移动。 使用50个关键帧图像捕获80个MIB在磁盘上捕获图3所示的地图。 虽然这里的运动是良性的，但是图底部围绕着角截面的玻璃，以及几个狭窄的通道，由于传感器无法可靠地检测玻璃或墙壁近50厘米的玻璃或墙壁，提供了一些挑战</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609093332.png"></p><h2 id="校园室外">校园——室外</h2><p>手持传感器，沿着375米的循环沿大学校园内的建筑物群，在2-3米/秒。 所得到的点云，如图4所示，当传感器在其在图4A的下部附近的树木附近的起始位置约为10米的单环闭合时，从单环闭合所形成的。 选择运行的步伐，以强调由于摇动和弹跳而具有高幅度旋转的非平面运动</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609093611.png"></p><h2 id="农场">农场</h2><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609094032.png"> <img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609094057.png"></p><h2 id="地下">地下</h2><p>Robot沿着220米的轨迹在一座古老的煤矿中散步，采集了如图6中所示的底图，共有44个全景图像。该实验提供了与前面描述的运动模型的实际不同的运动模型，因为矿井的湿法砾石地板意味着机器人必须不断地工作以保持其平衡。</p><p>滴水，雾和灰尘对单独的激光雷达扫描有很大的噪音 <img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609094248.png"></p><h2 id="空中">空中</h2><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609094528.png"></p><h2 id="车载">车载</h2><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609094459.png"></p><p><strong>前面几个实验的参数</strong></p><table><thead><tr class="header"><th>参数</th><th>值</th></tr></thead><tbody><tr class="odd"><td>激光雷达</td><td>Ouster OS1-64</td></tr><tr class="even"><td>速度</td><td>3m/s</td></tr></tbody></table><p><strong>车载实验的参数</strong></p><table><thead><tr class="header"><th>参数</th><th>值</th></tr></thead><tbody><tr class="odd"><td>激光雷达</td><td>Ouster OS1-64</td></tr><tr class="even"><td>速度</td><td>13.8m/s</td></tr><tr class="odd"><td>关键帧数量</td><td>1045</td></tr><tr class="even"><td>图片占用</td><td>1220MiB</td></tr><tr class="odd"><td>生成地图</td><td>20GiB</td></tr></tbody></table><h1 id="公开数据集实验">公开数据集实验</h1><p>数据集使用了<code>Newer College Dataset</code>[18]</p><h2 id="轨迹精度">轨迹精度</h2><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609095605.png"></p><p>ATE: 1.4km， 0.77m， 0.05%</p><h2 id="算法效率">算法效率</h2><p>使用平台：NVIDIA Jetson AGX Xavier</p><p>考虑高分辨率，2048x256和低分辨率，1024x128的关键帧</p><p>在图13中的每种配置中显示了将新LIDAR扫描到地图中所花费的时间。与Xavier的最大功率预算和高分辨率关键帧采取的中位时间为6毫秒</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/09/20210609100109.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;upslam-union-of-panoramas-slam&quot;&gt;UPSLAM: Union of Panoramas SLAM&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com:59080/images/2021/06/08/20210
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>Cross_view_slam论文阅读</title>
    <link href="http://yoursite.com/2021/06/05/Cross_view_slam%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2021/06/05/Cross_view_slam%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2021-06-05T10:03:53.000Z</published>
    <updated>2021-06-07T11:16:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="any-way-you-look-at-it-semantic-crossview-localization-and-mapping-with-lidar">Any Way You Look at It: Semantic Crossview Localization and Mapping With LiDAR</h1><p><img src="http://s1.nsloop.com:59080/images/2021/06/05/20210605180434.png"></p><h1 id="摘要">摘要</h1><p>GPS是迄今为止最受欢迎的全球本地化方法。 但是，在所有环境中，它并不总是可靠或准确。 SLAM方法使能局部估计能够提供将本地地图注册到全局的局部估计，这对于机器人间协作或人类互动可能是重要的。</p><p>在这项工作中，提出了一种利用语义的实时方法，仅使用以自我为中心的 3D 语义标记的 LiDAR 和 IMU 以及从卫星或空中机器人获得的自上而下的 RGB 图像来全局定位机器人。</p><h1 id="介绍">介绍</h1><p>提出一种将地面机器人的局部观测信息与来自卫星或无人机的大范围全局地图进行匹配的定位方法，这种方法比传统的基于已知地图定位方法更加具有挑战性，因为对于大多数机器人而言，<code>the overhead views</code> 与 以自我为中心的观测数据有较大不同。因此，在鸟瞰图中直接应用基于特征的配准方法对机器人进行定位通常是无效的。此外，卫星图通常在不同的时间拍摄，这意味着配准方法必须适应季节的变化。</p><p>近年来，使用人工神经网络（ANNS）的图像语义分割已成为成熟的技术。地图语义对于<code>cross-view registration</code>是理想的，因为在足够多样化的训练数据中，可提取具有视角和季节不变性的语义信息。此外，从鸟瞰图中提取粗略的语义信息比从<code>the overhead views</code> 中提取更加简单，无论是手工提取还是基于ANN的方法。</p><p>与此同时，Lidars在光束密度增加同事，价格和重量下降，因此将它们放在小型移动机器人上现在是实用的。同时应用两种传感技术将可以产生稠密的语义点云。</p><p>最近的工作使用了用于<code>cross-view</code>定位的图像语义信息[15]，[16]，但通常没有只有很少的利用了深度信息或环境的有力的结构假设。如基于几何方法如 [3] 通常在没有足够几何结构的环境中失效，相反，我们引入了一种结合这两种信息来源的方法，以实现更强大的语义<code>cross-view</code>定位系统。</p><p>主要贡献如下：</p><ul><li>提出一种实时的<code>cross-view</code>定位和建图框架，如果鸟瞰地图未知但有界，我们的方法也能够估计它的比例。</li><li>我们使用Semantickitti [17]以及我们自己的数据集以及在包括农村和城市环境的各个地点，验证我们所提出的方法。</li><li><a href="https://github.com/iandouglas96/cross_view_slam" target="_blank" rel="noopener">开源了代码</a></li></ul><h1 id="相关工作">相关工作</h1><h2 id="图像匹配">图像匹配</h2><p><code>Cross-view localization</code>问题定义为图像匹配问题[12]：给定一个由图片构成的数据库作为全局地图，以及一张待查询的图片作为局部观测。问题可描述为该问题通过获取一些描述符，使得来自多个视图的相同位置的图像在一些潜在的空间中接近。</p><p>早期的工作如[18]，[19]，使用局部特征描述符。 Majdik等[11]使用来自Google Street View的图像特征来与四旋翼无人机飞行拍摄图来进行匹配。然而，这些方法往往在极端视角的情况下失效。</p><p>这个限制导致了最近兴起的使用孪生神经网络[20], [21]方向，这些网络通过对不同视点的图像使用权重共享的分支网络结构在潜在空间进行交叉关联，来决定两个图像的相似度。整个网络在已知的图像对上训练，在线处理图像对时，这些网络相对较慢，因此不能在机器人应用程序中实时运行，其中必须同时查询许多可能的状态。</p><h2 id="基于视觉">基于视觉</h2><p>图像匹配算法提供了一种将图像与提供的现有图像数据库进行比较的方法，但它们并未明确寻求估计机器人或传感器的位置。定位需要具有<code>pose label</code>或整个航拍地图的图像数据库，如[22]中的描述。在这项工作中，作者使用鸟瞰坐标系中的一系列边缘来代表全局地图，然后在粒子滤波框架中与地面图像进行边缘匹配。但是，通过将全局地图减少为一系列边缘，将会丢弃大量的有用信息。</p><p>其他工作如[23]，使用立体图像来生成RGBD图像，然后将它从鸟瞰角度进行渲染生成，然后使用<code>chamfer matching</code>与已知地图进行匹配，尽管如此，这种方法未能解决由季节变化或环境中的动态物体（如人或汽车）等因素引起的摄影变化。</p><p>为了更好地应对季节性变化和更加极端的观点变化，最近的工作越来越多地分析了语义的定位使用。Castaldo等 [15]对地面图像分割，并使用<code>homography</code>和地平面将语义特征投影到自上而下的视角。然后开发出一种针对分割图像的语义描述符，并与已知地图进行比较，以在相机位置集合生成对应的热图(heatmap)。然而，改方法并没有利用时间或深度信息，导致大规模定位系统的收敛速度较慢。另外，他们的方法在<code>homography</code>不成立的情况下失效，如路面并非平坦的工况。</p><p>类似的工作还有Gawel et al. [16]，为空中图和地面视图生成语意图表示，然后构建描述符以匹配各种合成数据集。</p><h2 id="基于激光雷达">基于激光雷达</h2><p>Wolcott等人 [24]通过从各种透视图渲染一个已知的高分辨率点云并最大化互相关信息来对相机进行定位。Gawel等人 [3]通过从两个角度构建点云地图并使用几何描述符匹配它们来交叉定位地面和空中机器人。Barsan等人 [25]使用孪生网络实现厘米级定位。虽然上述方法准确和强大，这些方法需要相关环境的预先存在点云图，而我们只需要单个航拍图像。</p><p>与基于视觉的方法一样，最先进的基于Lidar的方法利用环境的语义结构。在早期的工作中，Matei等人[26] 从无人机采集的点云来拟合一个最接近的建筑模型，用于创建与地面图像相比的地面预测。</p><p>……</p><p>与这些工作相比，我们的框架只需要一个卫星或环境的鸟瞰图，这适合更多的应用。</p><h1 id="具体方法">具体方法</h1><p>我们的方法由两个主要组件组成：基于ICP的LIDAR SLAM系统 - Panoramas Slam [32]（Upslam） - 以及基于粒子滤波器的语义定位器，如图2所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/06/20210606185558.png"></p><h2 id="定位">定位</h2><p>粒子滤波器特别擅长处理多模态分布，这在机器人定位问题中经常出现，这将带来计算量的问题，我们通过使用优化策略来减少计算成本：</p><h3 id="问题描述">问题描述</h3><p>对于2D地图的定位，我们系统状态<span class="math inline">\(x\)</span>由<code>tuple</code>(<span class="math inline">\(p\in SE(2) , s \in [S_{min},S_{max}]\)</span>)构成，其中，<span class="math inline">\(p\)</span>表示机器人位姿，<span class="math inline">\(s\)</span>表示地图尺度(px/m)。我们还有控制输入量<span class="math inline">\(u\in SE(3)\)</span>，从上一帧到当前帧的相对位姿变换，该值来源于<code>UPSLAM</code>或者其他里程计。</p><p>因此，对于每个时间步t，都有一个运动模型<span class="math inline">\(P(x_t|x_{t-1},u_{t-1})\)</span>。此外，在每个时刻t，我们都有来自LiDAR和cameras的语义扫描<span class="math inline">\(z_t\)</span>，为了定义我们的粒子滤波器，我们必须先定义观测模型<span class="math inline">\(P(z_{t}|x_{t})\)</span></p><h3 id="运动模型">运动模型</h3><p>实际上，我们并不能获取真正的控制输入量，而是从frame-to-frame的估计中获取一个近似。</p><p>因为<code>UPSLAM</code>在3D空间中操作，因此我们首先将帧间运动投影到local的x-y地平面上，记为<span class="math inline">\(\operatorname{proj}(u) \in SE(2)\)</span>。此外，我们只使用<code>UPSLAM</code>的基于ICP的初始解作为运动估计，忽略了来自闭环后的优化位姿（这是为了避免里程的运动不连续性）。</p><p>最后，我们假设分布具有恒定的协方差，在<code>log-space</code>中具有尺度噪声，因此，有：</p><p><span class="math display">\[\begin{aligned}P\left(x_{t} \mid x_{t-1}, u_{t-1}\right)=(&amp;\left[\operatorname{proj}\left(u_{t-1}\right)+\mathcal{N}\left(0, \Sigma_{p}\right)\right] * p_{t-1},\left.\mathcal{N}\left(1, \Sigma_{s}\right) * s_{t-1}\right)\end{aligned}\]</span></p><p>另外，我们使用到起点位置的距离的inverse来对协方差<span class="math inline">\(\Sigma_s\)</span>进行缩放，以使得可以自然的收敛。一旦尺度方差低于阈值，我们则可固定尺度s。</p><h3 id="观测模型">观测模型</h3><p>我们的观测<span class="math inline">\(z\)</span>是语义的点云，我们可以表示为在机器人坐标系中具有相关性的标签：<span class="math inline">\(z=\{ (p_1,l_1),(p_2,l_2),\dots,(p_n,l_n) \}\)</span>。因此，我们可以将这些点投影到地平面上。</p><p>进一步的，对于任意给定的粒子状态，我们可以对机器人坐标系中的点在俯视的空中地图<span class="math inline">\(L\)</span>中查询，比较地图上的类别与预期类别，在给定粒子状态(位姿为<span class="math inline">\(d\)</span>)的条件下，一个简单的计算cost方法如下：</p><p><span class="math display">\[C^{\prime}=\sum_{i \in[1, n]} \mathbf{1}\left(L\left(d * p_{i}\right) \neq l_{i}\right)\]</span></p><p>为了通过扩大局部最小来提高收敛性，我们选择一个更柔软的成本函数。因此，我们不使用以二进制方式评估cost，而是以机器人坐标系中的点对应的类别为目标类别，在空中地图中查询相同类别的最近点，以两点距离作为cost：</p><p><span class="math display">\[C=\sum_{i \in[1, n]} \min _{\left\{p \mid L(p)=l_{i}\right\}}\left(\left\|p-p_{i}\right\|\right)\]</span></p><p>最后，我们通过对<span class="math inline">\(C\)</span>求逆和正则化来计算一个<code>ad-hoc probability</code>，另外，对于每一个类别的点，可以设置一个权重因子<span class="math inline">\(\alpha_l\)</span>：</p><p><span class="math display">\[P\left(z_{t} \mid x_{t}\right) \approx \frac{n}{\sum_{i \in[1, n]} \min _{\left\{p \mid L(p)=l_{i}\right\}}\left(\alpha_{l_{i}}\left\|p-p_{i}\right\|\right)}+\gamma\]</span></p><p>其中，<span class="math inline">\(\gamma\)</span>是正则化常数项，用于slow convergence??? 所有粒子的概率最后都要进行归一化，以使得所有粒子对应的观测模型概率之和为1。</p><p>可以注意到，这是一个<code>ad-hoc</code>观测，在实践中可以通过实验对常数进行调整，然而，这对于基于蒙特卡罗的定位方法并不少见，例如 [29]。</p><h3 id="性能优化">性能优化</h3><p>如果单纯的实现<span class="math inline">\(C^{\prime}\)</span>的cost计算，将会导致较大计算成本，因为它总结了所有的非凸最小值的情况。我们通过预先计算空中语义地图关于类别的截断场(Truncated Distance Field, TDF)来优化这个问题，这个计算非常直观，大概需要1分钟，但是只需要进行一次。</p><p>这个TDF地图对每一个点到其同类别的最近点的距离阈值进行编码，将最小化<span class="math inline">\(C^{\prime}\)</span>变成了一个简单的查表。另外，相比于简单的对所有点的结果进行求和，使用了如下方法优化：首先将语义激光扫描分散到极坐标系中，统计在每个极坐标系分割中每个类别的点数。然后局部的类别TDF场使用同种方式渲染显示。然后通过对两个图像的元素进行乘法，得到关于每个类别的内积，来近似<span class="math inline">\(C^{\prime}\)</span>的计算，提高计算效率。</p><p>我们发现，对于<code>100x25</code>像素的极坐标地图，每个粒子大概需要<code>500μs</code>来计算，几乎是用于查表的时间，如图3所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/07/20210607101819.png"></p><p>另外，地图的旋转在极坐标中表示为索引的偏移，计算非常快，可用于初始化阶段。我们随机采样了地图上的道路点，因为我们有强壮的先验信息：因为是从道路上开始的。对于每一个点，我们初始化<span class="math inline">\(k_s\)</span>个在<span class="math inline">\(s_{min}\)</span>和<span class="math inline">\(s_{max}\)</span>(1~10 px/m)尺度内均匀分布的粒子。对于每一个粒子，我们采样<span class="math inline">\(k_{\theta}\)</span>个可能的观测，使得我们可以高效的对索引进行偏移（可参考scan-context），然后选取最佳的观测作为初始粒子。</p><p>为了进一步加速算法，我们使用CPU并行计算每个粒子的cost，我们还基于高斯混合模型（GMM）的协方差的区域的总和适应粒子分布的基础上的粒子数</p><h2 id="语义分割">语义分割</h2><h3 id="卫星分割">卫星分割</h3><p>我们使用在 ImageNet 上预训练的 ResNet-34 [34] 骨干训练两个稍微修改过的完全卷积网络 (FCN) [33] 版本以分割卫星图像，图像直接从<code>Google-Earth</code>中获取。</p><p>我们使用了4个类别：</p><ul><li>road（道路）</li><li>terrain（地面）</li><li>vegetation（植被）</li><li>building（建筑）</li></ul><p>为了分析卫星图像，256×256 PX RGB图像被传递为图像分割网络的输入，该网络由3个手工标注的卫星图像数据训练得到，如表1所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/07/20210607153338.png"></p><p>图像是随机缩放，旋转，裁剪的，并翻转以产生更多的训练样本，卫星图像的随机缩放也允许模型在从多个高度收集的图像上概率更好地泛化。</p><p>网络输出例子和训练数据如图5所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/07/20210607153818.png"></p><p>令人惊讶的是，尽管模型只在3张图片上训练，但是每张图片都包含了许多对象实例</p><h3 id="激光扫描分割">激光扫描分割</h3><p>我们使用图 2 中绿色方框中显示的两个不同的pipline，用于根据数据集生成语义点云。通过使用不同的分割方法进行测试，展示了我们的方法可以适用于不同的传感系统，提供点级别的标签点云。</p><p><strong>PC</strong> 对于kitti数据集，使用了与卫星图像分割同样的FCN结构，对于激光扫描，使用带有X,Y,Z,Depth通道的64x2048的<code>2D-Poloar-Grid-Map</code>来表示(没有利用强度信息)。</p><p>我们在<code>SemanticKITTI</code>数据集上训练，然后使用序列{10}和{00,02,09}作为验证集和测试集。另外，我们添加地面车辆转化为road类别。</p><p><strong>RGB</strong></p><p>对于我们自己的Morgantown和Ucity数据集，我们使用不同于Kitti的激光雷达（Ouster OS-1），因此不能使用Semantickitti进行训练。因此，我们使用HRNets[13]来对RGB图像进行分割，然后校准到激光扫描中。利用外参信息，可以将激光点云投影到相机图像帧，然后根据RGB的分割对点云进行分配。</p><h2 id="建图">建图</h2><p>我们使用<code>UPSLAM</code>，但此处给出了在本工作中的一些更改的概况。对于这项工作，我们扩展了<code>UPSLAM</code>，整合从图像中提取的语义标签，以形成语义全景。需要注意的是，<code>UPSLAM</code>并没有使用语义信息来进行<code>scan-matching</code>，只是简单的使用刚体变换信息来整合语义信息。因此，由于我们不需要每一帧扫描的语义数据，我们可以以低于LIDAR的速率来运行推断，而无需删除ICP数据，以提高地图质量。深度、法向量、语义全景如图4所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/07/20210607164532.png"></p><p>除了使用<code>UPSLAM</code>估计的粒子过滤器运动模型之外，我们还计算每个更新的后粒子滤波器估计的协方差和均值。一旦协方差低于阈值<span class="math inline">\(\Sigma_t\)</span>，我们使用粒子滤波器估计的位置作为位子图中对应状态节点的先验，最终得到如图2所示的pose graph。</p><p><img src="http://s1.nsloop.com:59080/images/2021/06/06/20210606185558.png"></p><p>通过这样的方式，我们使得图优化结果与地理保持对齐，我们的实验表明，添加这些语义约束边可去除漂移，并有效地构建语义闭环约束。这使得建图器在没有闭环的情况下可以应对更大规模的轨迹，并保持全局一致性。</p><h1 id="实验">实验</h1><p><img src="http://s1.nsloop.com:59080/images/2021/06/07/20210607185914.png"></p><h1 id="参考文献">参考文献</h1><p>[1] W.Maetal.,“Findyourwaybyobservingthesunandothersemanticcues,”in Proc. IEEE Int. Conf. Robot. Automat., May 2017, pp. 6292–6299. [2] T. Dang et al., “Autonomous search for underground mine rescue using aerial robots,” in Proc. IEEE Aerosp. Conf., 2020, pp. 1–8. [3] A. Gawel et al., “3D registration of aerial and ground robots for disaster response: An evaluation of features, descriptors, and transformation esti- mation,” in Proc. IEEE Int. Symp. Saf., Secur. Rescue Robot., Oct. 2017, pp. 27–34. [4] J. Peterson et al., “Online aerial terrain mapping for ground robot naviga- tion,” Sensors, vol. 18, no. 2, Feb. 2018, Art no. 630. [5] N. Michael et al., “Collaborative mapping of an earthquake-damaged building via ground and aerial robots,” J. Field Robot., vol. 29, no. 5, pp. 832–841, 2012. [6] X. Liang, H. Wang, Y. Liu, W. Chen, and T. Liu, “Formation control of non- holonomic mobile robots without position and velocity measurements,”IEEE Trans. Robot., vol. 34, no. 2, pp. 434–446, Apr. 2018. [7] A. Franchi, G. Oriolo, and P. Stegagno, “Mutual localization in multi- robot systems using anonymous relative measurements,” Int. J. Robot. Res., vol. 32, no. 11, pp. 1302–1322, 2013. [8] A. Howard, “Multi-robot simultaneous localization and mapping using particle filters,” Int. J. Robot. Res., vol. 25, no. 12, pp. 1243–1256, 2006. [9] S. Wang et al., “A novel approach for lidar-based robot localization in a scale-drifted map constructed using monocular slam,” Sensors, vol. 19, no. 10, 2019, Art. no. 2230. [10] F. Dellaert, D. Fox, W. Burgard, and S. Thrun, “Monte Carlo localization for mobile robots,” in Proc. IEEE Int. Conf. Robot. Automat., vol. 2, 1999, pp. 1322–1328 vol.2. [11] A. L. Majdik, Y. Albers-Schoenberg, and D. Scaramuzza, “MAV urban localization from google street view data,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., Nov. 2013, pp. 3979–3986. [12] X. Gao, S. Shen, Z. Hu, and Z. Wang, “Ground and aerial meta-data integration for localization and reconstruction: A review,” Adv. Visual Corresp.: Models, Algorithms Appl., Pattern Recognit. Lett. vol. 127, pp. 202–214, 2019. [13] J. Wang et al., “Deep high-resolution representation learning for visual recognition,” IEEE Trans. Pattern Anal. Mach. Intell., early access: Apr. 01, 2020, doi: 10.1109/TPAMI.2020.2983686. [14] M. Wu, C. Zhang, J. Liu, L. Zhou, and X. Li, “Towards accurate high resolution satellite image semantic segmentation,” IEEE Access, vol. 7, pp. 55 609–55 619, 2019. [15] F. Castaldo, A. Zamir, R. Angst, F. Palmieri, and S. Savarese, “Semantic cross-view matching,” in Proc. IEEE Int. Conf. Comput. Vis. Workshops, Dec. 2015, pp. 9–17. [16] A. Gawel, C. D. Don, R. Siegwart, J. Nieto, and C. Cadena, “X-view: Graph-based semantic multi-view localization,” IEEE Robot. Automat. Lett., vol. 3, no. 3, pp. 1687–1694, Jul. 2018. [17] J. Behley et al., “SemanticKITTI: A dataset for semantic scene under- standing of LiDAR sequences,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2019, pp. 9297–9307. [18] D.M. Chen et al., “City-scale landmark identification on mobile devices,”in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2011, pp. 737–744. [19] Y. Li, N. Snavely, and D. P. Huttenlocher, “Location recognition using prioritized feature matching,” Computer Vision – ECCV, K. Daniilidis, P. Maragos, and N. Paragios, eds. Springer Berlin Heidelberg, 2010, pp. 791–804. [20] D. Kim and M. R. Walter, “Satellite image-based localization via learned embeddings,” in Proc. IEEE Int. Conf. Robot. Automat., May 2017, pp. 2073–2080. [21] Y. Tian, X. Deng, Y. Zhu, and S. Newsam, “Cross-time and orientation- invariant overhead image geolocalization using deep local features,” in Proc. IEEE Winter Conf. Appl. Comput. Vis., 2020, pp. 2512–2520. [22] K. Y. K. Leung, C. M. Clark, and J. P. Huissoon, “Localization in urban environments by matching ground level video images with an aerial image,” in Proc. IEEE Int. Conf. Robot. Automat., May 2008, pp. 551–556. [23] T. Senlet and A. Elgammal, “A framework for global vehicle localization using stereo images and satellite and road maps,” in Proc. IEEE Int. Conf. Comput. Vis. Workshops, Nov. 2011, pp. 2034–2041. [24] R. W. Wolcott and R. M. Eustice, “Visual localization within lidar maps for automated urban driving,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., Sep. 2014, pp. 176–183. [25] I. A. Barsan, S. Wang, A. Pokrovsky, and R. Urtasun, “Learning to localize using a lidar intensity Map,” in Proc. 2nd Conf. Robot Learn., Proc. Mach. Learn. Res., A. A. Billard Dragan, J. Peters, and J. Morimoto, Eds. PMLR, 29-31, vol. 87, Oct. 2018, pp. 605–616. [26] B. C. Matei et al., “Image to LiDar matching for geotagging in urban environments,” in Proc. IEEE Workshop Appl. Comput. Vis., Jan. 2013, pp. 413–420. [27] T. Senlet, T. El-Gaaly, and A. Elgammal, “Hierarchical semantic hashing: Visual localization from buildings on maps,” in Proc. 22nd Int. Conf. Pattern Recognit., Aug. 2014, pp. 2990–2995. [28] Y. Tian, C. Chen, and M. Shah, “Cross-view image matching for geo- localization in urban environments,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jul. 2017, pp. 3608–3616. [29] F. Yan, O. Vysotska, and C. Stachniss, “Global localization on open- streetmap using 4-bit semantic descriptors,” in Proc. Eur. Conf. Mobile Robots, 2019, pp. 1–7. [30] E. Stenborg, C. Toft, and L. Hammarstrand, “Long-term visual localization using semantically segmented images,” in Proc. IEEE Int. Conf. Robot. Automat., 2018, pp. 6484–6490. [31] Y. Liu, Y. Petillot, D. Lane, and S. Wang, “Global localization with object- level semantics and topology,” in Proc. Int. Conf. Robot. Automat., 2019, pp. 4909–4915. [32] A. Cowley, I. D. Miller, and C. J. Taylor, “UPSLAM: Union of panoramas SLAM,” in Proc. Int. Conf. Robot. Automat., 2021. [33] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 3431–3440. [34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770–778. [35] M. Cordts et al., “The cityscapes dataset for semantic urban scene un- derstanding,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 3213–3223. [36] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? The KITTI vision benchmark suite,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 3354–3361. [37] S. S. Shivakumar, T. Nguyen, I. D. Miller, S. W. Chen, V. Kumar, and C. J. Taylor, “Dfusenet: Deep fusion of RGB and sparse depth information for image guided dense depth completion,” in Proc. IEEE Intell. Transp. Syst. Conf., 2019, pp. 13–20.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;any-way-you-look-at-it-semantic-crossview-localization-and-mapping-with-lidar&quot;&gt;Any Way You Look at It: Semantic Crossview Localizati
      
    
    </summary>
    
    
    
      <category term="SLAM" scheme="http://yoursite.com/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>Adjoints and Covariances（伴随与协方差）</title>
    <link href="http://yoursite.com/2021/05/09/SE3%E4%BC%B4%E9%9A%8F/"/>
    <id>http://yoursite.com/2021/05/09/SE3%E4%BC%B4%E9%9A%8F/</id>
    <published>2021-05-09T14:55:30.000Z</published>
    <updated>2021-05-27T02:27:22.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="adjoints">Adjoints</h1><p>我们将介绍李群伴随的概念，这将帮助我们将右边的增量或矫正值与左边的增量或校正值联系起来。这种性质使我们能够用代数方法处理李群定义的不确定性，并得到不同协方差变换的表达式。我们将重点讨论3D构造，即 SE (3)，因为它的广泛适用性，但类似的定义应该适用于其他李群，因为他们主要依赖于伴随的定义。</p><p><a href="http://ncfrn.cim.mcgill.ca/members/pubs/barfoot_tro14.pdf" target="_blank" rel="noopener">Barfoot 和 Furgale (2014年)</a>和 <a href="https://arxiv.org/abs/1906.07795" target="_blank" rel="noopener">Mangelson 等人(2020年)</a>的文献中已经出现了大多数这样的表达式，但由于它们遵循左手惯例，所以不能直接用于 GTSAM。我们为 Mangelson 等人之后的协方差变换提供了结果表达式，但是我们建议参考他们的工作来理解这个过程的细节。</p><p>让我们考虑一个例子，我们在一个姿态<span class="math inline">\(\mathbf{T}_{WB_i}\)</span>加入小增量<span class="math inline">\(_{B_i}\mathbf{\xi}\)</span>:</p><p><span class="math display">\[\begin{aligned}\mathbf{T}_{W_i B_{i}} \text{Exp}( _{B_i}\mathbf{\xi})\end{aligned}\]</span></p><p><img src="http://s1.nsloop.com:59080/images/2021/05/26/20210526224034.png"></p><blockquote><p>以上遵循了right-hand定则，与GTSAM一致</p></blockquote><p>然而，一些应用则使用left-hand的形式:</p><span class="math display">\[\begin{aligned}\mathbf{T}_{W_{i+1} B} = \text{Exp}( _{W_i}\mathbf{\xi}) \mathbf{T}_{W_i B_i}\end{aligned}\]</span><p>那么，对应的就是参考坐标系的增量变化：</p><p><img src="http://s1.nsloop.com:59080/images/2021/05/26/20210526224431.png"></p><p>实际上，两种表达的意义都是一样的：</p><p><img src="http://s1.nsloop.com:59080/images/2021/05/26/20210526225000.png"></p><p>即有如下等式(暂时忽略时间索引)：</p><p><span class="math display">\[\begin{aligned}\text{Exp}( _{W}\mathbf{\xi}) \mathbf{T}_{WB} = \mathbf{T}_{WB} \text{Exp}( _{B}\mathbf{\xi})\end{aligned}\]</span></p><p>进一步的，就可以根据body系的增量来求出world系的增量：</p><p><span class="math display">\[\begin{aligned}\text{Exp}( _{W}\mathbf{\xi}) = \mathbf{T}_{WB} \text{Exp}( _{B}\mathbf{\xi}) \mathbf{T}_{WB}^{-1}\end{aligned}\]</span></p><p><img src="http://s1.nsloop.com:59080/images/2021/05/26/20210526230129.png"></p><p>对于我们的目的，使用一个等价的替代表达式是有用的，它直接应用于切空间的元素(<a href="https://arxiv.org/abs/1812.01537" target="_blank" rel="noopener">solà</a>等人给出了一个更完整的推导，由于一些属性我们在这里省略了) :</p><p><span class="math display">\[\begin{aligned}\text{Exp}( _{W}\mathbf{\xi}) \mathbf{T}_{WB_i} = \mathbf{T}_{WB_i} \text{Exp}( \text{Ad}_{T_{WB_i}^{-1}}  {_{W}}\mathbf{\xi})\end{aligned}\]</span></p><p>其中，<span class="math inline">\(\text{Ad}_{T_{WB_i}^{-1}}\)</span>称为<span class="math inline">\(T_{WB_i}^{-1}\)</span>的伴随，伴随直接作用于切线空间的元素上，改变它们的参考坐标系，即：</p><p><span class="math display">\[{_{B}}\mathbf{\xi} = \text{Ad}_{T_{WB_i}^{-1}} {_{W}}\mathbf{\xi}\]</span></p><p>我们也可以把这解释为一种方法，将左边(在世界坐标系中)施加的增量一致地移动到右边(body坐标系) ，这对于保持右边惯例对于回溯和概率分布的一致性特别有用。</p><blockquote><p>这是我们用来定义一些协方差转换的主要属性，并且它已经在<code>GTSAM</code>的<code>Pose3</code>中实现为<code>AdjointMap</code>。</p></blockquote><h1 id="distribution-of-the-inverse">Distribution of the inverse</h1><p>考虑这样一种情况: 我们有一个因子图的解，其协方差定义在body坐标系中。我们感兴趣的是获得一个表示world坐标系的协方差表达式.</p><table><thead><tr class="header"><th style="text-align: center;"><img src="http://s1.nsloop.com:59080/images/2021/05/27/20210527100129.png"></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><em>给定body坐标系的协方差<span class="math inline">\(B_i\)</span></em>(左图)，然而我们感兴趣的是右图的世界坐标系的协方差<span class="math inline">\(W\)</span></td></tr></tbody></table><p>假设具有正态分布的位姿表达如下：</p><span class="math display">\[\begin{aligned}\mathbf{\tilde{T}}_{WB} = \mathbf{T}_{WB} \text{Exp}( _{B}\mathbf{\eta})\end{aligned}\]</span><p>其中，<span class="math inline">\(_{B}\mathbf{\eta}\)</span>是协方差为<span class="math inline">\(\Sigma_{B}\)</span>的零均值高斯分布噪声，因此，逆位姿的分布可以通过对位姿表达式求逆：</p><p><span class="math display">\[\begin{aligned}(\mathbf{\tilde{T}}_{WB})^{-1} &amp; = (\mathbf{T}_{WB} \text{Exp}( _{B}\mathbf{\eta}) )^{-1}\\&amp; = (\text{Exp}( _{B}\mathbf{\eta}) )^{-1}\ \mathbf{T}_{WB}^{-1}\\&amp; = \text{Exp}(- _{B}\mathbf{\eta}) \ \mathbf{T}_{WB}^{-1}\end{aligned}\]</span></p><p>然而，在求逆后，噪声项被定义在左边，并且仍然是body坐标系下的表达，接下来使用伴随来将这一项移到右边:</p><p>1.根据前面推导的:</p><p><span class="math display">\[\begin{aligned}\text{Exp}( _{W}\mathbf{\xi}) \mathbf{T}_{WB_i} = \mathbf{T}_{WB_i} \text{Exp}( \text{Ad}_{T_{WB_i}^{-1}}  {_{W}}\mathbf{\xi})\end{aligned}\]</span></p><p>2.应用于位姿的逆表达（等号右侧），则有：</p><p><span class="math display">\[\begin{aligned}\text{Exp}( -{_{B}}\mathbf{\eta}) \mathbf{T}_{WB}^{-1} = \mathbf{T}_{WB}^{-1} \text{Exp}( -\text{Ad}_{T_{WB}} {_{B}}\mathbf{\eta})\end{aligned}\]</span></p><p>3.因此，有：</p><p><span class="math display">\[\begin{aligned}(\mathbf{\tilde{T}}_{WB})^{-1} = \ \mathbf{T}_{WB}^{-1}\ \text{Exp}(- \text{Ad}_{\mathbf{T}_{WB}} {_{B}}\mathbf{\eta})\end{aligned}\]</span></p><p>最终，得到了符合right-hand的分布，其中定义了world坐标系的协方差，如下：</p><p><span class="math display">\[\begin{aligned}\Sigma_{W} = \text{Ad}_{\mathbf{T}_{WB}} \Sigma_B \text{Ad}_{\mathbf{T}_{WB}}^{T}\end{aligned}\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;adjoints&quot;&gt;Adjoints&lt;/h1&gt;
&lt;p&gt;我们将介绍李群伴随的概念，这将帮助我们将右边的增量或矫正值与左边的增量或校正值联系起来。这种性质使我们能够用代数方法处理李群定义的不确定性，并得到不同协方差变换的表达式。我们将重点讨论3D构造，即 SE (3)
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Incremental-Segment-Based Localization in 3-D Point Clouds</title>
    <link href="http://yoursite.com/2021/05/09/Incremental_Segmental_localization/"/>
    <id>http://yoursite.com/2021/05/09/Incremental_Segmental_localization/</id>
    <published>2021-05-09T14:55:30.000Z</published>
    <updated>2021-05-09T15:02:58.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="incremental-segment-based-localization-in-3-d-point-clouds">Incremental-Segment-Based Localization in 3-D Point Clouds</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;incremental-segment-based-localization-in-3-d-point-clouds&quot;&gt;Incremental-Segment-Based Localization in 3-D Point Clouds&lt;/h1&gt;

      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>LiDAR-Camera 标定-5</title>
    <link href="http://yoursite.com/2021/04/27/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_5/"/>
    <id>http://yoursite.com/2021/04/27/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_5/</id>
    <published>2021-04-26T20:55:30.000Z</published>
    <updated>2021-06-03T16:14:32.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="d激光雷达与全向相机外参标定">3D激光雷达与全向相机外参标定</h1><p><img src="http://s1.nsloop.com:59080/images/2021/04/26/20210426223058.png"></p><h1 id="摘要">摘要</h1><p>我们提出了一种使用全向摄像头系统对3D激光扫描仪进行外部校准的方法，该程序要求至少从3个视角同时从激光扫描仪和相机系统观察平面棋盘格图案，棋盘格的平面法向量和位于表面的3D点约束了激光扫描仪和全向相机系统的相对姿态，这些约束可用于形成外参标定的非线性优化问题，用于求解外参以及对应的协方差。</p><h1 id="介绍">介绍</h1><p>Scaramuzza et al. (2007).首先提出3d激光雷达与全向相机的外参标定问题，他们提出了使用手动选择关联点的方法完成标定。</p><p>本文提出的方法<strong>不需要手动选取对应点</strong>的3d激光雷达与全向相机的外参标定方法。</p><h1 id="方法">方法</h1><p>在我们的校准程序中，我们使用安装在平面表面上的棋盘图案，我们将从现在开始将其称为目标平面，外部校准的近似设置以及用于实验的感知传感器（Velodyne HDL-64e 3D激光扫描仪和Pointgrey Ladybug3全向相机在图1中示出。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/26/20210426225004.png"></p><h2 id="激光雷达内参标定">激光雷达内参标定</h2><p>有关传感器的更多技术细节，参见McBride（2008）。 计算出的范围测量<span class="math inline">\(D_l\)</span>由于TOF计算中的误差而包含一些偏差，因此具有来自实际测量的偏置校正<span class="math inline">\(\delta D\)</span>，因此制造商通常会为每条线束提供一个标定值<span class="math inline">\(\delta D\)</span>来进行补偿，通常使用图2的方法：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/26/20210426225828.png"></p><p>激光扫描仪安装在壁前的支撑件上，并且通过考虑墙壁和激光扫描仪之间的手动测量距离来计算范围测量中的偏移量：</p><p><span class="math display">\[    \delta D = D_m -D_l\]</span></p><ul><li><span class="math inline">\(D_m\)</span>是墙与雷达之间的距离手工测量值</li><li><span class="math inline">\(D_l\)</span>是tof测量值</li></ul><p>我们提出了一种鲁棒的方法来在范围测量中自动计算这种最佳偏移<span class="math inline">\(\delta D\)</span>。</p><p>与制造商采用的校准方法相反，该方法仅要求用户将激光雷达放置于墙或者平面的前面，然后记录测量数据，如图3所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/27/20210427091346.png"></p><p>在墙壁或平面表面前方的传感器平台的不同位置记录激光测量。 现在，如果我们使用（1）中计算的<span class="math inline">\(\delta D\)</span>校正，并考虑躺在墙壁上的点，它们应该都是共面的。</p><p>但是由于<span class="math inline">\(\delta D\)</span>并非真值，因此这些点的重投影误差是显著的。因此可以通过最小化重投影误差来求解。</p><p>我们使用Ransac（Fischler和Bolles（1981））来估算在墙壁上的所有点的最佳拟合平面的等式，首先生成一个包含目标平面以及潜在激光点的边界框。然后，这些潜在激光点<span class="math inline">\(\{\tilde{Q}_l^i ; i=1,2,\cdots,N\}\)</span>用于<code>RANSAC</code>进行平面拟合并返回内点。<code>RANSAC</code>步骤如下：</p><ol type="1"><li>随机选取3个点from<span class="math inline">\(\{\tilde{Q}_l^i ; i=1,2,\cdots,N\}\)</span></li><li>求解3个点的平面方程</li><li>遍历其他点，寻找内点</li><li>重复，直到找到最佳平面方程</li></ol><p>对<code>RANSAC</code>的最终结果进行参数化，<span class="math inline">\(\mathbf{N}=[n_x,n_y,n_z]^{\mathbf{T} }\)</span>，其中<span class="math inline">\(\|\mathbf{N}\|\)</span>表示平面到坐标系原点的距离。</p><p>因此，平面中的点<span class="math inline">\(\tilde{P}=[X,Y,Z]^{T}\)</span>在平面法向量<span class="math inline">\(\mathbf{N}\)</span>的投影等价于平面距离<span class="math inline">\(\|\mathbf{N}\|\)</span>，即：</p><p><span class="math display">\[    \mathbf{P}\cdot\mathbf{N} = \|\mathbf{N}\|^{2}\]</span></p><p>记<span class="math inline">\(D\)</span>为点的距离，<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\omega\)</span>分别记为俯仰角和方位角，则有：</p><p><span class="math display">\[\begin{aligned} X &amp;=D \cos \theta \sin \omega, \\ Y &amp;=D \cos \theta \cos \omega \\ Z &amp;=D \sin \theta \end{aligned}\]</span></p><p>所以，使用平面法向量表示的点距离可以表示为（联合上面所有式子可得）：</p><p><span class="math display">\[D=\frac{\|\mathbf{N}\|}{n_{x} \cos \theta \sin \omega+n_{y} \cos \theta \cos \omega+n_{z} \sin \theta}\]</span></p><p>所以，我们现在获得了从<code>RANSAC</code>方法获得的点距离，以及激光雷达本身直接返回的点距离，可以构造如下非线性最小二乘：</p><p><span class="math display">\[\delta D_{i}^{\prime}=\underset{\delta D_{i}^{\prime} }{\operatorname{argmin} } \sum_{i=1}^{64} \sum_{j=1}^{n}\left\|D_{i j}-\left(D_{l_{i j} }+\delta D_{i}^{\prime}\right)\right\|\]</span></p><ul><li><span class="math inline">\(D_{i j}\)</span>是根据<code>RANSAC</code>计算的平面方程后得到的点距离</li><li><span class="math inline">\(D_{l_{i j} }\)</span>是雷达直接返回的测距值</li></ul><p>经过补偿之后的结果如图4所示：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/27/20210427093756.png"></p><h2 id="全向相机">全向相机</h2><p><code>PointGrey Ladybug 3（LB3）</code>是高分辨率全向摄像机系统，它有六个200万像素摄像头，其中五个CCD位于水平环中，另外一个位于垂直方向，使系统能够从超过80％的球形范围内收集视频。</p><p>摄像机由制造商预校准，因此单个相机的内在参数是已知的，同样的，以公共坐标系为参考基准（称为<code>camera head</code>），所有摄像头相对于参考坐标系的刚体变换也是已知的。</p><p>因此，我们需要估计<code>camera head</code>的姿势（相对于一些本地参考帧），以便我们可以代表相机头帧中的任何3D点，然后到任何相机的坐标系。</p><h2 id="激光雷达与全向相机外参标定">激光雷达与全向相机外参标定</h2><p>外参标定方法与Zhang(2004)的方法相似，要求系统在不同的位姿观察平面pattern（如棋盘格），并根据激光雷达和相机同时观测的数据建立约束。</p><p>目标平面的法向量以及平面上的激光点之间存在关系，可用于约束相机和激光雷达的相对位姿关系。我们已知目标平面的方程，为了方便起见，以该平面构建坐标系，令</p><p><span class="math display">\[    Z=0\]</span></p><p>令：</p><ul><li><span class="math inline">\(\tilde{P}_{w}\)</span>为在世界参考坐标系中的点(此处是attach到目标平面的坐标系)</li><li><span class="math inline">\({ }_{w}^{c_{i} } R\)</span>是从世界坐标系<span class="math inline">\(w\)</span>到第i帧相机坐标系<span class="math inline">\(c_i\)</span>的旋转变换</li><li><span class="math inline">\({}^\mathrm{c_i}\mathrm{t}_{\mathrm{c}_{\mathrm{i} } \mathrm{w} }\)</span>是平移量</li></ul><p>因此，将一个世界参考坐标系中的点转换到第i帧相机参考坐标系可表示为：</p><p><span class="math display">\[\tilde{P}_{c_{i} }={ }_{w}^{c_{i} } R \tilde{P}_{w}+{ }^{\mathbf{c}_{\mathbf{i} } } \mathbf{t}_{\mathbf{c}_{\mathbf{i} \mathbf{w} } }\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\tilde{P}_{c_{i} }\)</span>是世界坐标系投影到第i帧相机坐标系的点</li></ul><p>由于已知每个相机与<code>camera head</code>的相对位姿<span class="math inline">\({ }_{c_{i} }^{h} R,{ }^{\mathrm{h} } \mathbf{t}_{\mathbf{h c}_{\mathbf{i} } }\)</span>，因此可以将第i个相机坐标系中的点转换到<code>camera head</code>坐标系中：</p><p><span class="math display">\[\tilde{P}_{h}={ }_{c_{i} }^{h} R \tilde{P}_{c_{i} }+{ }^{\mathbf{h} } \mathbf{t}_{\mathbf{h c}_{\mathbf{i} } }\]</span></p><p>因此，如果已知<span class="math inline">\({ }_{w}^{c_{i} } R\)</span>,<span class="math inline">\({}^\mathrm{c_i}\mathrm{t}_{\mathrm{c}_{\mathrm{i} } \mathrm{w} }\)</span>，就可以将世界坐标系中目标平面上的点<span class="math inline">\(\tilde{P}_w\)</span>转换到<code>camera head</code>坐标系中。</p><p>论文采用了<code>(Zhang (1998))</code>的方法来获取相对目标平面（也就是棋盘格坐标系作为世界坐标系）的位姿变换。</p><p>特别的，对于<code>pin-hole</code>相机模型，3d点投影关系如下：</p><p><span class="math display">\[\tilde{p} = K_i T_{w}^{ci} \tilde{P}_{w}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(K_i\)</span>为3x4矩阵，是第i个相机的内参矩阵</li><li><span class="math inline">\(\tilde{P}_{w}\)</span>为世界参考坐标系（棋盘格坐标系）下的点的齐次表达形式<span class="math inline">\(\tilde{P}_{w}=\left[\begin{array}{llll}X &amp; Y &amp; Z &amp; 1\end{array}\right]^{\top}\)</span></li><li><span class="math inline">\(\tilde{p}\)</span>为投影得到的图像点<span class="math inline">\(\tilde{p}=[u , v , 1]^{\top}\)</span></li><li><span class="math inline">\(T_{w}^{ci}\)</span>为外参，是世界坐标系到相机坐标系的变换</li></ul><p>假设图像点受独立同分布的噪声影响，外参<span class="math inline">\(T_{w}^{ci}\)</span>的最大似然估计可以使用如下最小化重投影误差来表示：</p><p><span class="math display">\[\underset{ { }_{w}{ }_{i} R,{ }^{\mathrm{c} } \mathbf{i} \mathbf{t}_{\mathbf{c}_{\mathbf{w} } \mathbf{w} } }{\operatorname{argmin} } \sum_{k=1}^{n} \sum_{j=1}^{m}\left\|p_{\hat{k} j}-K_{i}\left[{ }_{w}^{c_{i} } R{ }^{\mathbf{c}_{\mathbf{i} } } \mathbf{t}_{\mathbf{c}_{\mathbf{i} \mathbf{w} } }\right] \tilde{P}_{j}\right\|\]</span></p><p>其中，</p><ul><li><span class="math inline">\(n\)</span>表示n张图片</li><li><span class="math inline">\(m\)</span>表示每张图片对应的m个点</li><li><span class="math inline">\({ }_{w}^{c_{i} } R=[\boldsymbol{r}_1,\boldsymbol{r}_2,\boldsymbol{r}_3]\)</span></li><li><span class="math inline">\({}^\mathrm{c_i}\mathrm{t}_{\mathrm{c}_{\mathrm{i} } \mathrm{w} }\)</span>使用3维欧氏向量表示</li></ul><p>因此，根据:</p><p><span class="math display">\[\begin{aligned}    Rp+t &amp;= P_{new} \\    p &amp;= R^{-1}(P_{new}-t)\end{aligned}\]</span></p><p>有：在第i个相机坐标系中的目标平面方程可以写成（利用平面<span class="math inline">\(Z=0\)</span>的条件）：</p><p><span class="math display">\[\mathbf{r}_{3} \cdot\left(\mathbf{p}+{ }^{\mathbf{c}_{\mathbf{i} } } \mathbf{t}_{\mathbf{c}_{\mathbf{i} \mathbf{w} } }\right)=0\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\boldsymbol{r}_3\)</span>是旋转矩阵<span class="math inline">\({ }_{w}^{c_{i} } R\)</span>第三列</li><li>旋转矩阵<span class="math inline">\({ }_{w}^{c_{i} } R\)</span>第三列与点做<code>点乘</code>相当于旋转矩阵的转置后的第3行与点做矩阵乘法，得到转换后的点的Z轴分量</li><li>同时，<span class="math inline">\(\boldsymbol{r}_3\)</span>也是世界坐标系（棋盘格坐标系）的法向量方向</li><li><span class="math inline">\(\boldsymbol{p}\)</span>是相机坐标系中，平面上的点</li></ul><p>目标平面的法向量在第i个相机坐标系表示如下：</p><p><span class="math display">\[\mathbf{N}_{\mathbf{c}_{\mathbf{i} } }=\left(\mathbf{r}_{\mathbf{3} } \cdot{ }^{\mathbf{c}_{\mathbf{i} } } \mathbf{t}_{\mathbf{c}_{\mathbf{i} \mathbf{w} } }\right) \mathbf{r}_{\mathbf{3} }\]</span></p><ul><li>由于<span class="math inline">\(\boldsymbol{r}_3\)</span>也是世界坐标系（棋盘格坐标系）的法向量方向，所以<span class="math inline">\(\|\mathbf{N}_{c_i}\|= \mathbf{r}_3 \cdot { }^{\mathbf{c}_{\mathbf{i} } } \mathbf{t}_{\mathbf{c}_{\mathbf{i} \mathbf{w} } }\)</span> 表示第i个相机坐标系到目标平面的距离 (注意是点乘)</li></ul><p>又因为第i个相机坐标系相对于<code>camera head</code>的位姿变换已知，因此可以计算以<code>camera head</code>坐标系为参考的目标平面法向量<span class="math inline">\(\mathbf{N}_h\)</span>：</p><p><strong>（注意，这不是简单的向量做旋转，因为距离也会改变，即<span class="math inline">\(\| N_h\|\)</span>是目标平面到<code>camera head</code>坐标系原点的距离），因此还要加上刚体变换的平移部分在法向量方向上的投影</strong></p><p><span class="math display">\[\mathbf{N}_{\mathbf{h} }=\frac{{ }^{h} R \mathbf{N}_{\mathbf{c}_{\mathbf{i} } } }{\left\|\mathbf{N}_{\mathbf{c}_{\mathbf{i} } }\right\|}\left(\left\|\mathbf{N}_{\mathbf{c}_{\mathbf{i} } }\right\|+\mathbf{N}_{\mathbf{c}_{\mathbf{i} } } \cdot \mathbf{h}_{\mathbf{t} \mathbf{h c}_{\mathbf{i} } }\right)\]</span></p><ul><li>一旦我们已知了目标平面法向量在<code>camera head</code>坐标系的表示，我们需要找到激光雷达坐标系中在目标平面中的3d点</li><li>我们使用上述<code>RANSAC</code>方法来提取这些3d点</li><li>上述两种信息为外参标定提供了约束条件</li></ul><p>令<span class="math inline">\(\{\tilde{P}_l^i ; i=1 , 2, \cdots ,n\}\)</span>为提取的目标平面中的激光雷达点，利用待估计的外参，可以转换到<code>camera head</code>坐标系中：</p><p><span class="math display">\[\tilde{P}_{h}^{i}={ }_{l}^{h} R \tilde{P}_{l}^{i}+{ }^{\mathbf{h} } \mathbf{t}_{\mathbf{h l} }\]</span></p><p>现在，如果将一束光线从<code>camera head</code>坐标系开始投射到目标平面上的一点，那么这个射线在平面法向量上的投影就等于从<code>camera head</code>坐标系到目标平面的距离，因此，从m个不同的视角获取数据，可以构造如下目标函数：</p><p><span class="math display">\[F=\sum_{i=1}^{m} \sum_{j=1}^{n}\left(\frac{\mathbf{N}_{\mathbf{h} }^{\mathbf{i} } }{\left\|\mathbf{N}_{\mathbf{h} }^{\mathrm{i} }\right\|} \cdot\left({ }_{l}^{h} R \mathbf{P}_{l}^{j}+{ }^{\mathbf{h} } \mathbf{t}_{\mathbf{h l} }\right)-\left\|\mathbf{N}_{\mathbf{h} }^{\mathbf{i} }\right\|\right)^{2}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\mathbf{N}_{\mathbf{h} }^{\mathbf{i} }\)</span>是目标平面在<code>camera head</code>坐标系中的第i个位姿对应的法向量</li><li>第一项表示激光雷达点在<code>camera head</code>坐标系构成的向量，在法向量方向的投影</li><li>第二项表示由相机信息获取的<code>camera head</code>坐标系与目标平面的距离</li></ul><h2 id="外参标定需要的最少视角个数">外参标定需要的最少视角个数</h2><p>需要至少三个目标平面的非共面观点来完全限制优化问题（17）以估计校准参数，</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/28/20210428091916.png"></p><p>只有一个平面的情况：如图5(a)所示：</p><ul><li>当保持姿态不变，沿目标平面萍乡的方向进行平移时，目标函数的值不变</li><li>以平面法向量为轴进行旋转时，目标函数的值也不变</li></ul><p>相似的，对于只有两个视角的情况：如图5(b)所示：</p><ul><li>传感器沿着平面交叉线平移并不改变目标函数值，从而在该方向上产生大的不确定性</li></ul><h2 id="估计参数的协方差">估计参数的协方差</h2><p>通过最小化目标函数得到的参数估计会由于传感器测量的不确定性导致具有一定的误差，如激光雷达测距误差约为0.02m。知道这种不确定性是非常重要的，以便在任何视觉或SLAM算法中使用此处计算的参数。</p><p>Haralick（1998）已经描述了通过任何标量非线性优化函数传播测量协方差的方法，唯一的假设是标量函数是非负的，具有有限的第一和二阶偏微分，对于理想数据即其值为零，并且输入中的随机扰动足够小，以便可以近似输出一阶泰勒系列扩张。</p><p>前文提出的目标函数满足这些假设，因此可以用这个方法来估计参数的协方差。</p><p>假设<code>camera head</code>坐标系相对于激光雷达坐标系可以用参数来表示：</p><p><span class="math display">\[\Theta=\left[{ }^{1} \mathbf{t}_{\mathbf{l h} }, \mathbf{\Phi}_{\mathbf{l h} }\right]^{\top}\]</span></p><p>其中，</p><ul><li><span class="math inline">\({ }^{1} \mathbf{t}_{\mathbf{l h} }=\left[t_{x}, t_{y}, t_{z}\right]^{\top}\)</span>表示从h坐标系到l坐标系的平移变换（在l坐标系的表示）</li><li><span class="math inline">\(\boldsymbol{\Phi}_{\mathrm{lh} }=\left[\theta_{x}, \theta_{y}, \theta_{z}\right]^{\top}\)</span>表示从h坐标系到l坐标系旋转</li></ul><p>关于参数<span class="math inline">\(\Theta\)</span>的协方差如下：</p><p><span class="math display">\[\Sigma_{\Theta}=\left[\frac{\partial^{2} F}{\partial \Theta^{2} }(X, \Theta)\right]^{-1} \frac{\partial^{2} F^{T} }{\partial X \partial \Theta}(X, \Theta) \Sigma_{X} \frac{\partial^{2} F}{\partial X \partial \Theta}(X, \Theta)\left[\frac{\partial^{2} F}{\partial \Theta^{2} }(X, \Theta)\right]^{-1}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(X=\left[\mathbf{N}_{\mathbf{h} }^{\mathbf{1} }, \tilde{P}_{l}^{1}, \tilde{P}_{l}^{2} \ldots \mathbf{N}_{\mathbf{h} }^{\mathbf{i} }, \tilde{P}_{l}^{1}, \ldots\right]^{T}\)</span>表示观测信息（包括平面法向量和目标平面中的激光点）</li></ul><h1 id="实验">实验</h1><h2 id="仿真实验略">仿真实验（略）</h2><h2 id="真实环境实验">真实环境实验</h2><p>所提出的外在校准方法已经在由安装有3D激光传感器和全向相机系统的车辆收集的实际数据上进行了测试，如图8所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/28/20210428095042.png"></p><p>我们有两套结果验证算法的准确性，在第一种情况下，我们考虑了类似于校准设置的设置：在车库内进行校准，棋盘图案安装在所有可用的平面表面上（包括侧壁和底层），如图9所示，由不同颜色表示的来自不同平面的点已经投影到相应的图像上。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/28/20210428095326.png"></p><p>在第二种情况下，我们将车辆从车库外面带走，并从福特校园周围的行驶中的车辆中收集了一些数据，在全向相机系统的5个摄像机上的5个摄像机上的360度视场的投影的结果如图10所示。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/28/20210428095455.png"></p><h1 id="代码库">代码库</h1><p>https://github.com/SubMishMar/cam_lidar_calib</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;d激光雷达与全向相机外参标定&quot;&gt;3D激光雷达与全向相机外参标定&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com:59080/images/2021/04/26/20210426223058.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;摘要&quot;&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>UTC Time and GPS Time Conversion</title>
    <link href="http://yoursite.com/2021/04/21/utc%E6%97%B6%E9%97%B4%E4%B8%8Egps%E6%97%B6%E9%97%B4/"/>
    <id>http://yoursite.com/2021/04/21/utc%E6%97%B6%E9%97%B4%E4%B8%8Egps%E6%97%B6%E9%97%B4/</id>
    <published>2021-04-21T06:55:30.000Z</published>
    <updated>2021-04-21T07:50:30.793Z</updated>
    
    <content type="html"><![CDATA[<h1 id="时钟系统的前世今生">时钟系统的前世今生</h1><blockquote><p>最近在完善惯导ros驱动，发现GPGGA语句和GPFPD语句输出的时间不一致，一个是utc时间，另一个是gps时间。为了实现时钟同步，就要完成二者之间的转换。</p></blockquote><h2 id="格林威治标准时间greenwich-mean-timegmt">格林威治标准时间（Greenwich Mean Time，GMT）</h2><p><code>格林尼治平均时间（Greenwich Mean Time，GMT）</code>是指位于英国伦敦郊区的皇家格林尼治天文台当地的平太阳时，格林尼治标准时间的正午是指当平太阳横穿格林尼治子午线时（也就是在格林尼治上空最高点时）的时间。由于地球每天的自转是有些不规则的，而且正在缓慢减速，因此格林尼治平时基于天文观测本身的缺陷，已经被原子钟报时的协调世界时（UTC）所取代。</p><blockquote><p>自1924年2月5日开始，格林尼治天文台负责每隔一小时向全世界发放调时信息。</p></blockquote><h2 id="世界时universal-time-ut">世界时（Universal Time, UT）</h2><p>后来，由于1925年以前人们在天文观测中，常常把每天的起始（0时）定为正午，而不是通常民用的午夜，给格林尼治平时的意义造成含糊，人们使用<code>世界时（Universal Time, UT）</code>一词来明确表示每天从午夜开始的格林尼治平时。</p><p>世界时是以地球自转为基准得到的时间尺度，其精度受到地球自转不均匀变化和极移的影响，为了解决这种影响，1955年国际天文联合会定义了UT0、UT1和UT2三个系统：</p><ul><li>UT0系统是由一个天文台的天文观测直接测定的世界时，没有考虑极移造成的天文台地理坐标变化。</li><li>UT1系统是在UT0的基础上加入了极移改正 Δλ，修正地轴摆动的影响。UT1是目前使用的世界时标准。被作为目前世界民用时间标准UTC在增减闰秒时的参照标准。</li><li>UT2系统是UT1的平滑处理版本，在UT1基础上加入了地球自转速率的季节性改正 ΔT。</li></ul><p>目前使用的世界时测算标准又称UT1。在UT1之前人们曾使用过UT0，但由于UT0没有考虑极移导致的天文台地理坐标变动的问题，因此测出的世界时不准确，现在已经不再被使用。</p><p>在UT1之后，由于人们发现，因为<strong>地球自转本身不均匀的问题</strong>，UT1定义的时间的流逝仍然不均匀，于是人们又发展了一些对UT1进行平滑处理后的时间标准，包括UT1R和UT2，但它们都未能彻底解决定义的时间的流逝不均匀的问题，这些<strong>时间标准现在都不再被使用</strong>。</p><h2 id="原子时international-atomic-time-tai">原子时（International Atomic Time, TAI）</h2><p>为了彻底解决定义的时间的流逝不均匀的问题，开始使用原子钟定义时间。人们首先用全世界的原子钟共同为地球确立了一个均匀流动的时间，称为<code>国际原子时（International Atomic Time, TAI）</code>。</p><blockquote><p>1967年第13届国际计量大会上通过一项决议，定义一秒为铯-133原子基态两个超精细能级间跃迁辐射振荡9,192,631,770周所持续的时间。[2][3]其起点为世界时1958年的开始。</p></blockquote><p>原子时起点定在1958年1月1日0时0分0秒（UT），即规定在这一瞬间原子时时刻与世界时刻重合。但事后发现，在该瞬间原子时与世界时的时刻之差为0.0039秒。这一差值就作为历史事实而保留下来。在确定原子时起点之后，由于<code>地球自转速度</code>的问题，使得原子时钟不能与世界时间保持协调。</p><h2 id="协调世界时coordinated-universal-time-utc">协调世界时（Coordinated Universal Time, UTC）</h2><p>为了使定义的时间与地球自转相配合，人们通过在TAI的基础上不定期<code>增减闰秒</code>的方式，使定义的时间与世界时（UT1）保持差异在0.9秒以内，这样定义的时间就是<code>协调世界时（Coordinated Universal Time, UTC）</code>。</p><blockquote><p>协调世界时是最接近格林威治标准时间（GMT）的几个替代时间系统之一。对于大多数用途来说，UTC时间被认为能与GMT时间互换，但<strong>GMT时间已不再被科学界所确定</strong>。</p></blockquote><p>UTC基于国际原子时，并通过不规则的加入闰秒来抵消地球自转变慢的影响。闰秒在必要的时候会被插入到UTC中，以保证协调世界时（UTC）与世界时（UT1）相差不超过0.9秒。</p><blockquote><p>这就是所谓的跳秒，由于需要适应地球自转变化，需要在不定时进行跳秒，截止2019年2月，已经18次跳秒。正因为跳秒的存在，才会导致后面介绍的GPS时与UTC时不一致。</p></blockquote><h2 id="gps时">GPS时</h2><p>GPS时是用于卫星定位系统时间，由于卫星系统是连续运行的，其要求时间系统也是连续的，因此采用原子钟的方法。GPS时间系统就是采用基于美国海军观测实验室维持的原子时。</p><p>GPS时在1980年1月6日0点0分与世界协调时(UTC)一致，此后就只按原子时来累计，不受外界影响，也不会产生跳秒。因此与UTC时间的差为秒的整数倍，即:</p><p><span class="math display">\[    Time_{GPS} = Time_{UTC}+n\]</span></p><p>特别的，GPS时间的计时方法采用星期数和秒周数来表示，其中周数作为C/A和P码中的十位字段发送，所以<span class="math inline">\(2^{10}=1024\)</span>周(19.6年)后会再次归零。</p><p>为了解决这个问题，现代化的GPS导航消息采用了13位的字段，每隔8192周(157年)才归零。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;时钟系统的前世今生&quot;&gt;时钟系统的前世今生&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;最近在完善惯导ros驱动，发现GPGGA语句和GPFPD语句输出的时间不一致，一个是utc时间，另一个是gps时间。为了实现时钟同步，就要完成二者之间的转换。&lt;/p&gt;
&lt;/bloc
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>LiDAR-Camera 标定-4</title>
    <link href="http://yoursite.com/2021/04/14/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_4/"/>
    <id>http://yoursite.com/2021/04/14/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_4/</id>
    <published>2021-04-13T20:55:30.000Z</published>
    <updated>2021-06-03T16:14:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spatiotemporal-calibration-of-camera-and-3d-laser-scanner">Spatiotemporal Calibration of Camera and 3D Laser Scanner</h1><p><img src="http://s1.nsloop.com:59080/images/2021/04/14/20210414165501.png"></p><h1 id="摘要">摘要</h1><p>我们提出了一种用于相机和3D激光扫描仪的开源时空校准框架。我们的解决方案基于常用的棋盘标记，需要在操作前进行一分钟校准，以提供准确和可重复的结果。该框架基于点对平面约束的批量优化，并且可以通过新颖的最小化来实现时间偏移校准，李代数中平面方程的连续表示以及B样条的使用。在仿真中评估了框架的属性，同时使用Velodyne VLP-16和SICK MRS6124 3D激光扫描仪通过两种不同的感官设置验证了性能。</p><h1 id="介绍">介绍</h1><p>提出的标定方法基于棋盘格的运动，以及一系列的来自相机和激光雷达的观测。然后执行批量优化来估计6dof刚体变换矩阵以及时间偏差。</p><p>该优化基于独立的激光雷达指向相机的平面约束，该平面约束由连续时间平面表示扩展，使得时间偏差可以得到优化。本文贡献在：</p><ul><li>使用通用的棋盘格标记来估计lidar、camera的外参和时间偏移</li><li>新颖的李代数形式的连续时间最小化平面表示，使用B样条</li><li>对初值不敏感，可以收敛到预期结果</li></ul><h1 id="方法">方法</h1><p>提出的框架专用于刚体连接的具有全局快门相机和3D Lidar，并且假设相机内参已知且已经校正了。此外，假设传感器之间的时间偏移未知，且较小近似于常值。</p><p>标定过程需要包含标定板与传感器之间的相对运动，因此应该被激光雷达和相机连续观测得到。录制数据之后，按照下图的过程进行处理：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/14/20210414171512.png"></p><p>其中，对于相机，则使用opencv传统检测算法进行检测；对于激光雷达，首先将点投影成range-image，然后通过手动标记或半自动跟踪的方法来提取标定板对应的平面点。</p><p>如果不考虑时间偏移，标定可以描述为基于待求解位姿的点-面优化问题：</p><p><span class="math display">\[T^{*} = \arg \min \sum_{i} \sum_{j} \pi(t_i)^{T}Tp_{i}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(T^{*}\)</span>是期望得到的从激光雷达坐标系到相机坐标系的变换矩阵</li><li><span class="math inline">\(p_i\)</span>表示标定板上第i个3D点的齐次坐标表示</li><li><span class="math inline">\(\pi(t_i)\)</span>表示由相机在<span class="math inline">\((t_i+\Delta t)\)</span>时刻估计得到的标定板平面等式。</li></ul><p>提出的方法还注意到时间的校准，即考虑到点云中的每个点都对应一个时间戳，则目标函数变为：</p><p><span class="math display">\[T^{*},\Delta t^{*} = \arg \min \sum_{i} \sum_{j} \pi(t_i+\Delta t)^{T}Tp_{i}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(\Delta t^{*}\)</span>是待估计的时间差</li><li>要形成上式，需要知道每个3d激光点对应的时间戳以及对应任意时间的平面表示</li><li>本文使用LM以及g2o来求解</li></ul><h2 id="d激光雷达时间偏移">3D激光雷达时间偏移</h2><p>对于机械旋转式激光雷达，都有一定的扫描周期，根据扫描起始和扫描结束，可以推断出对应点的时间：</p><h3 id="vlp16">vlp16</h3><p><span class="math display">\[t_{i}=t_{\text {cloud }}+\frac{\phi_{i}-\phi_{s}}{f\left(\phi_{e}-\phi_{s}\right)}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(t_{cloud}\)</span>表示扫描起始时间</li></ul><h3 id="sick-mrs6124">sick-mrs6124</h3><p><img src="http://s1.nsloop.com:59080/images/2021/04/15/20210415101745.png"></p><h2 id="连续时间平面表示">连续时间平面表示</h2><p>标定板在每一帧图像中都能检测到，由于相机内参、以及标定板物理参数已知，所以可以确定标定板坐标系到相机坐标系的变换。</p><p>因此，标定板的平面可以使用4维向量<span class="math inline">\((\bm{n},d)\)</span>来表示，其中<span class="math inline">\(\bm{n}\)</span>是平面法向量，<span class="math inline">\(d\)</span>是到坐标系原点的距离。对于所有的图像，可以获取到离散时间下的一组平面方程集合。</p><p>为了确定各个时间戳的平面方程，需要对这些方程进行插值，因为4维的平面表示形式并非最小的表示方式，因此可能还需要做归一化处理。为了避免这个问题，提出一种使用类似于SO(3)以及对应的李代数so(3)的思想来表示平面法向量</p><p>该思想只需要使用两个参数就可以表示归一化法向量，因此，提出的方法几乎与[21]提出的球面插值法相同。</p><p>因此，平面的超参数化是因为法向量<span class="math inline">\(\bm{n}\)</span>，如果使用两个成分来表示，那么平面可以表示为：<span class="math inline">\((\bm{n},d) \rightarrow (\omega_x,\omega_y,d)_{3\times 1}\)</span>:</p><p><span class="math display">\[\begin{aligned}\theta = acos(\bm{n}[2]) \\\omega_{x}=-\bm{n}[1]*\frac{\theta}{\sin \theta} \\\omega_{y}=\bm{n}[0]*\frac{\theta}{\sin \theta}\end{aligned}\]</span></p><blockquote><p>这个思想来源于 [20]A. Bartoli, “On the non-linear optimization of projective motion using minimal parameters“ European Conference on Computer Vision (ECCV), Copenhagen, 2002, 340–354. <img src="http://s1.nsloop.com:59080/images/2021/04/15/20210415161745.png"></p></blockquote><p>就是说，在3维欧氏空间中，以点(0,0,1)作为原点，以平行于x轴、y轴的方向作为坐标轴，展开一个超平面，那么3维欧氏空间中的矢量可以通过对数变换投影到该超平面上的一个点。</p><p><strong>二维情况</strong></p><p><img src="http://s1.nsloop.com:59080/images/2021/04/15/20210415163816.png"></p><p><strong>三维情况</strong></p><p><img src="http://s1.nsloop.com:59080/images/2021/04/15/20210415204817.png"></p><p>因此，选取3维欧氏空间中的单位向量<span class="math inline">\(q=[0,0,1]^{T}\)</span>，然后根据<span class="math inline">\(\cos \theta = q^{T} n\)</span>，那么可以得到<span class="math inline">\(\theta = \arccos (q^{T}n)=\arccos(\bm{n}[2])\)</span>，表示两个向量之间的夹角，特别的又因为球面半径是单位向量，因此，<span class="math inline">\(\theta\)</span>也表示两个向量之间的球面距离。</p><p>特别的，根据下式可构成超平面上的点<span class="math inline">\(p=(\omega_x,\omega_y)\)</span>：</p><p><span class="math display">\[\begin{aligned}\theta = \arccos(\bm{n}[2]) \\\omega_{x}=-\bm{n}[1]*\frac{\theta}{\sin \theta} \\\omega_{y}=\bm{n}[0]*\frac{\theta}{\sin \theta}\end{aligned}\]</span></p><p>其中，</p><p><span class="math display">\[\begin{aligned}    \sqrt{\omega_x^2 + \omega_y^2}     &amp;=     \sqrt{        \frac{\arccos^{2} (n[2])}{\sin^{2}( \arccos(n[2])) }        (n[0]^{2}+n[1]^{2})    }    \\    &amp;=    \sqrt{        \frac{\arccos^{2} (n[2])}{1- \cos^{2}(\arccos(n[2])) }        (1-n[2]^{2})    }    \\    &amp;= \arccos (n[2]) = \theta\end{aligned}\]</span></p><p>可以发现，在超平面上，点q和点p的距离等于球面上向量q和另一个向量之间的球面距离。</p><p>因此，实现了使用两个参数来表示欧氏空间中的向量，需要注意的是，<span class="math inline">\(\theta==0\)</span>时，取<span class="math inline">\(\frac{\theta}{\sin \theta}=1\)</span>，特别的，仅适用于<span class="math inline">\(\theta &lt; \pi\)</span>的情况下。</p><h2 id="平面方程插值">平面方程插值</h2><p>使用最小化的平面表示使得可以进行插值，然后返回到所需时间戳对应的4维的平面方程表示形式。</p><p>对于此任务，我们尝试了参数的线性插值，由于缺乏连续的微分导致优化陷入局部最小值。因此，提出了使用三次样条插值，确保最小平面表示具有一阶和二阶的连续微分。如下：</p><p><span class="math display">\[\mathbf{s}(t)=\mathbf{s}_{0} B_{0}(t)+\sum_{i=1}^{n}\left(\mathbf{s}_{i}-\mathbf{s}_{i-1}\right) B_{i}(t)\]</span></p><p>其中，</p><ul><li><span class="math inline">\(s(t)\)</span>是在时间t插值得到的结果</li><li><span class="math inline">\(s_i\)</span>是根据测量得到的第i个控制点的值</li><li><span class="math inline">\(B_i(t)\)</span>是cumulative basis function的第i个成分</li><li><span class="math inline">\(n\)</span>是B样条的阶数</li></ul><p>对于李代数，则有插值方程如下：</p><p><span class="math display">\[\mathbf{r}(t)=\log \left\{\exp \left(\mathbf{r}_{0} B_{0}(t)\right) \prod_{i=1}^{n} \exp \left(\boldsymbol{\Omega}_{i} B_{i}(t)\right)\right\}\]</span></p><p>其中，</p><ul><li><span class="math inline">\(r(t)\)</span>表示t时刻的李代数插值结果</li><li><span class="math inline">\(r_i\)</span>表示李代数表示的第i个控制点</li><li><span class="math inline">\(\Omega_i=\log(\exp(r_{i-1})^{T}\exp(r_i))\)</span>表示两个李代数之间的差值</li></ul><blockquote><p>来源于文献[23]：A. Patron-Perez, S. Lovegrove, and G. Sibley, “A spline-based trajec- tory representation for sensor fusion and rolling shutter cameras“, in International Journal of Computer Vision, vol. 113, no. 3, 208–219, 2015.</p></blockquote><p>使用4阶B样条插值，cumulative basis function 如下：</p><p><span class="math display">\[\mathbf{B}(u)=\frac{1}{6}\left[    \begin{array}{cccc}    6 &amp; 0 &amp; 0 &amp; 0 \\     5 &amp; 3 &amp; -3 &amp; 1 \\     1 &amp; 3 &amp; 3 &amp; -2 \\     0 &amp; 0 &amp; 0 &amp; 1\end{array}    \right]    \left[\begin{array}{c}    1 \\ u \\ u^{2} \\ u^{3}    \end{array}    \right]\]</span></p><p>其中，</p><ul><li><span class="math inline">\(u\)</span>是从t2到t3之间的归一化时间(0~1)，在实践中，我们还检查了检测到的棋盘的时间戳（T1，T2，T3，T4）是否均匀地分布在时间内，如果不满足这种情况，则不会执行插值</li></ul><p>重要的是，可以使用[23]的已知方程来导出B样条曲线的jacobians，特别的，本系统使用了[24]所述的更有效的雅可比表示。</p><blockquote><p>[24]C. Sommer, V. Usenko, D. Schubert, N. Demmel, and D. Cremers, “Efficient derivative computation for cumulative B-splines on Lie groups“, arXiv preprint, 1911.08860v1, 2019.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;spatiotemporal-calibration-of-camera-and-3d-laser-scanner&quot;&gt;Spatiotemporal Calibration of Camera and 3D Laser Scanner&lt;/h1&gt;
&lt;p&gt;&lt;img sr
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>LiDAR-Camera 标定-2</title>
    <link href="http://yoursite.com/2021/04/12/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_2/"/>
    <id>http://yoursite.com/2021/04/12/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_2/</id>
    <published>2021-04-12T13:05:30.000Z</published>
    <updated>2021-06-03T16:14:12.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="calibration-of-rgb-camera-with-velodyne-lidar">Calibration of RGB Camera With Velodyne LiDAR</h1><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412212556.png"></p><blockquote><p>适用于32线以上</p></blockquote><h1 id="摘要">摘要</h1><p>本文提出了一种从粗到细的lidar-camera的标定方法，以前的方法用于计算校准参数的已知棋盘标记的多个视图，或者它们仅限于具有小相互位移的传感器的校准。</p><p>我们的方法提出了一种用于粗校准的新型3D标记，可以在相机图像和激光扫描中鲁棒地检测到粗略校准。只需要单帧的cam-lidar数据就可以估计比较大的平移外参，后续的refinement步骤将在标定参数子空间中寻找更准确的参数。</p><p>本文同样提出了一种基于投影误差的标定准确度度量。</p><h1 id="介绍">介绍</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;calibration-of-rgb-camera-with-velodyne-lidar&quot;&gt;Calibration of RGB Camera With Velodyne LiDAR&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com:
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>LiDAR-Camera 标定-3</title>
    <link href="http://yoursite.com/2021/04/12/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_3/"/>
    <id>http://yoursite.com/2021/04/12/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_3/</id>
    <published>2021-04-12T13:05:30.000Z</published>
    <updated>2021-06-03T16:14:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="automatic-extrinsic-calibration-method-for-lidar-and-camera-sensor-setups">Automatic Extrinsic Calibration Method for LiDAR and Camera Sensor Setups</h1><p><img src="http://s1.nsloop.com:59080/images/2021/04/13/20210413113205.png"></p><h1 id="摘要">摘要</h1><p>本文提出了一种无需用户干预即可进行激光雷达-立体相机对的外在校准的方法。我们的校准方法旨在解决汽车设置中常见的限制，例如低分辨率和特定的传感器姿态。</p><h1 id="介绍">介绍</h1><p>现有的校准方法要么需要复杂的设置，要么缺乏通用性，因此结果的准确性在很大程度上取决于传感器的参数或环境的结构性。</p><p>与现有方法不同，没有进行严格的假设，因此允许中等分辨率如16线lidar，以及适用于传感器之间的相对姿势较大的情况。</p><p>我们的方法可以使用简单的设置在合理的时间内执行，该设置旨在利用来自两个设备的数据中的对应关系。</p><h1 id="标定">标定</h1><p>一个定制的平面target用于提供两个传感器之间的特征配对关系，如图2所示</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/13/20210413141802.png"></p><p>图2所示的模式具有几何和视觉特性，可以估计lidar、双目、单目的关键点。一方面，4个圆孔可以利用激光、双目点云的不连续性获取，另一方面，4个ArUco标记放置于4个角点附近，可以使用单目摄像头获取。</p><p>这种方法不会对传感器之间的相对姿态有较强的限制，因此适用于平移和旋转较大的情况。实际上，只需要满足两个约束即可：</p><ul><li>传感器之间必须有包含校准目标的共视区域</li><li>传感器可以良好的观察到校准目标（如圆孔），特别的，当校准中包含距离数据时，每一个圆至少需要3个点来表示。对于多线激光雷达而言，必须要有两根线达到同一个圆上</li><li>特别的，对于视觉传感器而言，需要提前知道设备内参</li></ul><p>提出的方法如图3所示，分为两个阶段：</p><ol type="1"><li>第一阶段包括校准目标的分割和每个传感器的参考点定位</li><li>第二阶段是求解参考点的转换</li></ol><p><img src="http://s1.nsloop.com:59080/images/2021/04/13/20210413142957.png"></p><h2 id="目标分割">目标分割</h2><p>这第一阶段旨在在每个传感器的数据中对校准目标进行定位，在此阶段中的信息是相对于该传感器坐标系而言的。最终输出一组包含4个3D点数据，</p><h3 id="激光雷达数据">激光雷达数据</h3><p>在将数据馈送到分割处理之前，通过了三个笛卡尔坐标的pass-through滤波器来提出无关数据的影响，因此必须根据传感器重叠区域的位置和大小来设置pass-through滤波器的限制。</p><p>预处理后的点云<span class="math inline">\(\mathcal{P}_{1}^{L}\)</span>包含校准目标和激通过孔中可见的点。对于<span class="math inline">\(\mathcal{P}_{1}^{L}\)</span>中的每一个点，按下式计算其深度梯度幅值：</p><p><span class="math display">\[p_{i, \Delta}=\max \left(p_{i-1, r}-p_{i, r}, p_{i+1, r}-p_{i, r}, 0\right)\]</span></p><p>其中，</p><ul><li><span class="math inline">\(p_{i,r}\)</span>表示关于点<span class="math inline">\(p_i\)</span>的测量</li><li><span class="math inline">\(p_{i-1},p_{i+1}\)</span>是点<span class="math inline">\(p_i\)</span>的同一线束扫描的邻近点</li></ul><p>然后，利用一个阈值对所有不连续性较高的点进行提取，得到点云<span class="math inline">\(\mathcal{P}_{2}^{L}\)</span></p><h3 id="双目立体摄像头数据">双目/立体摄像头数据</h3><p>当要校准的传感器之一是立体视觉系统时，首先通过立体声匹配过程将原始图像对转换为3D点云。</p><blockquote><p>在我们的实验中，我们使用OpenCV实现的[28]的半全局块匹配（SGBM）变体，我们发现我们发现合理准确地进行深度估计。注意，当涉及这种模态时，预计校准目标将具有一些纹理（例如，木纹），以便可以成功解决立体声对应问题。但是，在我们的实验中，我们发现由模式边界引起的强度差异通常是足够的，。</p></blockquote><p>与激光雷达数据处理类似，首先通过pass-through滤波器进行过滤。不同的是，对于立体视觉，通过提取点云目标的边缘，具体地，使用sobel算子来对图像提取边缘，然后根据边缘强度对点云进行滤除，得到点云<span class="math inline">\(\mathcal{P}_{2}^{S}\)</span></p><h3 id="对深度数据提取目标点">对深度数据提取目标点</h3><p>这一步主要是对激光雷达/立体视觉预处理后的数据进行下一步处理：</p><p>（1）平面分割：</p><p>首先，使用RANSAC算法对点云<span class="math inline">\(\mathcal{P}_{1}\)</span>(包括激光雷达/立体视觉)进行平面拟合得到模型<span class="math inline">\(\pi\)</span>，为了确保模型的准确，使用了较为严格的RANSAC阈值，并且提取的平面必须大概与传感器坐标系成垂直关系，使用一个容限<span class="math inline">\(\alpha_{plane}\)</span>.</p><p>然后，根据得到的平面模型<span class="math inline">\(\pi\)</span>，对点云<span class="math inline">\(\mathcal{P}_{2}\)</span>中的点进行剔除，然后得到点云<span class="math inline">\(\mathcal{P}_{3}\)</span></p><p>（2）转换到2d空间：</p><p>由于所有其余点都属于同一平面，因此在该点执行降维：通过转换x-y平面与模型<span class="math inline">\(\pi\)</span>重合，可以将点云<span class="math inline">\(\mathcal{P}_{3}\)</span>转换到平面点<span class="math inline">\(\mathcal{P}_{4}\)</span></p><p>（3）圆形提取</p><p>接下来，使用2D圆分割来提取<span class="math inline">\(\mathcal{P}_{4}\)</span>中存在的图案孔的模型，此步骤是迭代地执行的过程中，在寻找最可能的圆圈，并且在寻找下一个圆之前，剔除掉已经找到的圆。如果找到4个圆，才进入下一步。</p><p>为此，将中心分成四个一组，并将它们形成的矩形的尺寸（对角线，高度，宽度和周长）与理论值进行比较，公差δ一致性表示为与中心线的偏差百分比。 期望值。 大概只有一组中心可以满足这些限制，</p><p>一旦识别到圆，就可以将其圆心反投影回3d空间，形成点云<span class="math inline">\(\mathcal{P}_{p}\)</span>，<span class="math inline">\(\mathcal{P}_{p}\)</span>必须正好是4个中心点。</p><h3 id="单目摄像头数据">单目摄像头数据</h3><p>如果要校准的传感器是单眼相机，则参考点的提取需要检测ARUCO标记，其提供所需的提示来检索目标的几何形状。Aruco标记是由黑色边界和内部二进制矩阵制成的合成方标记，旨在允许其明确识别[27]。 在我们的校准目标中，使用四个ARUCO标记，每个角落都是一个; 由于此位置，它们不会影响其他方式的目标或孔检测。</p><p>如果相机内参以及标记的尺寸已知，就可以通过Pnp恢复每个标记相对于相机坐标系的位姿，在我们的实施中，我们将四个标记设置为一个ARUCO板，这个板允许利用4个标记来共同估计校准目标的位姿。</p><p>然后通过LM优化来最小化重投影误差来求解ARUCO板的位姿，使用4个标记的位姿均值作为初始值，最后得到了ARUCO板中心的3d位姿。</p><p>为了生成与点云<span class="math inline">\(\mathcal{P}_{p}\)</span>同等的4个点，利用已知的相对位置关系来分别提取4个圆孔中心的3D点，得到点云<span class="math inline">\(\mathcal{P}_{M}\)</span></p><h3 id="点云聚类">点云聚类</h3><p>在分割阶段的最后，已经得到了两组点云<span class="math inline">\(\mathcal{P}_{p}\)</span>，每一组点云都是相对于传感器坐标系的。</p><p>这些数据足以找到转换传感器的相对姿势的转换，然而，方法固有的不同噪声源（例如，传感器噪声和诸如Ransac等非确定性程序）可能会影响结果的准确性。为了提高算法的稳健性，我们通过反复应用分割步骤并以两种不同的方式累积结果来增加可用的信息</p><p>（1）数据帧累积</p><p>由于场景可以是静止的，因此可以通过累积N帧的点云<span class="math inline">\(\mathcal{P}_{p}\)</span>来得到<span class="math inline">\(\mathcal{P}_{p}^{&#39;}\)</span>，如果找到的超过4个圆，则不可用</p><p>（2）目标板不同位姿的数据累积</p><p>本方法可以通过单个目标板的位姿来求解，然而，通过考虑超过四个参考点，可以提高估计的准确性。另一方面，单帧提取得到的4个点有可能不共面，通过多帧可以提高标定效果。</p><h2 id="配准">配准</h2><p>在分割阶段结束后，一共获取到两组<span class="math inline">\(\mathcal{P}_{p}^{&#39;}\)</span>点云，每组分别对应一个传感器，主要包含每个圆圈中心相对于该传感器坐标系的点。并且，两组点云的点对关系是已知的。</p><h3 id="点关联">点关联</h3><p>提出了一种策略，来避免设置两组<span class="math inline">\(\mathcal{P}_{p}^{&#39;}\)</span>点云的点具有相同的顺序关系。</p><p>首先将<span class="math inline">\(\mathcal{P}_{p}^{&#39;}\)</span>点云中的4个点投影到球面坐标系，然后（only assume that the point that appears highest in the cloud, that is, the one with the lowest inclination angle, belongs to the upper row of the calibration target (i.e., either the top-left or the top-right circle).）</p><p>上面步骤首先确定了一个上方的点，然后根据这一点到另外三个的距离确定了正确的排序。因此，可以建立起关联：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/13/20210413220250.png"></p><h3 id="求解">求解</h3><p>排序好的两组点云，记为<span class="math inline">\(\mathcal{P}_{c}^{&#39;X},\mathcal{P}_{c}^{&#39;Y}\)</span>，通过使用<code>Umeyama</code>配准[30]，可以寻找两组点云的刚体变换，其中，两组点云中的点具有匹配关系如下：</p><p><span class="math display">\[p_{i, a}^{X}=p_{i, a}^{Y} \wedge p_{i, m}^{X}=p_{i, m}^{Y}\]</span></p><p>即目标函数为两组点云之间的距离：</p><p><span class="math display">\[\frac{1}{4 \cdot M} \sum_{i=0}^{4 \cdot M}\left\|\mathbf{p}_{i}^{X}-\mathbf{T}_{X Y} \mathbf{p}_{i}^{Y}\right\|^{2}\]</span></p><p>由于点云配对关系已知，所以使用svd即可求得闭式解。方便的是，<code>Umeyama</code>方法可以处理所有点都在同一平面上的情况，例如使用单个图案位置（M = 1）时，这样可以避免将它们误认为是反射。</p><h1 id="实验">实验</h1><p>除了需要人工放置标记板之外，其他参数使用固定值如下表：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/13/20210413221315.png"></p><p>其中，除非另有说明，否则参考点累积超过30帧（n = 30）</p><h2 id="gazebo仿真">gazebo仿真</h2><p><img src="http://s1.nsloop.com:59080/images/2021/04/13/20210413222309.png"></p><p>在实验中，将目标放置在后面的墙壁，使得穿过圆孔的LIDAR梁到达表面，在前景和背景点之间产生必要的梯度。</p><p>高斯噪声<span class="math inline">\(\epsilon \sim \mathcal{N}(0,(K\sigma_{0})^{2})\)</span>被施加到传感器的捕获数据，对于像素强度和激光距离，<span class="math inline">\(\sigma_{0}= 0.007\)</span> m和<span class="math inline">\(\sigma_{0}= 0.008\)</span>米，其中：</p><ul><li>K=0表示理想环境</li><li>K=1表示真实环境</li><li>K=2表示噪声环境</li></ul><h3 id="特征提取实验">特征提取实验</h3><p>实际上，该方法无法在一些极端配置中提供结果; 具体为在LIDAR扫描仪的情况下，它们有限的分辨率使得不可能在远距离找到圆圈，而立体声受到深度估计的实质性降级的影响，即这种模块遭受的距离增加。</p><p>因此，在典型用例中，应该通过将图案位置限制为相对于传感器的合理距离范围来避免这些情况</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/13/20210413225825.png"></p><h3 id="求解结果">求解结果</h3><p><img src="http://s1.nsloop.com:59080/images/2021/04/13/20210413230741.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/04/13/20210413230807.png"></p><h2 id="真实环境实验">真实环境实验</h2><p><img src="http://s1.nsloop.com:59080/images/2021/04/13/20210413230839.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;automatic-extrinsic-calibration-method-for-lidar-and-camera-sensor-setups&quot;&gt;Automatic Extrinsic Calibration Method for LiDAR and Came
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>LiDAR-Camera 标定-1</title>
    <link href="http://yoursite.com/2021/04/12/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_1/"/>
    <id>http://yoursite.com/2021/04/12/LIDAR_CAMERA%E6%A0%87%E5%AE%9A_1/</id>
    <published>2021-04-12T01:05:30.000Z</published>
    <updated>2021-06-03T16:14:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="lidar-camera-calibration-using-3d-3d-point-correspondences">LiDAR-Camera Calibration using 3D-3D Point correspondences</h1><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412091225.png"></p><h1 id="摘要">摘要</h1><p>采用多个传感器来提供冗余信息，该信息可以减少具有错误测量的可能性。 在上述情况下，必须相对于单个参考帧从各种传感器获得数据，以便可以融合数据，并且可以利用冗余。</p><p>基于标记的[2]以及LIDAR和摄像机的自动校准已经提出，但在这些使用的方法和实验中讨论了高密度，更昂贵的激光雷达，并且当较低密度的LIDAR时，不太适用，例如 使用VLP-16。 我们提出了一种非常准确和可重复的方法来估计相机和激光器之间的6度自由度的外部校准参数</p><h1 id="传感器及参数准备">传感器及参数准备</h1><p>我们提出的方法利用LIDAR和相机的传感器数据。 在启动激光雷达相机校准过程之前应该知道相机的内在参数.</p><p>每次收集数据，LIDAR和相机都在3D空间中的任意距离保持,它们之间的转换是手动测量的。虽然，使用卷尺测量是粗糙的，但它用作使用各种算法获得的值的进行检查，测量平移量比旋转量容易的多，在其他情况下，当旋转极小时，我们假定它们为零。如果存在较大的角度，只能用三角板估计一下 。</p><h1 id="d-3d关联">2D-3D关联</h1><p>在使用3D-3D点对应关系的方法之前，我们尝试了涉及2D-3D对应关系的方法。我们设计了自己的实验设置，以帮助校准激光器和相机。</p><h2 id="实验设置">实验设置</h2><p><strong>需要设置的标记</strong>：中空矩形纸板。</p><p>即使是普通纸板也可以很好地工作，因为我们将在即将到来的讨论中看到，提供更少的对应关系，而不是挖空的矩形纸板</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412093538.png"></p><h2 id="数据提取">数据提取</h2><p>首先使用2D-3D方法：这种方法涉及通过匹配2D-3D点对应的方式在相机和激光器之间找到6-DOF变换，可以通过手动在图像中标记<strong>具有3-4像素的精度</strong>的图像中的特征点来容易地获得2D对应关系。</p><p>直接获取对应3d点的方法并不直接，这是因为lidar点云稀疏</p><p>平面纸板可以提供4个角点，在3D中，这些点可以通过直线拟合交点得到，在2D中，可以直接从marker的像素坐标获得。</p><p>如果空心纸板的外边框也使用，那么就可以提供8个3D-2D点关联，这样的设置允许有足够的数据来运行PNP算法的Ransac版本，并且还将有助于减少嘈杂的数据。</p><p>如果纸板的一侧与地面平行，由于激光水平扫描，那么可能只能获取到纸板的竖直边缘，而不能获取到水平边缘。为了克服这一点，我们将板倾斜以在一个边缘和地面平面之间制作大约45度来获取所有四个边缘获得点。 Ransac用于拟合LIDAR的点的直线。</p><p>这个节点允许手动绘制多边形，然后自动提取内部直线：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412100207.png"></p><p>纸板上最突出的特征是角点，它可以在图像上相对容易地标记，并且由于我们对四个边缘具有相当准确的线条方程，因此它们的交叉点在3D中计算，这些空间中的直线实际上可能无法相交，但是非常接近。 我们将角点近似于两行之间最短线段的中点。</p><p>实验发现：两个线段之间的距离是<span class="math inline">\(10^{-4}\)</span>米的距离，边缘长度之间的误差平均约为1厘米.</p><p>本实验获取到了20个角点：2个空心矩阵（2*8）+ 1个实心纸板（4）</p><h2 id="问题求解">问题求解</h2><p>Perspective n-Point (PnP)方法用于寻找2D-3D匹配点对之间的刚体变换，其中式（1）展示了3D点投影方程：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412101804.png"></p><p>式（2）展示了使用的通用cost-function：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412101850.png"></p><p>首先，使用了PNP和EPNP方法来最小化上述cost-func，然后通过手动剔除外点，进一步降低重投影误差到1.88个像素，但是，并没有求解出接近手动测量值的<span class="math inline">\(\{R,t\}\)</span></p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412103120.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412104259.png"></p><p>在上述实验中，由于激光雷达和相机比较接近，仅相差12厘米，且实验中仅使用了12个点进行EPNP，并没有达到预期效果。在随后的实验中，相机和lidar距离更远，减轻了误差的影响。</p><p>在检查数据时，我们发现了一些噪声点，这些噪声点造成了较大的重投影误差，我们运行了一个自定义的EPNP和RanSac算法，理论上可以确保滤除噪声点的影响。</p><p>通过实验发现，重投影误差小于1个像素，但是得到的<span class="math inline">\(\{R,t\}\)</span>与期望值相差更大，这可能意味着最小化重投影误差可能不是一种整体最优的方法，因此必须使用其他更好的度量。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412104559.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412104615.png"></p><h1 id="d-3d关联-1">3D-3D关联</h1><p>2D-3D对应方法在我们的实验设置中结果似乎不太理想，这可能是由于2D标记点的不准确或者使用含有噪声点进行PnP。尽管重投影误差似乎已经最小化，但是求出来的结果与测量值相差较大。</p><p>因此，这部分涉及使用增强 - 现实（AR）标签和LIDAR点云来找到外部校准参数，开源社区[7] [5]已释放多个版本的AR标签。 这里提出的方法使用ARUCO标签[5]。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412105046.png"></p><p>为了找到相机与Velodyne之间的转换，我们需要两组3D点：一个在相机坐标系中，另一组在Velodyne坐标系中。 一旦发现这些点对应，就可以求解了。</p><h2 id="实验设置-1">实验设置</h2><p>矩形纸板可以是任何任意尺寸。 我们执行的实验使用了一个Velodyne VLP-16 [3]，其在单次扫描中仅具有16个环，与较高密度的LIDAR相比（每次扫描的32和64环）相比。 对于低密度的LIDAR，如果板的尺寸很小，并且LIDAR保持比特定距离更远，则击中板的环数变为低（2至3个环，导致边缘仅2至3点） ，使其非常困难地适应边缘（使用Ransac）</p><p>实验中使用的纸板的长度/宽度在45.0-55.0厘米之间，保持距离激光雷达2米左右距离，就可以有足够的点来拟合直线和计算交点。</p><h3 id="相机坐标系中的3d点">相机坐标系中的3D点</h3><p>ARUCO标记是特殊编码的模式，其促进标记本身的检测和纠错。 有关如何在这里找到工作的更多详细信息[5]</p><p>ArcoTag贴在纸板上，如果已知<strong>纸板的尺寸</strong>和<strong>ARUCO标记的位置</strong>，则可以容易地计算角落的位置（来自aruco标记的中心）。</p><p>ArcoTag提供相机坐标系和标记中心的<span class="math inline">\(\{R,t\}\)</span>变换，这个变换可以用来将角点从标记所在的坐标系转换到相机坐标系。</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412154023.png"></p><h3 id="激光雷达坐标系中的3d点">激光雷达坐标系中的3D点</h3><p>通过检测纸板的边缘可以找到激光雷达中的点，这又可以以类似的方式为拐角又可以解决第3节中描述的类似方式。</p><p>使用ArcoTag获取到的转换矩阵（特别是平移量）非常准确，一旦获取到两组3D点集合，就可以使用ICP算法进行求解了。</p><h2 id="求解">求解</h2><p>ICP算法最小化式（3）表示的cost-func：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412154220.png"></p><p>一般ICP算法认为点云中的最近点作为对应关系（有其他选择最接近点的其他变体,找到正确的对应关系可能是棘手的，可能导致不期望的解决方案.</p><p>由于在本方法中，点与点之间的对应关系是已知的，因此icp算法存在闭式解（Kabsch算法[9] [10]找到两点云之间的旋转，并且坐标系对齐后就可以找到平移）：</p><p>下面的推导使用与[10]中的相同的参数。</p><p>首先，我们假设旋转是已知的，所以先求解平移量：</p><p>已知目标函数：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412154803.png"></p><p>对平移量t求导并等于0：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412154827.png"></p><p>进一步有：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412155111.png"></p><p>即：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412155139.png"></p><p>使用求出来的平移量替换式（4）中的部分，有：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412155524.png"></p><p>令：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412155545.png"></p><p>目标函数变成：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412155655.png"></p><hr><p>解释如下： 令<span class="math inline">\(X_{i}^{&#39;}-Y_{i}=[x_i,y_i,z_i]^{T}\)</span></p><p>对不同的点得到的[x_i,y_i,z_i]进行堆叠，则有：</p><p><span class="math display">\[\begin{aligned}    (X&#39;-Y)^{T}(X&#39;-Y)&amp;=    \begin{bmatrix}        x_1 &amp; y_1 &amp; z_1 \\        x_2 &amp; y_2 &amp; z_2    \end{bmatrix}    \begin{bmatrix}        x_1 &amp; x_2 \\        y_1 &amp; y_2 \\        z_1 &amp; z_2 \\    \end{bmatrix}    \\    &amp;=    \begin{bmatrix}        x_1^{2}+y_1^{2}+z_1^{2} &amp; m \\        n &amp; x_2^{2}+y_2^{2}+z_2^{2}    \end{bmatrix}\end{aligned}\]</span></p><p>实际上，我们的目标函数就是对角线元素之和。</p><hr><p>进一步的，利用矩阵的迹的性质，可以进一步简化：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412161346.png"></p><p>又因为旋转矩阵<span class="math inline">\(R\)</span>是正交矩阵，因此有<span class="math inline">\(|X_i^{&#39;}|^{2}=|X_i|^{2}\)</span>，进一步有：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412161850.png"></p><p>观察可知，前半部分是固定的，只有后半部分与旋转<span class="math inline">\(R\)</span>有关，而且我们的目标是最小化，因此，即最大化后面的部分：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412162006.png"></p><p>使用旋转<span class="math inline">\(R\)</span>替换回<span class="math inline">\(X^{&#39;}\)</span>，有：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412162046.png"></p><p>问题转化为，求<span class="math inline">\(R\)</span>使得<span class="math inline">\(Tr(XY^{T}R)\)</span>最大</p><p>进一步的，根据定理：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412170421.png"></p><p>因此，对<span class="math inline">\(XY^{T}\)</span>进行SVD分解，得到<span class="math inline">\(XY^{T}=UDV^{T}\)</span>，</p><p>令<span class="math inline">\(R=VU^T\)</span>，则</p><p><span class="math display">\[Tr(XY^{T}R)=Tr(UDV^{T}VU^{T})=Tr((UD^{\frac{1}{2}})(UD^{\frac{1}{2}})^{T})\]</span></p><p>因此，旋转<span class="math inline">\(R=VU^T\)</span></p><h2 id="多帧求解">多帧求解</h2><p>在初步实验中，观察到尽管在封闭空间中，激光雷达点云并非静止，为了减少噪声，采用多次扫描的方法。多帧数据中，保持激光雷达与相机空间关系不变：</p><p>对于每一帧数据估计出来的平移量，求均值有：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412171816.png"></p><p>对于旋转量，先转换到四元数，然后求平均：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412171922.png"></p><p>然后，还要进行归一化：</p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412172144.png"></p><h2 id="结果">结果</h2><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412173750.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412173818.png"></p><p><img src="http://s1.nsloop.com:59080/images/2021/04/12/20210412173831.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;lidar-camera-calibration-using-3d-3d-point-correspondences&quot;&gt;LiDAR-Camera Calibration using 3D-3D Point correspondences&lt;/h1&gt;
&lt;p&gt;&lt;img 
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>WGS-84转局部ENU坐标</title>
    <link href="http://yoursite.com/2021/03/01/%E7%BB%8F%E7%BA%AC%E5%BA%A6%E8%BD%AC%E5%B1%80%E9%83%A8ENU/"/>
    <id>http://yoursite.com/2021/03/01/%E7%BB%8F%E7%BA%AC%E5%BA%A6%E8%BD%AC%E5%B1%80%E9%83%A8ENU/</id>
    <published>2021-03-01T09:05:33.000Z</published>
    <updated>2021-03-03T07:12:00.347Z</updated>
    
    <content type="html"><![CDATA[<h1 id="wgs-84转ecef">WGS-84转ECEF</h1><p><span class="math display">\[\mathbf{R}_0=\begin{bmatrix}    -\sin(\lambda_0) &amp; \cos(\lambda_0) &amp; 0 \\    -\cos(\lambda_0)\sin(\phi_0) &amp; -\sin(\lambda_0) \sin(\phi_0) &amp; \cos(\phi_0) \\    \cos(\lambda_0)\cos(\phi_0) &amp; \sin(\lambda_0)\cos(\phi_0) &amp; \sin(\phi_0)\end{bmatrix} \]</span></p><p><span class="math display">\[\begin{bmatrix}    e_i \\ n_i \\ u_i\end{bmatrix}=\mathbf{R}_0 \begin{bmatrix}    x_i-x_0 \\ y_i-y_0 \\ z_i-z_0\end{bmatrix}\]</span></p><p><span class="math display">\[s_f=\frac{t_{1}^{gnss-deq}-t_{i}^{lidar}}{t_{1}^{gnss-deq}-t_{0}^{gnss-deq}}\]</span></p><p><span class="math display">\[    bel(\boldsymbol{P}_{i})=    \begin{cases}    \exp \left( \frac{ - \| \tilde{\boldsymbol{P}}_{i}-\boldsymbol{P}_{i} \|}{\|(\Delta \boldsymbol{T}_{i,i-1})_{\left [ x,y,z \right]} \|} \right ) &amp;     \| \tilde{\boldsymbol{P}}_{i}-\boldsymbol{P}_{i} \|    \leq     \|(\Delta \boldsymbol{T}_{i,i-1})_{\left [ x,y,z \right]} \|    \\     0 &amp;     \| \tilde{\boldsymbol{P}}_{i}-\boldsymbol{P}_{i} \|    &gt;    \|(\Delta \boldsymbol{T}_{i,i-1})_{\left [ x,y,z \right]} \|     \end{cases}\]</span></p><p><span class="math display">\[E_i^{gnss} = k_p^{i} \|  (\boldsymbol{T}_{i})_{\left [ x,y,z \right]} - \boldsymbol{P}_{i} \|_{2}^{2}\]</span></p><p><span class="math display">\[[I_{k-n}^{imu-deq},I_{k-n+1}^{imu-deq},\cdots,I_k^{imu-deq},\cdots,I_{k+n-1}^{imu-deq},I_{k+n}^{imu-deq}]\]</span></p><p><span class="math display">\[    F_{G}^{i}=    \begin{cases}    1 &amp;     \| \| (I_{i}^{&#39;})_{\left [ acc \right]}\|-\mathbf{G}_{0} \|_2    \leq 0.3    \\     0 &amp;     \| \| (I_{i}^{&#39;})_{\left [ acc \right]}\|-\mathbf{G}_{0} \|    &gt; 0.3    \end{cases}\]</span></p><p><span class="math display">\[E_{i}^{atti}=F_{G}^{i}k_{G}^{i} \left \| H((\boldsymbol{T}_i)_{\left[r,p\right]},(\mathbf{I}_{i})_{\left[acc \right]})-\frac{\mathbf{G}_0}{\|\mathbf{G}_0\|} \right \|\]</span></p><p><span class="math display">\[E_i^{loop}=k_{L}^{i} \| L(\Delta \boldsymbol{T}_{ji}^{-1} \boldsymbol{T}_{i}^{-1} \boldsymbol{T}_{i}) \|_{2}^{2}\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;wgs-84转ecef&quot;&gt;WGS-84转ECEF&lt;/h1&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\mathbf{R}_0=
\begin{bmatrix}
    -\sin(\lambda_0) &amp;amp; \cos(\lambda
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>IMU预积分模型</title>
    <link href="http://yoursite.com/2021/02/19/IMU%E9%A2%84%E7%A7%AF%E5%88%86%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%AF%BC/"/>
    <id>http://yoursite.com/2021/02/19/IMU%E9%A2%84%E7%A7%AF%E5%88%86%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%AF%BC/</id>
    <published>2021-02-19T07:05:33.000Z</published>
    <updated>2021-05-31T09:00:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是预积分">什么是预积分</h1><p><img src="http://s1.nsloop.com:59080/images/2021/02/19/20210219155712.png"></p><blockquote><p>即对两个关键帧之间的imu数据进行整合，得到一个factor，并且该factor基本上不随两个关键帧的状态改变而变化</p></blockquote><h1 id="预积分计算">预积分计算</h1><p>回顾惯导解算，已知系统位置、速度、姿态的微分方程如下：</p><p><span class="math display">\[\begin{array}{l}\dot{\mathbf{p}}_{w b_{t}}=\mathbf{v}_{t}^{w} \\\dot{\mathbf{v}}_{t}^{w}=\mathbf{a}_{t}^{w} \\\dot{\mathbf{q}}_{w b_{t}}=\mathbf{q}_{w b_{t}} \otimes\left[\begin{array}{c}0 \\\frac{1}{2} \boldsymbol{\omega}^{b_{t}}\end{array}\right]\end{array}\]</span></p><p>基于上述微分方程，可以得到连续时间下的状态传播方程：</p><p><span class="math display">\[\mathbf{p}_{w b_{j}}=\mathbf{p}_{w b_{i}}+\mathbf{v}_{i}^{w} \Delta t+\iint_{t \in[i, j]}\left(\mathbf{q}_{w b_{t}} \mathbf{a}^{b_{t}}-\mathbf{g}^{w}\right) \delta t^{2}\]</span></p><p><span class="math display">\[\mathbf{v}_{j}^{w}=\mathbf{v}_{i}^{w}+\int_{t \in[i, j]}\left(\mathbf{q}_{w b_{t}} \mathbf{a}^{b_{t}}-\mathbf{g}^{w}\right) \delta t\]</span></p><p><span class="math display">\[\mathbf{q}_{w b_{j}}=\int_{t \in[i, j]} \mathbf{q}_{w b_{t}} \otimes\left[\begin{array}{c}0 \\ \frac{1}{2} \boldsymbol{\omega}^{b_{t}}\end{array}\right] \delta t\]</span></p><p>由于上面三个公式都与前一个时刻的状态有关，因此需要进行变换：</p><p>根据<span class="math inline">\(\mathbf{q}_{w b_{t}}=\mathbf{q}_{w b_{i}} \otimes \mathbf{q}_{b_{i} b_{t}}\)</span>，即可将上面三个公式中的前一个时刻的位姿项提取出来：</p><p><span class="math display">\[\mathbf{p}_{w b_{j}}=\mathbf{p}_{w b_{i}}+\mathbf{v}_{i}^{w} \Delta t-\frac{1}{2} \mathbf{g}^{w} \Delta t^{2}+\mathbf{q}_{w b_{i}} \underbrace{\iint_{t \in[i, j]}\left(\mathbf{q}_{b_{i} b_{t}} \mathbf{a}^{b_{t}}\right) \delta t^{2}}_{\mathbf{\alpha}}\]</span></p><p><span class="math display">\[\mathbf{v}_{j}^{w}=\mathbf{v}_{i}^{w}-\mathbf{g}^{w} \Delta t+\mathbf{q}_{w b_{i}}\underbrace{\int_{t \in[i, j]}\left(\mathbf{q}_{b_{i} b_{t}} \mathbf{a}^{b_{t}}\right) \delta t}_{\mathbf{\beta}}\]</span></p><p><span class="math display">\[\mathbf{q}_{w b_{j}}=\mathbf{q}_{w b_{i}}\underbrace{\int_{t \in[i, j]} \mathbf{q}_{b_{i} b_{t}} \otimes\left[\begin{array}{c}0 \\ \frac{1}{2} \boldsymbol{\omega}^{b_{t}}\end{array}\right] \delta t}_{\mathbf{\gamma}}\]</span></p><p>将上式的积分项分别提取出来，有：</p><p><span class="math display">\[\boldsymbol{\alpha}_{b_{i} b_{j}}=\iint_{t \in[i, j]}\left(\mathbf{q}_{b_{i} b_{t}} \mathbf{a}^{b_{t}}\right) \delta t^{2}\]</span></p><p><span class="math display">\[\boldsymbol{\beta}_{b_{i} b_{j}}=\int_{t \in[i, j]}\left(\mathbf{q}_{b_{i} b_{t}} \mathbf{a}^{b_{t}}\right) \delta t\]</span></p><p><span class="math display">\[\mathbf{q}_{b_{i} b_{j}}=\int_{t \in[i, j]} \mathbf{q}_{b_{i} b_{t}} \otimes\left[    \begin{array}{c}    0 \\     \frac{1}{2} \boldsymbol{\omega}^{b_{t}}    \end{array}\right] \delta t\]</span></p><p>由于上面的推导都是基于连续时间下的，在实际使用中，通常使用离散形式计算，采用中值积分法：</p><p><span class="math display">\[\boldsymbol{\omega}=\frac{1}{2}\left[\left(\boldsymbol{\omega}^{b_{k}}-\mathbf{b}_{k}^{g}\right)+\left(\boldsymbol{\omega}^{b_{k+1}}-\mathbf{b}_{k}^{g}\right)\right]\]</span></p><p><span class="math display">\[\mathbf{a}=\frac{1}{2}\left[\mathbf{q}_{b_{i} b_{k}}\left(\mathbf{a}^{b_{k}}-\mathbf{b}_{k}^{a}\right)+\mathbf{q}_{b_{i} b_{k+1}}\left(\mathbf{a}^{b_{k+1}}-\mathbf{b}_{k}^{a}\right)\right]\]</span></p><p>那么预积分量<span class="math inline">\(\boldsymbol{\alpha}_{b_{i} b_{j}},\boldsymbol{\beta}_{b_{i} b_{j}},\mathbf{q}_{b_{i} b_{j}}\)</span>可以通过迭代计算得到，当新的一帧imu数据到达时，计算该imu预积分的第k+1次迭代，有：</p><p><span class="math display">\[\boldsymbol{\alpha}_{b_{i} b_{k+1}}=\boldsymbol{\alpha}_{b_{i} b_{k}}+\boldsymbol{\beta}_{b_{i} b_{k}} \delta t+\frac{1}{2} \mathbf{a} \delta t^{2}\]</span></p><p><span class="math display">\[\boldsymbol{\beta}_{b_{i} b_{k+1}}=\boldsymbol{\beta}_{b_{i} b_{k}}+\mathbf{a} \delta t\]</span></p><p><span class="math display">\[\mathbf{q}_{b_{i} b_{k+1}}=\mathbf{q}_{b_{i} b_{k}} \otimes\left[    \begin{array}{c}    1 \\     \frac{1}{2} \boldsymbol{\omega} \delta t    \end{array}\right]\]</span></p><p>预积分量计算完成</p><p><strong>基于预积分量</strong>的导航状态更新公式为：</p><p><span class="math display">\[\left[    \begin{array}{c}        \mathbf{p}_{w b_{j}} \\         \mathbf{v}_{j}^{w} \\         \mathbf{q}_{w b_{j}} \\         \mathbf{b}_{j}^{a} \\         \mathbf{b}_{j}^{g}    \end{array}\right]=\left[    \begin{array}{c}        \mathbf{p}_{w b_{i}}+\mathbf{v}_{i}^{w} \Delta t-\frac{1}{2} \mathbf{g}^{w} \Delta t^{2}+\mathbf{q}_{w b_{i}} \boldsymbol{\alpha}_{b_{i} b_{j}} \\         \mathbf{v}_{i}^{w}-\mathbf{g}^{w} \Delta t+\mathbf{q}_{w b_{i}} \boldsymbol{\beta}_{b_{i} b_{j}} \\         \mathbf{q}_{w b_{i}} \mathbf{q}_{b_{i} b_{j}} \\         \mathbf{b}_{i}^{a} \\         \mathbf{b}_{i}^{g}    \end{array}\right]\]</span></p><blockquote><p>此中，陀螺仪和加速度计的零偏不变是认为在这个预积分中，由于时间很短，bias基本没有变化。然而，在整个系统中，其实是认为bias在缓慢变化的，因此，陀螺仪加速度计的模型为： <img src="http://s1.nsloop.com:59080/images/2021/02/23/20210223212141.png"></p></blockquote><h1 id="预积分更新">预积分更新</h1><p>从上面的推导可以发现，预积分量中包含了bias，而在后续的优化过程中，bias作为待优化状态量会随优化而发生改变，因此，预积分量应该随之更新，为了避免完全重新计算预积分，一个技巧是把预积分结果在bias处进行泰勒展开，通过线性近似得到更新的预积分：</p><p><span class="math display">\[\boldsymbol{\alpha}_{b_{i} b_{j}}=\overline{\boldsymbol{\alpha}}_{b_{i} b_{j}}+\mathbf{J}_{b_{i}^{a}}^{\alpha} \boldsymbol{b}_{i}^{a}+\mathbf{J}_{b_{i}^{g}}^{\alpha} \delta \mathbf{b}_{i}^{g}\]</span></p><p><span class="math display">\[\boldsymbol{\beta}_{b_{i} b_{j}}=\overline{\boldsymbol{\beta}}_{b_{i} b_{j}}+\mathbf{J}_{b_{i}^{a}}^{\beta} \delta \mathbf{b}_{i}^{a}+\mathbf{J}_{b_{i}^{g}}^{\beta} \delta \mathbf{b}_{i}^{g}\]</span></p><p><span class="math display">\[\mathbf{q}_{b_{i} b_{j}}=\overline{\mathbf{q}}_{b_{i} b_{j}} \otimes\left[    \begin{array}{c}        1 \\         \frac{1}{2} \mathbf{J}_{b_{i}^{g}}^{q} \delta \mathbf{b}_{i}^{g}    \end{array}\right]\]</span></p><p>其中，（详细计算见后续）</p><p><span class="math display">\[\begin{aligned} \mathbf{J}_{b_{i}^{a}}^{\alpha} &amp;=\frac{\partial \boldsymbol{\alpha}_{b_{i} b_{j}}}{\partial \delta \mathbf{b}_{i}^{a}} \\ \mathbf{J}_{b_{i}^{g}}^{\alpha} &amp;=\frac{\partial \boldsymbol{\alpha}_{b_{i} b_{j}}}{\partial \delta \mathbf{b}_{i}^{g}} \\ \mathbf{J}_{b_{i}^{a}}^{\beta} &amp;=\frac{\partial \boldsymbol{\beta}_{b_{i} b_{j}}}{\partial \delta \mathbf{b}_{i}^{a}} \\ \mathbf{J}_{b_{i}^{g}}^{\beta} &amp;=\frac{\partial \beta_{b_{i} b_{j}}}{\partial \delta \mathbf{b}_{i}^{g}} \\ \mathbf{J}_{b_{i}^{g}}^{q} &amp;=\frac{\mathbf{q}_{i_{i} b_{j}}}{\partial \mathbf{b}_{i}^{g}} \end{aligned}\]</span></p><h1 id="预积分误差协方差计算">预积分(误差)协方差计算</h1><p>由于预积分量是由imu数据迭代计算得到的，然而imu单次测量包含噪声，时间越长，预积分量越不准确，因此，需要计算对应的协方差来表示其不确定性，方差计算公式如下：</p><p><span class="math display">\[\boldsymbol{P}_{i, k+1}=\mathbf{F}_{k} \boldsymbol{P}_{i, k} \mathbf{F}_{k}^{\top}+\mathbf{G}_{k} \boldsymbol{Q} \mathbf{G}_{k}^{\top}\]</span></p><blockquote><p>注意：上式的<span class="math inline">\(\mathbf{F}_{k},\mathbf{G}_{k}\)</span>是离散时间下的状态转移矩阵</p></blockquote><p>下面需要求关于预积分误差的微分方程：</p><p>已知连续时间下的微分方程形式为：</p><p><span class="math display">\[\dot{\boldsymbol{X}}=\boldsymbol{F}_{t} \boldsymbol{X}+\boldsymbol{G}_{t} \boldsymbol{N}\]</span></p><p>其中，</p><p><span class="math display">\[\boldsymbol{X}=\left[    \begin{array}{l}    \delta \boldsymbol{\alpha}_{t}^{b_{k}} \\     \delta \boldsymbol{\theta}_{t}^{b_{k}} \\     \delta \boldsymbol{\beta}_{t}^{b_{k}} \\     \delta \boldsymbol{b}_{a_{t}} \\     \delta \boldsymbol{b}_{w_{t}}    \end{array}\right]\]</span></p><p><span class="math display">\[\boldsymbol{N}=\left[    \begin{array}{l}    \boldsymbol{n}_{a} \\    \boldsymbol{n}_{w} \\    \boldsymbol{n}_{b_{a}} \\     \boldsymbol{n}_{b_{w}}    \end{array}\right]\]</span></p><h2 id="delta-dottheta_tb_k微分方程推导"><span class="math inline">\(\delta \dot{\theta}_t^{b_k}\)</span>微分方程推导</h2><p>符号简化：<span class="math inline">\(\delta \dot{\theta}_t^{b_k} \rightarrow \delta \dot \theta\)</span></p><ol type="1"><li>写出不考虑误差的微分方程</li></ol><p><span class="math display">\[\dot{\boldsymbol{q}}_{t}=\frac{1}{2} \boldsymbol{q}_{t} \otimes\left[    \begin{array}{c}    0 \\     \boldsymbol{\omega}_{t}-\boldsymbol{b}_{\omega_{t}}    \end{array}\right]\]</span></p><ol start="2" type="1"><li>写出考虑误差的微分方程</li></ol><p><span class="math display">\[\dot{\tilde{\boldsymbol{q}}}_{t}=\frac{1}{2} \tilde{\boldsymbol{q}}_{t} \otimes\left[    \begin{array}{c}    0 \\     \tilde{\boldsymbol{\omega}}_{t}-\tilde{\boldsymbol{b}}_{\omega_{t}}    \end{array}\right]\]</span></p><ol start="3" type="1"><li>写出带有误差的参数与理想真实值之间的关系</li></ol><p><span class="math display">\[\tilde{\boldsymbol{q}}_{t}=\boldsymbol{q}_{t} \otimes \delta \boldsymbol{q}\]</span></p><p><span class="math display">\[\tilde{\boldsymbol{\omega}}_{t}=\boldsymbol{\omega}_{t}+\boldsymbol{n}_{\omega}\]</span></p><p><span class="math display">\[\tilde{\boldsymbol{b}}_{\omega_{t}}=\boldsymbol{b}_{\omega_{t}}+\delta \boldsymbol{b}_{\omega_{t}}\]</span></p><p>其中，<span class="math inline">\(\delta \theta\)</span>是计算坐标系与真实导航坐标系的偏差 或者 在body系与计算误差body系之间的偏差</p><p><span class="math display">\[\delta \boldsymbol{q}=\left[    \begin{array}{c}    \cos \left(\frac{|\delta \theta|}{2}\right) \\     \frac{\delta \boldsymbol{\theta}}{|\delta \theta|} \sin \left(\frac{|\delta \theta|}{2}\right)    \end{array}\right]     \approx\left[    \begin{array}{c}    1 \\     \frac{\delta \boldsymbol{\theta}}{2}    \end{array}\right]\]</span></p><ol start="4" type="1"><li>将误差值与理想真实值的关系代入(2)</li></ol><p><span class="math display">\[\left(\boldsymbol{q}_{t} \dot{\otimes} \delta \boldsymbol{q}\right)=\frac{1}{2} \boldsymbol{q}_{t} \otimes \delta \boldsymbol{q} \otimes\left[    \begin{array}{c}    0 \\     \boldsymbol{\omega}_{t}+\boldsymbol{n}_{\omega}-\boldsymbol{b}_{\omega_{t}}-\delta \boldsymbol{b}_{\omega_{t}}    \end{array}\right]\]</span></p><p>其中，</p><p><span class="math display">\[\left(\boldsymbol{q}_{t} \dot{\otimes} \delta \boldsymbol{q}\right)=\dot{\boldsymbol{q}}_{t} \otimes \delta \boldsymbol{q}+\boldsymbol{q}_{t} \otimes \delta \dot{\boldsymbol{q}}\]</span></p><ol start="5" type="1"><li>把(1)中的关系代入(4)</li></ol><p><span class="math display">\[\begin{aligned}\left(\boldsymbol{q}_{t} \dot{\otimes} \delta \boldsymbol{q}\right) &amp;= \frac{1}{2} \boldsymbol{q}_{t} \otimes \delta \boldsymbol{q} \otimes\left[\begin{array}{c}0 \\ \boldsymbol{\omega}_{t}+\boldsymbol{n}_{\omega}-\boldsymbol{b}_{\omega_{t}}-\delta \boldsymbol{b}_{\omega_{t}}\end{array}\right] \\\dot{\boldsymbol{q}}_{t} \otimes \delta \boldsymbol{q}+\boldsymbol{q}_{t} \otimes \delta \boldsymbol{q} &amp;= \\\frac{1}{2} \boldsymbol{q}_{t} \otimes\left[\begin{array}{c}0 \\ \boldsymbol{\omega}_{t}-\boldsymbol{b}_{\omega_{t}}\end{array}\right] \otimes \delta \boldsymbol{q}+\boldsymbol{q}_{t} \otimes \dot{\delta \boldsymbol{q}} &amp;=\end{aligned}\]</span></p><ol start="6" type="1"><li>化简</li></ol><p>(5)两边同时左乘<span class="math inline">\((\boldsymbol{q}_t)^{-1}\)</span>，然后移项得到：</p><p><span class="math display">\[\delta \dot{\boldsymbol{q}}=\frac{1}{2} \delta \boldsymbol{q} \otimes\left[    \begin{array}{c}    0 \\     \boldsymbol{\omega}_{t}+\boldsymbol{n}_{\omega}-\boldsymbol{b}_{\omega_{t}}-\delta \boldsymbol{b}_{\omega_{t}}    \end{array}\right]-\frac{1}{2}\left[    \begin{array}{c}    0 \\     \boldsymbol{\omega}_{t}-\boldsymbol{b}_{\omega_{t}}    \end{array}\right] \otimes \delta \boldsymbol{q}\]</span></p><blockquote><p>根据四元数乘法性质，可以将四元数乘法转换成矩阵与向量相乘： <img src="http://s1.nsloop.com:59080/images/2021/02/26/20210226173136.png"></p></blockquote><p>因此，可得：</p><p><span class="math display">\[\begin{aligned}\delta \dot{\boldsymbol{q}}&amp;=\frac{1}{2}\left[    \begin{array}{c}0 \\     \boldsymbol{\omega}_{1}    \end{array}\right]_{R} \delta \boldsymbol{q}-\frac{1}{2}\left[    \begin{array}{c}0 \\     \boldsymbol{\omega}_{2}    \end{array}\right]_{L} \delta \boldsymbol{q} \\&amp;=\frac{1}{2}\left[    \begin{array}{cc}    0 &amp; \left(\boldsymbol{\omega}_{2}-\boldsymbol{\omega}_{1}\right)^{T} \\     \left(\boldsymbol{\omega}_{1}-\boldsymbol{\omega}_{2}\right) &amp; -\left[\boldsymbol{\omega}_{1}+\boldsymbol{\omega}_{2}\right]_{\times}    \end{array}\right] \delta \boldsymbol{q}\end{aligned}\]</span></p><p>其中,</p><p><span class="math display">\[\boldsymbol{\omega}_{1}=\boldsymbol{\omega}_{t}+\boldsymbol{n}_{\omega}-\boldsymbol{b}_{\omega_{t}}-\delta \boldsymbol{b}_{\omega_{t}}\]</span></p><p><span class="math display">\[\boldsymbol{\omega}_{2}=\boldsymbol{\omega}_{t}-\boldsymbol{b}_{\omega_{t}}\]</span></p><p>又因为：</p><p><span class="math display">\[\delta \dot{\boldsymbol{q}}=\left[    \begin{array}{l}    0 \\     \frac{\delta \dot{\theta}}{2}    \end{array}\right]\]</span></p><p>可以得到关于<span class="math inline">\(\delta \dot{\theta}\)</span>的方程：</p><p><span class="math display">\[\begin{aligned}\delta \dot{\boldsymbol{\theta}}&amp;=-\left[\boldsymbol{\omega}_{1}+\boldsymbol{\omega}_{2}\right] \times \frac{\delta \boldsymbol{\theta}}{2}+\left(\boldsymbol{\omega}_{1}-\boldsymbol{\omega}_{2}\right)\\&amp;=-\left[2 \boldsymbol{\omega}_{t}+\boldsymbol{n}_{\omega}-2 \boldsymbol{b}_{\omega_{t}}-\delta \boldsymbol{b}_{\omega_{t}}\right]_\times \frac{\delta \boldsymbol{\theta}}{2}+\boldsymbol{n}_{\omega}-\delta \boldsymbol{b}_{\omega_{t}}\end{aligned}\]</span></p><p>忽略上式中的二阶小项，可得<span class="math inline">\(\delta \dot{\theta}_t^{b_k}\)</span>微分方程</p><p><span class="math display">\[\delta \dot{\boldsymbol{\theta}}=-\left[\boldsymbol{\omega}_{t}-\boldsymbol{b}_{\omega_{t}}\right]_{\times} \delta \boldsymbol{\theta}+\boldsymbol{n}_{\omega}-\delta \boldsymbol{b}_{\omega_{t}}\]</span></p><h2 id="delta-dotbeta_tb_k微分方程推导"><span class="math inline">\(\delta \dot{\beta}_t^{b_k}\)</span>微分方程推导</h2><p>符号简化：<span class="math inline">\(\delta \dot{\beta}_t^{b_k} \rightarrow \delta \dot \beta\)</span></p><ol type="1"><li>写出不考虑误差的微分方程</li></ol><p><span class="math display">\[\dot{\boldsymbol{\beta}}=\boldsymbol{R}_{t}\left(\boldsymbol{a}_{t}-\boldsymbol{b}_{a_{t}}\right)\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{R}_{t}\)</span>表示载体姿态</p><ol start="2" type="1"><li>写出考虑误差的微分方程</li></ol><p><span class="math display">\[\dot{\tilde{\boldsymbol{\beta}}}=\tilde{\boldsymbol{R}}_{t}\left(\tilde{\boldsymbol{a}}_{t}-\tilde{\boldsymbol{b}}_{a_{t}}\right)\]</span></p><ol start="3" type="1"><li>写出带有误差的参数与理想真实值之间的关系</li></ol><p><span class="math display">\[\tilde{\boldsymbol{\beta}}=\boldsymbol{\beta}+\delta \boldsymbol{\beta}\]</span></p><p><span class="math display">\[\tilde{\boldsymbol{a}}_{t}=\boldsymbol{a}_{t}+\boldsymbol{n}_{a}\]</span></p><p><span class="math display">\[\tilde{\boldsymbol{b}}_{a_{t}}=\boldsymbol{b}_{a_{t}}+\delta \boldsymbol{b}_{a_{t}}\]</span></p><p><span class="math display">\[\begin{aligned}\tilde{\boldsymbol{R}}_{t}&amp;=\boldsymbol{R}_{t} \exp \left([\delta \boldsymbol{\theta}]_{\times}\right) \\&amp;=\boldsymbol{R}_{t}\left(\boldsymbol{I}+[\delta \boldsymbol{\theta}]_{\times}\right)\end{aligned}\]</span></p><blockquote><p>关于姿态与理想真实值的关系： 因为 <span class="math inline">\(\delta \boldsymbol{\theta}\)</span>表示的是在载体姿态上的误差，直接右乘即可</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;什么是预积分&quot;&gt;什么是预积分&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://s1.nsloop.com:59080/images/2021/02/19/20210219155712.png&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;即对两个关键帧之间的imu
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Zotero导入CNKI文献</title>
    <link href="http://yoursite.com/2021/02/11/Zotero%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2021/02/11/Zotero%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</id>
    <published>2021-02-11T09:05:33.000Z</published>
    <updated>2021-05-31T09:01:21.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="why-zotero">Why Zotero?</h1><ul><li>可以一键导入并下载CNKI文献(pdf)</li><li>可以抓取硕士论文目录</li><li>与Word兼容性良好(安装插件即可实现gb7714等参考文献引用格式)</li><li>……</li></ul><h1 id="zotero以及浏览器插件安装">Zotero以及浏览器插件安装</h1><p>参考官网即可： https://www.zotero.org/</p><p>下载地址： https://www.zotero.org/download/</p><h1 id="cnki中文插件安装">CNKI中文插件安装</h1><p>参考： https://github.com/l0o0/translators_CN</p><h2 id="zotero-translator_cn-安装">Zotero translator_CN 安装</h2><p><strong>1 下载网页翻译器(web translator)文件</strong></p><p><img src="https://s1.ax1x.com/2020/08/19/dlKNRK.png"></p><p><strong>2 解压下载的压缩包，找到</strong>translators<strong>目录，将目录中的文件复制到 Zotero 的 translators 目录</strong></p><p><img src="https://s1.ax1x.com/2020/09/07/wnDwlV.jpg"><br><img src="https://s1.ax1x.com/2020/08/19/dlM36S.png"></p><blockquote><p>如果是macos系统，Zotero的目录就在用户主目录下</p></blockquote><p><strong>3 更新 translator 信息，Firefox 和 Chrome 浏览器操作类似。下面以 Firefox 为例</strong><br><img src="https://s1.ax1x.com/2020/08/19/dlQgKS.gif"></p><p>Chrome 浏览器按照下面信息找到更新按钮</p><p><img src="https://s1.ax1x.com/2020/08/19/dlKUxO.png"></p><p><strong>更新时请多点几下，根据我的经验，Chrome 浏览器更新比较快，Firefox 会比较慢</strong></p><p>如果你使用学校的 VPN 来登录知网，可以参考这个<a href="https://zhuanlan.zhihu.com/p/111857132" target="_blank" rel="noopener">链接</a>进行设置。设置过程不复杂，就是用特殊符号把网址中的字符替换掉。</p><h2 id="如何在zotero-connector-中添加中文姓名处理以及保留知网caj格式文件的设置"><span id="jump">🍇 如何在Zotero Connector 中添加中文姓名处理以及保留知网CAJ格式文件的设置</span></h2><p>需要特别注意的是，这里在 Zotero Connector 中添加的参数，只是方便控制的网页翻译器的数据抓取行为，限本页面列出的一些翻译器中起作用，并不影响其他翻译器和Zotero的其他功能。 添加的参数有：</p><ul><li><code>translators.zhnamesplit</code>，默认为true，抓取过程会拆分姓和名，如果想全并姓名，请设置为false</li><li><code>translators.CNKIPDF</code>,默认为true，~<del>下载知网上文章的PDF文件，如果想要下载学位论文的CAJ格式，请设置为false</del>~ (这个方法有点问题，建议直接下载pdf版本即可，后面会有利用插件添加硕士论文目录的部分)</li></ul><p>设置方法请参考下面：</p><p><img src="https://s1.ax1x.com/2020/08/19/dl1AyT.gif"></p><p>为防止设置错误，可以把参数名复制过去。设置完成后，请刷新网页，再重新抓取。如果你参数名写错了也没事，不会有什么问题，放着就好。</p><h2 id="使用方法">使用方法</h2><p><strong>1 打开<a href="https://www.cnki.net/" target="_blank" rel="noopener">知网</a></strong></p><p><img src="http://s1.nsloop.com:59080/images/2021/02/11/20210211211932.png"></p><p><strong>2 点击插件，选择需要导入的文献</strong></p><p><img src="http://s1.nsloop.com:59080/images/2021/02/11/20210211212044.png"></p><p><strong>3 确认即可导入Zotero</strong></p><p><img src="http://s1.nsloop.com:59080/images/2021/02/11/20210211212218.png"></p><p><strong>4 双击即可打开对应的pdf</strong></p><p><img src="http://s1.nsloop.com:59080/images/2021/02/11/20210211212359.png"></p><blockquote><p>由于直接下载的硕士论文pdf版本没有目录，因此下面通过安装插件的方法来解决这个问题</p></blockquote><h1 id="jasminum---茉莉花插件安装使用">Jasminum - 茉莉花插件安装使用</h1><p>插件官方地址：https://github.com/l0o0/jasminum</p><h2 id="安装步骤">安装步骤</h2><p><strong>1 Jasminum插件</strong></p><p>下载最新的<a href="https://github.com/l0o0/jasminum/releases/latest" target="_blank" rel="noopener">xpi</a>文件进行安装，安装方法：打开 Zotero -&gt; 工具 -&gt; 插件 -&gt; 右上小齿轮图标 -&gt; Install Add-on From File ... -&gt; 选择下载好的xpi文件。</p><p><strong>2 PDFtk server插件</strong></p><p>PDFtk server，该书签添加工具有 Windows， Linux 和 Mac，请根据自己的系统下载对应的版本进行安装，并在选项中设置好对应的目录。<a href="https://www.pdflabs.com/tools/pdftk-server/" target="_blank" rel="noopener">PDFtk server 下载链接</a></p><blockquote><p>官网：https://www.pdflabs.com/tools/pdftk-server/<br><font color="red">After installation, open a Terminal, type pdftk and press Return. Pdftk will respond by displaying brief usage information(注意！安装后请试试这一步，出现使用说明说明安装成功)</font>. Access pdftk documenation by running man pdftk.</p></blockquote><p><strong>Mac 用户</strong>（感谢<span class="citation" data-cites="GuokaiLiu">[@GuokaiLiu]</span>(https://github.com/GuokaiLiu)同学在 <a href="https://github.com/l0o0/jasminum/issues/7#issuecomment-706448964" target="_blank" rel="noopener">issue</a> 中的补充） macos(10.15)用户： 下载：https://www.pdflabs.com/tools/pdftk-the-pdf-toolkit/pdftk_server-2.02-mac_osx-10.11-setup.pkg</p><blockquote><p>关于Mac系统的插件路径配置（我没有进行这一步，有需要的再看）<br>配置方法： 打开Zotero-&gt;首选项-&gt;茉莉花<br><img src="http://s1.nsloop.com:59080/images/2021/02/11/20210211212937.png"> 路径：<code>/opt/pdflabs/pdftk/</code>. （该路径默认对外隐藏无法选取） 选择路径的技巧：<code>shift+command+G</code>: 输入：<code>/opt/pdflabs/pdftk/</code>，选择<code>bin</code>确认</p></blockquote><h2 id="使用方法同上">使用方法（同上）</h2><p>稍微等一会，插件就抓取到目录了</p><p><img src="http://s1.nsloop.com:59080/images/2021/02/11/20210211213244.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;why-zotero&quot;&gt;Why Zotero?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;可以一键导入并下载CNKI文献(pdf)&lt;/li&gt;
&lt;li&gt;可以抓取硕士论文目录&lt;/li&gt;
&lt;li&gt;与Word兼容性良好(安装插件即可实现gb7714等参考文献引用格式)&lt;/li&gt;
&lt;li
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Eigen-Operation</title>
    <link href="http://yoursite.com/2021/02/01/Eigen%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2021/02/01/Eigen%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</id>
    <published>2021-02-01T03:05:32.000Z</published>
    <updated>2021-02-02T02:01:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="eigen常用操作eigen-cheatsheet">Eigen常用操作|Eigen Cheatsheet</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; A simple quickref for Eigen. Add anything that&#39;s missing.</span><br><span class="line">&#x2F;&#x2F; Main author: Keir Mierle</span><br></pre></td></tr></table></figure><h2 id="包含头文件">1. 包含头文件</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Eigen/Dense&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="矩阵向量声明">2. 矩阵、向量声明</h2><h3 id="矩阵声明">2.1 矩阵声明</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Matrix&lt;<span class="keyword">double</span>, <span class="number">3</span>, <span class="number">3</span>&gt; A;               <span class="comment">// Fixed rows and cols. Same as Matrix3d.</span></span><br><span class="line">Matrix&lt;<span class="keyword">double</span>, <span class="number">3</span>, Dynamic&gt; B;         <span class="comment">// Fixed rows, dynamic cols.</span></span><br><span class="line">Matrix&lt;<span class="keyword">double</span>, Dynamic, Dynamic&gt; C;   <span class="comment">// Full dynamic. Same as MatrixXd.</span></span><br><span class="line">Matrix&lt;<span class="keyword">double</span>, <span class="number">3</span>, <span class="number">3</span>, RowMajor&gt; E;     <span class="comment">// Row major; default is column-major.</span></span><br><span class="line">Matrix3f P, Q, R;                     <span class="comment">// 3x3 float matrix.</span></span><br></pre></td></tr></table></figure><h3 id="向量声明">2.2 向量声明</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Vector3f x, y, z;                     <span class="comment">// 3x1 float matrix.</span></span><br><span class="line">RowVector3f a, b, c;                  <span class="comment">// 1x3 float matrix.</span></span><br><span class="line">VectorXd v;                           <span class="comment">// Dynamic column vector of doubles</span></span><br><span class="line"><span class="keyword">double</span> s;</span><br></pre></td></tr></table></figure><h2 id="基础操作">3. 基础操作</h2><h3 id="计算大小">3.1 计算大小</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Basic usage</span></span><br><span class="line"><span class="comment">// Eigen          // Matlab           // comments</span></span><br><span class="line">x.<span class="built_in">size</span>()          <span class="comment">// length(x)        // vector size</span></span><br><span class="line">C.rows()          <span class="comment">// size(C,1)        // number of rows</span></span><br><span class="line">C.cols()          <span class="comment">// size(C,2)        // number of columns</span></span><br></pre></td></tr></table></figure><h3 id="访问元素">3.2 访问元素</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x(i)              <span class="comment">// x(i+1)           // Matlab is 1-based</span></span><br><span class="line">C(i,j)            <span class="comment">// C(i+1,j+1)       //</span></span><br></pre></td></tr></table></figure><h3 id="改变大小">3.3 改变大小</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A.resize(<span class="number">4</span>, <span class="number">4</span>);   <span class="comment">// Runtime error if assertions are on.</span></span><br><span class="line">B.resize(<span class="number">4</span>, <span class="number">9</span>);   <span class="comment">// Runtime error if assertions are on.</span></span><br><span class="line">A.resize(<span class="number">3</span>, <span class="number">3</span>);   <span class="comment">// Ok; size didn't change.</span></span><br><span class="line">B.resize(<span class="number">3</span>, <span class="number">9</span>);   <span class="comment">// Ok; only dynamic cols changed.</span></span><br></pre></td></tr></table></figure><h3 id="矩阵赋值">3.4 矩阵赋值</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A &lt;&lt; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>,     <span class="comment">// Initialize A. The elements can also be</span></span><br><span class="line">     <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>,     <span class="comment">// matrices, which are stacked along cols</span></span><br><span class="line">     <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>;     <span class="comment">// and then the rows are stacked.</span></span><br><span class="line">B &lt;&lt; A, A, A;     <span class="comment">// B is three horizontally stacked A's.</span></span><br><span class="line">A.<span class="built_in">fill</span>(<span class="number">10</span>);       <span class="comment">// Fill A with all 10's.</span></span><br></pre></td></tr></table></figure><h2 id="特殊矩阵">4. 特殊矩阵</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Eigen                                    // Matlab</span></span><br><span class="line">MatrixXd::Identity(rows,cols)               <span class="comment">// eye(rows,cols)</span></span><br><span class="line">C.setIdentity(rows,cols)                    <span class="comment">// C = eye(rows,cols)</span></span><br><span class="line">MatrixXd::Zero(rows,cols)                   <span class="comment">// zeros(rows,cols)</span></span><br><span class="line">C.setZero(rows,cols)                        <span class="comment">// C = zeros(rows,cols)</span></span><br><span class="line">MatrixXd::Ones(rows,cols)                   <span class="comment">// ones(rows,cols)</span></span><br><span class="line">C.setOnes(rows,cols)                        <span class="comment">// C = ones(rows,cols)</span></span><br><span class="line">MatrixXd::Random(rows,cols)                 <span class="comment">// rand(rows,cols)*2-1            // MatrixXd::Random returns uniform random numbers in (-1, 1).</span></span><br><span class="line">C.setRandom(rows,cols)                      <span class="comment">// C = rand(rows,cols)*2-1</span></span><br><span class="line">VectorXd::LinSpaced(<span class="built_in">size</span>,low,high)          <span class="comment">// linspace(low,high,size)'</span></span><br><span class="line">v.setLinSpaced(<span class="built_in">size</span>,low,high)               <span class="comment">// v = linspace(low,high,size)'</span></span><br><span class="line">VectorXi::LinSpaced(((hi-low)/<span class="built_in">step</span>)+<span class="number">1</span>,      <span class="comment">// low:step:hi</span></span><br><span class="line">                    low,low+<span class="built_in">step</span>*(<span class="built_in">size</span><span class="number">-1</span>))  <span class="comment">//</span></span><br></pre></td></tr></table></figure><h2 id="矩阵元素提取与替换">5. 矩阵元素提取与替换</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Matrix slicing and blocks. All expressions listed here are read/write.</span></span><br><span class="line"><span class="comment">// Templated size versions are faster. Note that Matlab is 1-based (a size N</span></span><br><span class="line"><span class="comment">// vector is x(1)...x(N)).</span></span><br><span class="line"><span class="comment">/******************************************************************************/</span></span><br><span class="line"><span class="comment">/*                  PLEASE HELP US IMPROVING THIS SECTION                     */</span></span><br><span class="line"><span class="comment">/* Eigen 3.4 supports a much improved API for sub-matrices, including,        */</span></span><br><span class="line"><span class="comment">/* slicing and indexing from arrays:                                          */</span></span><br><span class="line"><span class="comment">/* http://eigen.tuxfamily.org/dox-devel/group__TutorialSlicingIndexing.html   */</span></span><br><span class="line"><span class="comment">/******************************************************************************/</span></span><br><span class="line"><span class="comment">// Eigen                           // Matlab</span></span><br><span class="line">x.head(n)                          <span class="comment">// x(1:n)</span></span><br><span class="line">x.head&lt;n&gt;()                        <span class="comment">// x(1:n)</span></span><br><span class="line">x.tail(n)                          <span class="comment">// x(end - n + 1: end)</span></span><br><span class="line">x.tail&lt;n&gt;()                        <span class="comment">// x(end - n + 1: end)</span></span><br><span class="line">x.segment(i, n)                    <span class="comment">// x(i+1 : i+n)</span></span><br><span class="line">x.segment&lt;n&gt;(i)                    <span class="comment">// x(i+1 : i+n)</span></span><br><span class="line">P.block(i, j, rows, cols)          <span class="comment">// P(i+1 : i+rows, j+1 : j+cols)</span></span><br><span class="line">P.block&lt;rows, cols&gt;(i, j)          <span class="comment">// P(i+1 : i+rows, j+1 : j+cols)</span></span><br><span class="line">P.row(i)                           <span class="comment">// P(i+1, :)</span></span><br><span class="line">P.col(j)                           <span class="comment">// P(:, j+1)</span></span><br><span class="line">P.leftCols&lt;cols&gt;()                 <span class="comment">// P(:, 1:cols)</span></span><br><span class="line">P.leftCols(cols)                   <span class="comment">// P(:, 1:cols)</span></span><br><span class="line">P.middleCols&lt;cols&gt;(j)              <span class="comment">// P(:, j+1:j+cols)</span></span><br><span class="line">P.middleCols(j, cols)              <span class="comment">// P(:, j+1:j+cols)</span></span><br><span class="line">P.rightCols&lt;cols&gt;()                <span class="comment">// P(:, end-cols+1:end)</span></span><br><span class="line">P.rightCols(cols)                  <span class="comment">// P(:, end-cols+1:end)</span></span><br><span class="line">P.topRows&lt;rows&gt;()                  <span class="comment">// P(1:rows, :)</span></span><br><span class="line">P.topRows(rows)                    <span class="comment">// P(1:rows, :)</span></span><br><span class="line">P.middleRows&lt;rows&gt;(i)              <span class="comment">// P(i+1:i+rows, :)</span></span><br><span class="line">P.middleRows(i, rows)              <span class="comment">// P(i+1:i+rows, :)</span></span><br><span class="line">P.bottomRows&lt;rows&gt;()               <span class="comment">// P(end-rows+1:end, :)</span></span><br><span class="line">P.bottomRows(rows)                 <span class="comment">// P(end-rows+1:end, :)</span></span><br><span class="line">P.topLeftCorner(rows, cols)        <span class="comment">// P(1:rows, 1:cols)</span></span><br><span class="line">P.topRightCorner(rows, cols)       <span class="comment">// P(1:rows, end-cols+1:end)</span></span><br><span class="line">P.bottomLeftCorner(rows, cols)     <span class="comment">// P(end-rows+1:end, 1:cols)</span></span><br><span class="line">P.bottomRightCorner(rows, cols)    <span class="comment">// P(end-rows+1:end, end-cols+1:end)</span></span><br><span class="line">P.topLeftCorner&lt;rows,cols&gt;()       <span class="comment">// P(1:rows, 1:cols)</span></span><br><span class="line">P.topRightCorner&lt;rows,cols&gt;()      <span class="comment">// P(1:rows, end-cols+1:end)</span></span><br><span class="line">P.bottomLeftCorner&lt;rows,cols&gt;()    <span class="comment">// P(end-rows+1:end, 1:cols)</span></span><br><span class="line">P.bottomRightCorner&lt;rows,cols&gt;()   <span class="comment">// P(end-rows+1:end, end-cols+1:end)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Of particular note is Eigen's swap function which is highly optimized.</span></span><br><span class="line"><span class="comment">// Eigen                           // Matlab</span></span><br><span class="line">R.row(i) = P.col(j);               <span class="comment">// R(i, :) = P(:, j)</span></span><br><span class="line">R.col(j1).swap(mat1.col(j2));      <span class="comment">// R(:, [j1 j2]) = R(:, [j2, j1])</span></span><br></pre></td></tr></table></figure><h2 id="矩阵操作">6. 矩阵操作</h2><h3 id="转置与旋转">6.1 转置与旋转</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Views, transpose, etc;</span></span><br><span class="line"><span class="comment">/******************************************************************************/</span></span><br><span class="line"><span class="comment">/*                  PLEASE HELP US IMPROVING THIS SECTION                     */</span></span><br><span class="line"><span class="comment">/* Eigen 3.4 supports a new API for reshaping:                                */</span></span><br><span class="line"><span class="comment">/* http://eigen.tuxfamily.org/dox-devel/group__TutorialReshape.html           */</span></span><br><span class="line"><span class="comment">/******************************************************************************/</span></span><br><span class="line"><span class="comment">// Eigen                           // Matlab</span></span><br><span class="line">R.adjoint()                        <span class="comment">// R'</span></span><br><span class="line">R.transpose()                      <span class="comment">// R.' or conj(R')       // Read-write</span></span><br><span class="line">R.diagonal()                       <span class="comment">// diag(R)               // Read-write</span></span><br><span class="line">x.asDiagonal()                     <span class="comment">// diag(x)</span></span><br><span class="line">R.transpose().colwise().reverse()  <span class="comment">// rot90(R)              // Read-write</span></span><br><span class="line">R.rowwise().reverse()              <span class="comment">// fliplr(R)</span></span><br><span class="line">R.colwise().reverse()              <span class="comment">// flipud(R)</span></span><br><span class="line">R.replicate(i,j)                   <span class="comment">// repmat(P,i,j)</span></span><br></pre></td></tr></table></figure><h3 id="矩阵运算">6.2 矩阵运算</h3><h4 id="基本算数运算">6.2.1 基本算数运算</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// All the same as Matlab, but matlab doesn't have *= style operators.</span></span><br><span class="line"><span class="comment">// Matrix-vector.  Matrix-matrix.   Matrix-scalar.</span></span><br><span class="line">y  = M*x;          R  = P*Q;        R  = P*s;</span><br><span class="line">a  = b*M;          R  = P - Q;      R  = s*P;</span><br><span class="line">a *= M;            R  = P + Q;      R  = P/s;</span><br><span class="line">                   R *= Q;          R  = s*P;</span><br><span class="line">                   R += Q;          R *= s;</span><br><span class="line">                   R -= Q;          R /= s;</span><br></pre></td></tr></table></figure><h4 id="点运算">6.2.2 点运算</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Vectorized operations on each element independently</span></span><br><span class="line"><span class="comment">// Eigen                       // Matlab</span></span><br><span class="line">R = P.cwiseProduct(Q);         <span class="comment">// R = P .* Q</span></span><br><span class="line">R = P.<span class="built_in">array</span>() * s.<span class="built_in">array</span>();     <span class="comment">// R = P .* s</span></span><br><span class="line">R = P.cwiseQuotient(Q);        <span class="comment">// R = P ./ Q</span></span><br><span class="line">R = P.<span class="built_in">array</span>() / Q.<span class="built_in">array</span>();     <span class="comment">// R = P ./ Q</span></span><br><span class="line">R = P.<span class="built_in">array</span>() + s.<span class="built_in">array</span>();     <span class="comment">// R = P + s</span></span><br><span class="line">R = P.<span class="built_in">array</span>() - s.<span class="built_in">array</span>();     <span class="comment">// R = P - s</span></span><br><span class="line">R.<span class="built_in">array</span>() += s;                <span class="comment">// R = R + s</span></span><br><span class="line">R.<span class="built_in">array</span>() -= s;                <span class="comment">// R = R - s</span></span><br><span class="line">R.<span class="built_in">array</span>() &lt; Q.<span class="built_in">array</span>();         <span class="comment">// R &lt; Q</span></span><br><span class="line">R.<span class="built_in">array</span>() &lt;= Q.<span class="built_in">array</span>();        <span class="comment">// R &lt;= Q</span></span><br><span class="line">R.cwiseInverse();              <span class="comment">// 1 ./ P</span></span><br><span class="line">R.<span class="built_in">array</span>().inverse();           <span class="comment">// 1 ./ P</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">sin</span>()                <span class="comment">// sin(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">cos</span>()                <span class="comment">// cos(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">pow</span>(s)               <span class="comment">// P .^ s</span></span><br><span class="line">R.<span class="built_in">array</span>().square()             <span class="comment">// P .^ 2</span></span><br><span class="line">R.<span class="built_in">array</span>().cube()               <span class="comment">// P .^ 3</span></span><br><span class="line">R.cwiseSqrt()                  <span class="comment">// sqrt(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">sqrt</span>()               <span class="comment">// sqrt(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">exp</span>()                <span class="comment">// exp(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">log</span>()                <span class="comment">// log(P)</span></span><br><span class="line">R.cwiseMax(P)                  <span class="comment">// max(R, P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">max</span>(P.<span class="built_in">array</span>())       <span class="comment">// max(R, P)</span></span><br><span class="line">R.cwiseMin(P)                  <span class="comment">// min(R, P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">min</span>(P.<span class="built_in">array</span>())       <span class="comment">// min(R, P)</span></span><br><span class="line">R.cwiseAbs()                   <span class="comment">// abs(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">abs</span>()                <span class="comment">// abs(P)</span></span><br><span class="line">R.cwiseAbs2()                  <span class="comment">// abs(P.^2)</span></span><br><span class="line">R.<span class="built_in">array</span>().abs2()               <span class="comment">// abs(P.^2)</span></span><br><span class="line">(R.<span class="built_in">array</span>() &lt; s).select(P,Q );  <span class="comment">// (R &lt; s ? P : Q)</span></span><br><span class="line">R = (Q.<span class="built_in">array</span>()==<span class="number">0</span>).select(P,R) <span class="comment">// R(Q==0) = P(Q==0)</span></span><br><span class="line">R = P.unaryExpr(ptr_fun(func)) <span class="comment">// R = arrayfun(func, P)   // with: scalar func(const scalar &amp;x);</span></span><br></pre></td></tr></table></figure><h4 id="矩阵函数">6.2.3 矩阵函数</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reductions.</span></span><br><span class="line"><span class="keyword">int</span> r, c;</span><br><span class="line"><span class="comment">// Eigen                  // Matlab</span></span><br><span class="line">R.minCoeff()              <span class="comment">// min(R(:))</span></span><br><span class="line">R.maxCoeff()              <span class="comment">// max(R(:))</span></span><br><span class="line">s = R.minCoeff(&amp;r, &amp;c)    <span class="comment">// [s, i] = min(R(:)); [r, c] = ind2sub(size(R), i);</span></span><br><span class="line">s = R.maxCoeff(&amp;r, &amp;c)    <span class="comment">// [s, i] = max(R(:)); [r, c] = ind2sub(size(R), i);</span></span><br><span class="line">R.sum()                   <span class="comment">// sum(R(:))</span></span><br><span class="line">R.colwise().sum()         <span class="comment">// sum(R)</span></span><br><span class="line">R.rowwise().sum()         <span class="comment">// sum(R, 2) or sum(R')'</span></span><br><span class="line">R.prod()                  <span class="comment">// prod(R(:))</span></span><br><span class="line">R.colwise().prod()        <span class="comment">// prod(R)</span></span><br><span class="line">R.rowwise().prod()        <span class="comment">// prod(R, 2) or prod(R')'</span></span><br><span class="line">R.trace()                 <span class="comment">// trace(R)</span></span><br><span class="line">R.all()                   <span class="comment">// all(R(:))</span></span><br><span class="line">R.colwise().all()         <span class="comment">// all(R)</span></span><br><span class="line">R.rowwise().all()         <span class="comment">// all(R, 2)</span></span><br><span class="line">R.any()                   <span class="comment">// any(R(:))</span></span><br><span class="line">R.colwise().any()         <span class="comment">// any(R)</span></span><br><span class="line">R.rowwise().any()         <span class="comment">// any(R, 2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Dot products, norms, etc.</span></span><br><span class="line"><span class="comment">// Eigen                  // Matlab</span></span><br><span class="line">x.norm()                  <span class="comment">// norm(x).    Note that norm(R) doesn't work in Eigen.</span></span><br><span class="line">x.squaredNorm()           <span class="comment">// dot(x, x)   Note the equivalence is not true for complex</span></span><br><span class="line">x.dot(y)                  <span class="comment">// dot(x, y)</span></span><br><span class="line">x.cross(y)                <span class="comment">// cross(x, y) Requires #include &lt;Eigen/Geometry&gt;</span></span><br></pre></td></tr></table></figure><h4 id="类型转换">6.2.4 类型转换</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//// Type conversion</span></span><br><span class="line"><span class="comment">// Eigen                  // Matlab</span></span><br><span class="line">A.cast&lt;<span class="keyword">double</span>&gt;();         <span class="comment">// double(A)</span></span><br><span class="line">A.cast&lt;<span class="keyword">float</span>&gt;();          <span class="comment">// single(A)</span></span><br><span class="line">A.cast&lt;<span class="keyword">int</span>&gt;();            <span class="comment">// int32(A)</span></span><br><span class="line">A.real();                 <span class="comment">// real(A)</span></span><br><span class="line">A.imag();                 <span class="comment">// imag(A)</span></span><br><span class="line"><span class="comment">// if the original type equals destination type, no work is done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Note that for most operations Eigen requires all operands to have the same type:</span></span><br><span class="line">MatrixXf F = MatrixXf::Zero(<span class="number">3</span>,<span class="number">3</span>);</span><br><span class="line">A += F;                <span class="comment">// illegal in Eigen. In Matlab A = A+F is allowed</span></span><br><span class="line">A += F.cast&lt;<span class="keyword">double</span>&gt;(); <span class="comment">// F converted to double and then added (generally, conversion happens on-the-fly)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Eigen can map existing memory into Eigen matrices.</span></span><br><span class="line"><span class="keyword">float</span> <span class="built_in">array</span>[<span class="number">3</span>];</span><br><span class="line">Vector3f::Map(<span class="built_in">array</span>).<span class="built_in">fill</span>(<span class="number">10</span>);            <span class="comment">// create a temporary Map over array and sets entries to 10</span></span><br><span class="line"><span class="keyword">int</span> data[<span class="number">4</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line"><span class="function">Matrix2i <span class="title">mat2x2</span><span class="params">(data)</span></span>;                    <span class="comment">// copies data into mat2x2</span></span><br><span class="line">Matrix2i::Map(data) = <span class="number">2</span>*mat2x2;           <span class="comment">// overwrite elements of data with 2*mat2x2</span></span><br><span class="line">MatrixXi::Map(data, <span class="number">2</span>, <span class="number">2</span>) += mat2x2;      <span class="comment">// adds mat2x2 to elements of data (alternative syntax if size is not know at compile time)</span></span><br></pre></td></tr></table></figure><h4 id="求解线性方程组">6.2.5 求解线性方程组</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Solve Ax = b. Result stored in x. Matlab: x = A \ b.</span></span><br><span class="line">x = A.ldlt().solve(b);  <span class="comment">// A sym. p.s.d.    #include &lt;Eigen/Cholesky&gt;</span></span><br><span class="line">x = A.llt() .solve(b);  <span class="comment">// A sym. p.d.      #include &lt;Eigen/Cholesky&gt;</span></span><br><span class="line">x = A.lu()  .solve(b);  <span class="comment">// Stable and fast. #include &lt;Eigen/LU&gt;</span></span><br><span class="line">x = A.qr()  .solve(b);  <span class="comment">// No pivoting.     #include &lt;Eigen/QR&gt;</span></span><br><span class="line">x = A.svd() .solve(b);  <span class="comment">// Stable, slowest. #include &lt;Eigen/SVD&gt;</span></span><br><span class="line"><span class="comment">// .ldlt() -&gt; .matrixL() and .matrixD()</span></span><br><span class="line"><span class="comment">// .llt()  -&gt; .matrixL()</span></span><br><span class="line"><span class="comment">// .lu()   -&gt; .matrixL() and .matrixU()</span></span><br><span class="line"><span class="comment">// .qr()   -&gt; .matrixQ() and .matrixR()</span></span><br><span class="line"><span class="comment">// .svd()  -&gt; .matrixU(), .singularValues(), and .matrixV()</span></span><br></pre></td></tr></table></figure><h4 id="求解特征值">6.2.6 求解特征值</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Eigenvalue problems</span></span><br><span class="line"><span class="comment">// Eigen                          // Matlab</span></span><br><span class="line">A.eigenvalues();                  <span class="comment">// eig(A);</span></span><br><span class="line"><span class="function">EigenSolver&lt;Matrix3d&gt; <span class="title">eig</span><span class="params">(A)</span></span>;     <span class="comment">// [vec val] = eig(A)</span></span><br><span class="line">eig.eigenvalues();                <span class="comment">// diag(val)</span></span><br><span class="line">eig.eigenvectors();               <span class="comment">// vec</span></span><br><span class="line"><span class="comment">// For self-adjoint matrices use SelfAdjointEigenSolver&lt;&gt;</span></span><br></pre></td></tr></table></figure><h2 id="参考">参考</h2><ol type="1"><li><a href="http://eigen.tuxfamily.org/dox-devel/AsciiQuickReference.txt" target="_blank" rel="noopener">Eigen short ASCII reference</a>eference.txt)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;eigen常用操作eigen-cheatsheet&quot;&gt;Eigen常用操作|Eigen Cheatsheet&lt;/h1&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span 
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
